<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch-foundations-for-inference" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Foundations for Inference</title>
  
  <introduction>
    <p>
      Statistical inference is primarily concerned with understanding and quantifying the
      uncertainty of parameter estimates. While the equations and details change depending on
      the setting, the foundations for inference are the same throughout all of statistics.
    </p>
    
    <p>
      We start with a familiar topic: the idea of using a sample proportion to estimate
      a population proportion. Next, we create what's called a <term>confidence interval</term>,
      which is a range of plausible values where we may find the true population value.
      Finally, we introduce the <term>hypothesis testing framework</term>, which allows us to
      formally evaluate claims about the population, such as whether a survey provides strong
      evidence that a candidate has the support of a majority of the voting population.
    </p>
  </introduction>

  <!-- Section 5.1: Point estimates and sampling variability -->
  <section xml:id="sec-point-estimates">
    <title>Point Estimates and Sampling Variability</title>
    
    <introduction>
      <p>
        Companies such as Pew Research frequently conduct polls as a way to understand the
        state of public opinion or knowledge on many topics, including politics, scientific
        understanding, brand recognition, and more. The ultimate goal in taking a poll is
        generally to use the responses to estimate the opinion or knowledge of the broader
        population.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-point-estimates-error">
      <title>Point Estimates and Error</title>
      
      <p>
        Suppose a poll suggested the US President's approval rating is 45%. We would consider
        45% to be a <term>point estimate</term> of the approval rating we might see if we
        collected responses from the entire population. This entire-population response
        proportion is generally referred to as the <term>parameter of interest</term>.
      </p>
      
      <p>
        When the parameter is a proportion, it is often denoted by <m>p</m>, and we often
        refer to the sample proportion as <m>\hat{p}</m> (pronounced <em>p-hat</em><fn>Not to
        be confused with <em>phat</em>, the slang term used for something cool, like this book.</fn>).
        Unless we collect responses from every individual in the population, <m>p</m> remains
        unknown, and we use <m>\hat{p}</m> as our estimate of <m>p</m>. The difference we
        observe from the poll versus the parameter is called the <term>error</term> in the
        estimate.
      </p>
      
      <p>
        Generally, the error consists of two aspects: sampling error and bias.
      </p>
      
      <p>
        <term>Sampling error</term>, sometimes called <em>sampling uncertainty</em>,
        describes how much an estimate will tend to vary from one sample to the next.
        For instance, the estimate from one sample might be 1% too low while in another
        it may be 3% too high. Much of statistics, including much of this book, is focused
        on understanding and quantifying sampling error, and we will find it useful to
        consider a sample's size to help us quantify this error; the <term>sample size</term>
        is often represented by the letter <m>n</m>.
      </p>
      
      <p>
        <term>Bias</term> describes a systematic tendency to over- or under-estimate the
        true population value. For example, if we were taking a student poll asking about
        support for a new college stadium, we'd probably get a biased estimate of the
        stadium's level of student support by wording the question as, <em>Do you support
        your school by supporting funding for the new stadium?</em> We try to minimize bias
        through thoughtful data collection procedures, which were discussed in
        Chapter 1 and are the topic of many other books.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-variability-point-estimate">
      <title>Understanding the Variability of a Point Estimate</title>
      
      <p>
        Suppose the proportion of American adults who support the expansion of solar energy is
        <m>p = 0.88</m>, which is our parameter of interest.<fn>We haven't actually conducted
        a census to measure this value perfectly. However, a very large sample has suggested
        the actual level of support is about 88%.</fn> If we were to take a poll of 1000 American
        adults on this topic, the estimate would not be perfect, but how close might we expect
        the sample proportion in the poll would be to 88%? We want to understand, <em>how does
        the sample proportion <m>\hat{p}</m> behave when the true population proportion is 0.88</em>.<fn>88%
        written as a proportion would be 0.88. It is common to switch between proportion and percent.
        However, formulas presented in this book always refer to the proportion, not the percent.</fn>
        Let's find out!
      </p>
      
      <p>
        We can simulate responses we would get from a simple random sample of 1000 American
        adults, which is only possible because we know the actual support for expanding solar
        energy is 0.88. Here's how we might go about constructing such a simulation:
      </p>
      
      <ol>
        <li>There were about 250 million American adults in 2018. On 250 million pieces of paper,
            write <q>support</q> on 88% of them and <q>not</q> on the other 12%.</li>
        <li>Mix up the pieces of paper and pull out 1000 pieces to represent our sample of 1000
            American adults.</li>
        <li>Compute the fraction of the sample that say <q>support</q>.</li>
      </ol>
      
      <p>
        Any volunteers to conduct this simulation? Probably not. Running this simulation with
        250 million pieces of paper would be time-consuming and very costly, but we can simulate
        it using computer code; we've written a short program in <xref ref="fig-solar-poll-r-code"/>
        in case you are curious what the computer code looks like. In this simulation, the sample
        gave a point estimate of <m>\hat{p}_1 = 0.894</m>. We know the population proportion for
        the simulation was <m>p = 0.88</m>, so we know the estimate had an error of
        <m>0.894 - 0.88 = +0.014</m>.
      </p>
      
      <figure xml:id="fig-solar-poll-r-code">
        <caption>For those curious, this is code for a single <m>\hat{p}</m> simulation using the
        statistical software called R. Each line that starts with <c>#</c> is a <em>code comment</em>,
        which is used to describe in regular language what the code is doing. We've provided
        software labs in R at <url href="https://www.openintro.org/book/os">openintro.org/book/os</url>
        for anyone interested in learning more.</caption>
        <program language="r">
          <input>
# 1. Create a set of 250 million entries,
#    where 88% of them are "support" and 12% are "not".
pop_size &lt;- 250000000
possible_entries &lt;- c(rep("support", 0.88 * pop_size),
                       rep("not", 0.12 * pop_size))

# 2. Sample 1000 entries without replacement.
sampled_entries &lt;- sample(possible_entries, size = 1000)

# 3. Compute p-hat: count the number that are "support",
#    then divide by the sample size.
sum(sampled_entries == "support") / 1000
          </input>
        </program>
      </figure>
      
      <p>
        One simulation isn't enough to get a great sense of the distribution of estimates we
        might expect in the simulation, so we should run more simulations. In a second simulation,
        we get <m>\hat{p}_2 = 0.885</m>, which has an error of +0.005. In another,
        <m>\hat{p}_3 = 0.878</m> for an error of -0.002. And in another, an estimate of
        <m>\hat{p}_4 = 0.859</m> with an error of -0.021. With the help of a computer, we've
        run the simulation 10,000 times and created a histogram of the results from all 10,000
        simulations in <xref ref="fig-sampling-distribution-solar"/>. This distribution of
        sample proportions is called a <term>sampling distribution</term>. We can characterize
        this sampling distribution as follows:
      </p>
      
      <ul>
        <li><strong>Center.</strong> The center of the distribution is
            <m>\bar{x}_{\hat{p}} = 0.880</m>, which is the same as the parameter. Notice that
            the simulation mimicked a simple random sample of the population, which is a
            straightforward sampling strategy that helps avoid sampling bias.</li>
        <li><strong>Spread.</strong> The standard deviation of the distribution is
            <m>s_{\hat{p}} = 0.010</m>. When we're talking about a sampling distribution or
            the variability of a point estimate, we typically use the term <term>standard error</term>
            rather than <em>standard deviation</em>, and the notation <m>SE_{\hat{p}}</m> is used
            for the standard error associated with the sample proportion.</li>
        <li><strong>Shape.</strong> The distribution is symmetric and bell-shaped, and it
            <em>resembles a normal distribution</em>.</li>
      </ul>
      
      <p>
        These findings are encouraging! When the population proportion is <m>p = 0.88</m> and
        the sample size is <m>n = 1000</m>, the sample proportion <m>\hat{p}</m> tends to give
        a pretty good estimate of the population proportion. We also have the interesting
        observation that the histogram resembles a normal distribution.
      </p>
      
      <figure xml:id="fig-sampling-distribution-solar">
        <caption>A histogram of 10,000 sample proportions, where each sample is taken from a
        population where the population proportion is 0.88 and the sample size is <m>n = 1000</m>.</caption>
        <image source="images/ch_foundations_for_inf/figures/sampling_10k_prop_88p/sampling_10k_prop_88p.png" width="80%">
          <description>A histogram is shown for 10,000 sample proportions where each sample is
          taken from a population where the population proportion is 0.88 and the sample size is
          n = 1000. The distribution is bell-shaped (appears nearly normal), is centered at 0.88
          and has a standard deviation of about 0.01.</description>
        </image>
      </figure>
      
      <assemblage>
        <title>Sampling distributions are never observed, but we keep them in mind</title>
        <p>
          In real-world applications, we never actually observe the sampling distribution, yet
          it is useful to always think of a point estimate as coming from such a hypothetical
          distribution. Understanding the sampling distribution will help us characterize and
          make sense of the point estimates that we do observe.
        </p>
      </assemblage>
      
      <example xml:id="ex-smaller-sample-size">
        <statement>
          <p>
            If we used a much smaller sample size of <m>n = 50</m>, would you guess that
            the standard error for <m>\hat{p}</m> would be larger or smaller than when we
            used <m>n = 1000</m>?
          </p>
        </statement>
        <solution>
          <p>
            Intuitively, it seems like more data is better than less data, and generally
            that is correct! The typical error when <m>p = 0.88</m> and <m>n = 50</m>
            would be larger than the error we would expect when <m>n = 1000</m>.
          </p>
        </solution>
      </example>
      
      <p>
        Example <xref ref="ex-smaller-sample-size"/> highlights an important property we will
        see again and again: a bigger sample tends to provide a more precise point estimate
        than a smaller sample.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-central-limit-theorem">
      <title>Central Limit Theorem</title>
      
      <p>
        The distribution in <xref ref="fig-sampling-distribution-solar"/> looks an awful lot
        like a normal distribution. That is no anomaly; it is the result of a general principle
        called the <term>Central Limit Theorem</term>.
      </p>
      
      <assemblage>
        <title>Central Limit Theorem and the success-failure condition</title>
        <p>
          When observations are independent and the sample size is sufficiently large, the
          sample proportion <m>\hat{p}</m> will tend to follow a normal distribution with the
          following mean and standard error:
        </p>
        <md>
          <mrow>\mu_{\hat{p}} \amp= p</mrow>
          <mrow>SE_{\hat{p}} \amp= \sqrt{\frac{p(1-p)}{n}}</mrow>
        </md>
        <p>
          In order for the Central Limit Theorem to hold, the sample size is typically
          considered sufficiently large when <m>np \geq 10</m> and <m>n(1-p) \geq 10</m>,
          which is called the <term>success-failure condition</term>.
        </p>
      </assemblage>
      
      <p>
        The Central Limit Theorem is incredibly important, and it provides a foundation for much
        of statistics. As we begin applying the Central Limit Theorem, be mindful of the two
        technical conditions: the observations must be independent, and the sample size must be
        sufficiently large such that <m>np \geq 10</m> and <m>n(1-p) \geq 10</m>.
      </p>
      
      <example xml:id="ex-confirm-clt-applies">
        <statement>
          <p>
            Earlier we estimated the mean and standard error of <m>\hat{p}</m> using simulated
            data when <m>p = 0.88</m> and <m>n = 1000</m>. Confirm that the Central Limit Theorem
            applies and the sampling distribution is approximately normal.
          </p>
        </statement>
        <solution>
          <ol>
            <li>
              <p>
                <strong>Independence.</strong> There are <m>n = 1000</m> observations for each
                sample proportion <m>\hat{p}</m>, and each of those observations are independent
                draws. <em>The most common way for observations to be considered independent is if
                they are from a simple random sample.</em>
              </p>
            </li>
            <li>
              <p>
                <strong>Success-failure condition.</strong> We can confirm the sample size is
                sufficiently large by checking the success-failure condition and confirming
                the two calculated values are greater than 10:
                <md>
                  <mrow>np \amp= 1000 \times 0.88 = 880 \geq 10</mrow>
                  <mrow>n(1-p) \amp= 1000 \times (1 - 0.88) = 120 \geq 10</mrow>
                </md>
              </p>
            </li>
          </ol>
          <p>
            The independence and success-failure conditions are both satisfied, so the Central
            Limit Theorem applies, and it's reasonable to model <m>\hat{p}</m> using a normal
            distribution.
          </p>
        </solution>
      </example>
      
      <assemblage>
        <title>How to verify sample observations are independent</title>
        <p>
          Subjects in an experiment are considered independent if they undergo random assignment
          to the treatment groups.
        </p>
        <p>
          If the observations are from a simple random sample, then they are independent.
        </p>
        <p>
          If a sample is from a seemingly random process, e.g. an occasional error on an assembly
          line, checking independence is more difficult. In this case, use your best judgement.
        </p>
      </assemblage>
      
      <p>
        An additional condition that is sometimes added for samples from a population is that
        they are no larger than 10% of the population. When the sample exceeds 10% of the
        population size, the methods we discuss tend to overestimate the sampling error slightly
        versus what we would get using more advanced methods.<fn>For example, we could use what's
        called the <em>finite population correction factor</em>: if the sample is of size <m>n</m>
        and the population size is <m>N</m>, then we can multiply the typical standard error formula
        by <m>\sqrt{\frac{N-n}{N-1}}</m> to obtain a smaller, more precise estimate of the actual
        standard error. When <m>n &lt; 0.1 \times N</m>, this correction factor is relatively small.</fn>
        This is very rarely an issue, and when it is an issue, our methods tend to be conservative,
        so we consider this additional check as optional.
      </p>
      
      <example xml:id="ex-se-calculation">
        <statement>
          <p>
            Compute the theoretical mean and standard error of <m>\hat{p}</m> when <m>p = 0.88</m>
            and <m>n = 1000</m>, according to the Central Limit Theorem.
          </p>
        </statement>
        <solution>
          <p>
            The mean of the <m>\hat{p}</m>'s is simply the population proportion:
            <me>\mu_{\hat{p}} = 0.88</me>
          </p>
          <p>
            The calculation of the standard error of <m>\hat{p}</m> uses the following formula:
            <md>
              <mrow>SE_{\hat{p}} \amp= \sqrt{\frac{p(1-p)}{n}}</mrow>
              <mrow>\amp= \sqrt{\frac{0.88 \times (1 - 0.88)}{1000}}</mrow>
              <mrow>\amp= 0.010</mrow>
            </md>
          </p>
        </solution>
      </example>
      
      <example xml:id="ex-normal-prob-sample-prop">
        <statement>
          <p>
            Estimate how frequently the sample proportion <m>\hat{p}</m> should be within
            0.02 (2%) of the population value, <m>p = 0.88</m>. Based on
            Examples <xref ref="ex-confirm-clt-applies"/> and <xref ref="ex-se-calculation"/>,
            we know that the distribution is approximately
            <m>N(\mu_{\hat{p}} = 0.88, SE_{\hat{p}} = 0.010)</m>.
          </p>
        </statement>
        <solution>
          <p>
            After so much practice in Section 4.1, this normal distribution example will
            hopefully feel familiar! We would like to understand the fraction of <m>\hat{p}</m>'s
            between 0.86 and 0.90:
          </p>
          <figure xml:id="fig-p-hat-from-86-and-90">
            <caption>A normal distribution centered at 0.88 with a standard deviation of 0.01
            is shown, where the region between 0.86 and 0.90 has been shaded.</caption>
            <image source="images/ch_foundations_for_inf/figures/p-hat_from_86_and_90/p-hat_from_86_and_90.png" width="35%">
              <description>Normal curve centered at 0.88 with shaded region between 0.86 and 0.90</description>
            </image>
          </figure>
          <p>
            With <m>\mu_{\hat{p}} = 0.88</m> and <m>SE_{\hat{p}} = 0.010</m>, we can compute
            the Z-score for both the left and right cutoffs:
            <md>
              <mrow>Z_{0.86} \amp= \frac{0.86 - 0.88}{0.010} = -2</mrow>
              <mrow>Z_{0.90} \amp= \frac{0.90 - 0.88}{0.010} = 2</mrow>
            </md>
          </p>
          <p>
            We can use either statistical software, a graphing calculator, or a table to find
            the areas to the tails, and in any case we will find that they are each 0.0228. The
            total tail areas are <m>2 \times 0.0228 = 0.0456</m>, which leaves the shaded area
            of 0.9544. That is, about 95.44% of the sampling distribution in
            <xref ref="fig-sampling-distribution-solar"/> is within <m>\pm 0.02</m> of the
            population proportion, <m>p = 0.88</m>.
          </p>
        </solution>
      </example>
      
      <exercise>
        <statement>
          <p>
            In Example <xref ref="ex-smaller-sample-size"/> we discussed how a smaller sample
            would tend to produce a less reliable estimate. Explain how this intuition is
            reflected in the formula for
            <m>SE_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}}</m>.<fn>Since the sample size <m>n</m> is
            in the denominator (on the bottom) of the fraction, a bigger sample size means the
            entire expression when calculated will tend to be smaller. That is, a larger sample
            size would correspond to a smaller standard error.</fn>
          </p>
        </statement>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-clt-real-world">
      <title>Applying the Central Limit Theorem to a real-world setting</title>
      
      <p>
        We do not actually know the population proportion unless we conduct an expensive poll of
        all individuals in the population. Our earlier value of <m>p = 0.88</m> was based on poll
        conducted by Pew Research of 1000 American adults that found <m>\hat{p} = 0.887</m>
        of them favored expanding solar energy. The researchers might have wondered: does the
        sample proportion from the poll approximately follow a normal distribution? We can check
        the conditions from the Central Limit Theorem:
      </p>
      
      <ol>
        <li>
          <p>
            <strong>Independence.</strong> The poll is a simple random sample of American adults,
            which means that the observations are independent.
          </p>
        </li>
        <li>
          <p>
            <strong>Success-failure condition.</strong> To check this condition, we need the
            population proportion, <m>p</m>, to check if both <m>np</m> and <m>n(1-p)</m> are
            greater than 10. However, we do not actually know <m>p</m>, which is exactly why the
            pollsters would take a sample! In cases like these, we often use <m>\hat{p}</m> as
            our next best way to check the success-failure condition:
            <md>
              <mrow>n\hat{p} \amp= 1000 \times 0.887 = 887</mrow>
              <mrow>n(1-\hat{p}) \amp= 1000 \times (1 - 0.887) = 113</mrow>
            </md>
            The sample proportion <m>\hat{p}</m> acts as a reasonable substitute for <m>p</m>
            during this check, and each value in this case is well above the minimum of 10.
          </p>
        </li>
      </ol>
      
      <p>
        This <term>substitution approximation</term> of using <m>\hat{p}</m> in place of
        <m>p</m> is also useful when computing the standard error of the sample proportion:
        <md>
          <mrow>SE_{\hat{p}} \amp= \sqrt{\frac{p(1-p)}{n}}</mrow>
          <mrow>\amp\approx \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}</mrow>
          <mrow>\amp= \sqrt{\frac{0.887 \times (1 - 0.887)}{1000}}</mrow>
          <mrow>\amp= 0.010</mrow>
        </md>
        This substitution technique is sometimes referred to as the <q>plug-in principle</q>.
        In this case, <m>SE_{\hat{p}}</m> didn't change enough to be detected using only 3
        decimal places versus when we completed the calculation with 0.88 earlier. The computed
        standard error tends to be reasonably stable even when observing slightly different
        proportions in one sample or another.
      </p>
    </subsection>

    <subsection xml:id="subsec-more-details-clt">
      <title>More details regarding the Central Limit Theorem</title>
      
      <p>
        We've applied the Central Limit Theorem in numerous examples so far this chapter:
      </p>
      
      <blockquote>
        <p>
          <em>When observations are independent and the sample size is sufficiently large,
          the distribution of <m>\hat{p}</m> resembles a normal distribution with</em>
          <md>
            <mrow>\mu_{\hat{p}} \amp= p</mrow>
            <mrow>SE_{\hat{p}} \amp= \sqrt{\frac{p(1-p)}{n}}</mrow>
          </md>
          <em>The sample size is considered sufficiently large when <m>np \geq 10</m> and
          <m>n(1-p) \geq 10</m>.</em>
        </p>
      </blockquote>
      
      <p>
        In this section, we'll explore the success-failure condition and seek to better
        understand the Central Limit Theorem.
      </p>
      
      <p>
        An interesting question to answer is, <em>what happens when <m>np &lt; 10</m> or
        <m>n(1-p) &lt; 10</m>?</em> As we did in
        Section <xref ref="subsec-variability-point-estimate"/>, we can simulate drawing
        samples of different sizes where, say, the true proportion is <m>p = 0.25</m>. Here's
        a sample of size 10:
      </p>
      
      <blockquote>
        <p>
          no, no, yes, yes, no, no, no, no, no, no
        </p>
      </blockquote>
      
      <p>
        In this sample, we observe a sample proportion of yeses of
        <m>\hat{p} = \frac{2}{10} = 0.2</m>. We can simulate many such proportions to
        understand the sampling distribution of <m>\hat{p}</m> when <m>n = 10</m> and
        <m>p = 0.25</m>, which we've plotted in <xref ref="fig-sampling-10-prop-25p"/>
        alongside a normal distribution with the same mean and variability. These distributions
        have a number of important differences.
      </p>
      
      <figure xml:id="fig-sampling-10-prop-25p">
        <caption>Left: simulations of <m>\hat{p}</m> when the sample size is <m>n = 10</m> and
        the population proportion is <m>p = 0.25</m>. Right: a normal distribution with the same
        mean (0.25) and standard deviation (0.137).</caption>
        <image source="images/ch_foundations_for_inf/figures/sampling_10_prop_25p/sampling_10_prop_25p.png" width="97%">
          <description>There are two plots. The first plot is a histogram of 10,000 simulations
          of p-hat when the sample size is n equals 10 and the population proportion is p equals
          0.25. The possible values are 0.0, 0.1, 0.2, and so on up to 1.0, though the graph only
          shows values up to 0.8. The distribution is centered at about 0.25, and is slightly
          right-skewed. The frequencies are about 500 for 0.0, 1900 for 0.1, 2800 for 0.2, 2400
          for 0.3, 1500 for 0.4, 500 for 0.5, 100 for 0.6, and the bin heights for the remaining
          values have bin heights that are not visually distinguishable from zero. The second plot
          shows a normal distribution centered at 0.25 with a standard deviation of 0.137. The
          plot has a vertical line located at 0.0, which makes it more visually evident that a
          portion of the area under the normal distribution -- about 5% of this area -- represents
          values below 0.0.</description>
        </image>
      </figure>
      
      <p>
        <xref ref="fig-clt-grid-1"/> and <xref ref="fig-clt-grid-2"/> show sampling
        distributions for several scenarios of <m>p</m> and <m>n</m>.
      </p>
      
      <figure xml:id="fig-clt-grid-1">
        <caption>Sampling distributions for several scenarios of <m>p</m> and <m>n</m>.
        Rows: <m>p = 0.10</m>, <m>p = 0.20</m>, <m>p = 0.50</m>, <m>p = 0.80</m>, and
        <m>p = 0.90</m>. Columns: <m>n = 10</m> and <m>n = 25</m>.</caption>
        <image source="images/ch_foundations_for_inf/figures/clt_prop_grid/clt_prop_grid_1.png" width="97%">
          <description>Sampling distributions are shown for several scenarios for parameters p
          and n. The graphs are arranged in a grid of 5 rows representing proportions 0.1, 0.2,
          0.5, 0.8, and 0.9 and 2 columns of sample sizes n equals 10 and 25. In each graph, the
          distribution is centered at the proportion. Given that these are proportions based on
          relatively small sample sizes, the bins do look relatively discrete (jumpy from one to
          the next), though less so for the distributions based on n equals 25. In cases where the
          true underlying proportion is near the lower bound of 0 or the upper bound of 1, the
          distribution tends to skew away from that boundary. This is most noticeable for both the
          distributions representing proportions closer to either boundary and for the smaller
          sample size. One distribution stands out among the 10 shown: the sample with p equals
          0.5 and n equals 25, which shows a bell-shaped distribution resembling the normal
          distribution, though the data are still somewhat discrete.</description>
        </image>
      </figure>
      
      <figure xml:id="fig-clt-grid-2">
        <caption>Sampling distributions for several scenarios of <m>p</m> and <m>n</m>.
        Rows: <m>p = 0.10</m>, <m>p = 0.20</m>, <m>p = 0.50</m>, <m>p = 0.80</m>, and
        <m>p = 0.90</m>. Columns: <m>n = 50</m>, <m>n = 100</m>, and <m>n = 250</m>.</caption>
        <image source="images/ch_foundations_for_inf/figures/clt_prop_grid/clt_prop_grid_2.png" width="97%">
          <description>Sampling distributions are shown for several scenarios for parameters p
          and n. The graphs are arranged in a grid of 5 rows representing proportions 0.1, 0.2,
          0.5, 0.8, and 0.9 and 3 columns of sample sizes n equals 50, 100, and 250. Relative to
          the previous figure, which considered similar proportion scenarios but with n equals 10
          and 25, the data in these graphs looks less discrete -- that is, they appear to almost
          be continuous. This is most evident for the largest sample sizes. Nearly all of the
          graphs shown also closely resemble the normal distribution, in some cases with the larger
          sample sizes that it resembles it so closely that there are not substantial visual
          differences. One aspect less evident -- but still present -- in the last figure but that
          continues into and becomes much more obvious in this figure, is that the distributions of
          the sample proportions tend to have a much smaller standard deviation with the larger
          sample sizes. That is, the sample proportion distributions for larger sample sizes tend
          to be smaller than they were for smaller sample sizes. Also, the variability within a
          graph also appears to be largest for the proportion p equals 0.5 than it is for the other
          proportions when considering a single proportion -- and this property is apparent upon
          inspection of a distribution based on any of the considered sample sizes.</description>
        </image>
      </figure>
      
      <table>
        <title>Comparison of distributions</title>
        <tabular>
          <row header="yes">
            <cell></cell>
            <cell>Unimodal?</cell>
            <cell>Smooth?</cell>
            <cell>Symmetric?</cell>
          </row>
          <row>
            <cell>Normal: <m>N(0.25, 0.14)</m></cell>
            <cell>Yes</cell>
            <cell>Yes</cell>
            <cell>Yes</cell>
          </row>
          <row>
            <cell><m>n = 10</m>, <m>p = 0.25</m></cell>
            <cell>Yes</cell>
            <cell>No</cell>
            <cell>No</cell>
          </row>
        </tabular>
      </table>
      
      <p>
        Notice that the success-failure condition was not satisfied when <m>n = 10</m> and
        <m>p = 0.25</m>:
        <md>
          <mrow>np = 10 \times 0.25 = 2.5 \amp\amp n(1-p) = 10 \times 0.75 = 7.5</mrow>
        </md>
        This single sampling distribution does not show that the success-failure condition is
        the perfect guideline, but we have found that the guideline did correctly identify that
        a normal distribution might not be appropriate.
      </p>
      
      <p>
        We can complete several additional simulations, shown in
        <xref ref="fig-clt-grid-1"/> and <xref ref="fig-clt-grid-2"/>, and we can see some
        trends:
      </p>
      
      <ol>
        <li>When either <m>np</m> or <m>n(1-p)</m> is small, the distribution is more
            <term>discrete</term>, i.e. <em>not continuous</em>.</li>
        <li>When <m>np</m> or <m>n(1-p)</m> is smaller than 10, the skew in the distribution
            is more noteworthy.</li>
        <li>The larger both <m>np</m> <em>and</em> <m>n(1-p)</m>, the more normal the
            distribution. This may be a little harder to see for the larger sample size in these
            plots as the variability also becomes much smaller.</li>
        <li>When <m>np</m> and <m>n(1-p)</m> are both very large, the distribution's
            discreteness is hardly evident, and the distribution looks much more like a normal
            distribution.</li>
      </ol>
      
      <p>
        So far we've only focused on the skew and discreteness of the distributions. We haven't
        considered how the mean and standard error of the distributions change. Take a moment to
        look back at the graphs, and pay attention to three things:
      </p>
      
      <ol>
        <li>The centers of the distribution are always at the population proportion, <m>p</m>,
            that was used to generate the simulation. Because the sampling distribution of
            <m>\hat{p}</m> is always centered at the population parameter <m>p</m>, it means
            the sample proportion <m>\hat{p}</m> is <term>unbiased</term> when the data are
            independent and drawn from such a population.</li>
        <li>For a particular population proportion <m>p</m>, the variability in the sampling
            distribution decreases as the sample size <m>n</m> becomes larger. This will likely
            align with your intuition: an estimate based on a larger sample size will tend to be
            more accurate.</li>
        <li>For a particular sample size, the variability will be largest when <m>p = 0.5</m>.
            The differences may be a little subtle, so take a close look. This reflects the role
            of the proportion <m>p</m> in the standard error formula:
            <m>SE = \sqrt{\frac{p(1-p)}{n}}</m>. The standard error is largest when
            <m>p = 0.5</m>.</li>
      </ol>
      
      <p>
        At no point will the distribution of <m>\hat{p}</m> look <em>perfectly</em> normal,
        since <m>\hat{p}</m> will always take discrete values (<m>x/n</m>). It is always a
        matter of degree, and we will use the standard success-failure condition with minimums
        of 10 for <m>np</m> and <m>n(1-p)</m> as our guideline within this book.
      </p>
    </subsection>
        
    <subsection xml:id="subsec-extending-framework">
      <title>Extending the framework for other statistics</title>
      
      <p>
        The strategy of using a sample statistic to estimate a parameter is quite common, and
        it's a strategy that we can apply to other statistics besides a proportion. For instance,
        if we want to estimate the average salary for graduates from a particular college, we
        could survey a random sample of recent graduates; in that example, we'd be using a sample
        mean <m>\bar{x}</m> to estimate the population mean <m>\mu</m> for all graduates. As
        another example, if we want to estimate the difference in product prices for two websites,
        we might take a random sample of products available on both sites, check the prices on
        each, and then compute the average difference; this strategy certainly would give us some
        idea of the actual difference through a point estimate.
      </p>
      
      <p>
        While this chapter emphasizes a single proportion context, we'll encounter many different
        contexts throughout this book where these methods will be applied. The principles and
        general ideas are the same, even if the details change a little. We've also sprinkled
        some other contexts into the exercises to help you start thinking about how the ideas
        generalize.
      </p>
    </subsection>
    
    <exercises>
      <title>Section 5.1 Exercises</title>
      
      <exercise xml:id="ex-identify-parameter-1">
        <title>Identify the parameter, Part I</title>
        <statement>
          <p>
            For each of the following situations, state whether the parameter of interest
            is a mean or a proportion. It may be helpful to examine whether individual
            responses are numerical or categorical.
          </p>
          <ol>
            <li>In a survey, one hundred college students are asked how many hours per week
                they spend on the Internet.</li>
            <li>In a survey, one hundred college students are asked: "What percentage of
                the time you spend on the Internet is part of your course work?"</li>
            <li>In a survey, one hundred college students are asked whether or not they
                cited information from Wikipedia in their papers.</li>
            <li>In a survey, one hundred college students are asked what percentage of
                their total weekly spending is on alcoholic beverages.</li>
            <li>In a sample of one hundred recent college graduates, it is found that 85
                percent expect to get a job within one year of their graduation date.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-identify-parameter-2">
       <title>Identify the parameter, Part II</title>
        <statement>
          <p>
            For each of the following situations, state whether the parameter of interest
            is a mean or a proportion.
          </p>
          <ol>
            <li>A poll shows that 64% of Americans personally worry a great deal about
                federal spending and the budget deficit.</li>
            <li>A survey reports that local TV news has shown a 17% increase in revenue
                within a two year period while newspaper revenues decreased by 6.4% during
                this time period.</li>
            <li>In a survey, high school and college students are asked whether or not
                they use geolocation services on their smart phones.</li>
            <li>In a survey, smart phone users are asked whether or not they use a
                web-based taxi service.</li>
            <li>In a survey, smart phone users are asked how many times they used a
                web-based taxi service over the last year.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-quality-control">
       <title>Quality control</title>
        <statement>
          <p>
            As part of a quality control process for computer chips, an engineer at a
            factory randomly samples 212 chips during a week of production to test the
            current rate of chips with severe defects. She finds that 27 of the chips
            are defective.
          </p>
          <ol>
            <li>What population is under consideration in the data set?</li>
            <li>What parameter is being estimated?</li>
            <li>What is the point estimate for the parameter?</li>
            <li>What is the name of the statistic we use to measure the uncertainty of
                the point estimate?</li>
            <li>Compute this value for this context.</li>
            <li>The historical rate of defects is 10%. Should the engineer be surprised
                by the observed rate of defects during the current week?</li>
            <li>Suppose the true population value was found to be 10%. If we use this
                proportion to recompute the value in part (e) using <m>p = 0.1</m>
                instead of <m>\hat{p}</m>, does the resulting value change much?</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-unexpected-expense">
       <title>Unexpected expense</title>
        <statement>
          <p>
            In a random sample of 765 adults in the United States, 322 say they could not
            cover a $400 unexpected expense without borrowing money or going into debt.
          </p>
          <ol>
            <li>What population is under consideration in the data set?</li>
            <li>What parameter is being estimated?</li>
            <li>What is the point estimate for the parameter?</li>
            <li>What is the name of the statistic we use to measure the uncertainty of
                the point estimate?</li>
            <li>Compute the value from part (d) for this context.</li>
            <li>A cable news pundit thinks the value is actually 50%. Should she be
                surprised by the data?</li>
            <li>Suppose the true population value was found to be 40%. If we use this
                proportion to recompute the value in part (e) using <m>p = 0.4</m>
                instead of <m>\hat{p}</m>, does the resulting value change much?</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-repeated-water-samples">
       <title>Repeated water samples</title>
        <statement>
          <p>
            A nonprofit wants to understand the fraction of households that have elevated
            levels of lead in their drinking water. They expect at least 5% of homes will
            have elevated levels of lead, but not more than about 30%. They randomly
            sample 800 homes and work with the owners to retrieve water samples, and they
            compute the fraction of these homes with elevated lead levels. They repeat
            this 1,000 times and build a distribution of sample proportions.
          </p>
          <ol>
            <li>What is this distribution called?</li>
            <li>Would you expect the shape of this distribution to be symmetric, right
                skewed, or left skewed? Explain your reasoning.</li>
            <li>If the proportions are distributed around 8%, what is the variability
                of the distribution?</li>
            <li>What is the formal name of the value you computed in (c)?</li>
            <li>Suppose the researchers' budget is reduced, and they are only able to
                collect 250 observations per sample, but they can still collect 1,000
                samples. They build a new distribution of sample proportions. How will
                the variability of this new distribution compare to the variability of
                the distribution when each sample contained 800 observations?</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-repeated-student-samples">
       <title>Repeated student samples</title>
        <statement>
          <p>
            Of all freshman at a large college, 16% made the dean's list in the current
            year. As part of a class project, students randomly sample 40 students and
            check if those students made the list. They repeat this 1,000 times and
            build a distribution of sample proportions.
          </p>
          <ol>
            <li>What is this distribution called?</li>
            <li>Would you expect the shape of this distribution to be symmetric, right
                skewed, or left skewed? Explain your reasoning.</li>
            <li>Calculate the variability of this distribution.</li>
            <li>What is the formal name of the value you computed in (c)?</li>
            <li>Suppose the students decide to sample again, this time collecting 90
                students per sample, and they again collect 1,000 samples. They build a
                new distribution of sample proportions. How will the variability of this
                new distribution compare to the variability of the distribution when each
                sample contained 40 observations?</li>
          </ol>
        </statement>
      </exercise>
    </exercises>
  </section>

  <!-- Section 5.2: Confidence intervals for a proportion -->
  <section xml:id="sec-confidence-intervals">
    <title>Confidence Intervals for a Proportion</title>
    
    <introduction>
      <p>
        The sample proportion <m>\hat{p}</m> provides a single plausible value
        for the population proportion <m>p</m>. However, the sample proportion
        isn't perfect and will have some <em>standard error</em>
        associated with it.
        When stating an estimate for the population  proportion,
        it is better practice to provide a plausible
        <em>range of values</em> instead of supplying just the point
        estimate.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-capturing-parameter">
      <title>Capturing the Population Parameter</title>
      
      <p>
        Using only a point estimate is like fishing in a murky
        lake with a spear. We can throw a spear where we
        saw a fish, but we will probably miss. On the other hand,
        if we toss a net in that area, we have a good chance of
        catching the fish.
        A <term>confidence interval</term> is like fishing with a net,
        and it represents a range of plausible values where we
        are likely to find the population parameter.
      </p>
      
      <p>
        If we report a point estimate <m>\hat{p}</m>, we probably
        will not hit the exact population proportion. On the
        other hand, if we report a range of plausible values,
        representing a confidence interval,
        we have a good shot at capturing the parameter.
      </p>
      
      <exercise>
        <statement>
          <p>
            If we want to be very certain we capture the population
            proportion in an interval, should we use a wider interval
            or a smaller interval?
          </p>
        </statement>
        <solution>
          <p>
            If we want to be more
            certain we will capture the fish, we might use a
            wider net. Likewise, we use a wider confidence interval
            if we want to be more certain that we capture the
            parameter.
          </p>
        </solution>
      </exercise>
      
      <figure xml:id="fig-25-confidence-intervals">
        <caption>Twenty-five point estimates and confidence
            intervals from the simulations in
            <xref ref="subsec-variability-point-estimate"/>.
            These intervals are shown relative to the population
            proportion <m>p = 0.88</m>.
            Only 1 of these 25
            intervals did not capture the population
            proportion, and this interval has been bolded.</caption>
        <image source="images/ch_foundations_for_inf/figures/95PercentConfidenceInterval/95PercentConfidenceInterval.png" width="75%">
          <description>Twenty-five point estimates and confidence intervals from the simulations are shown. These intervals are shown relative to the population proportion p equals 0.88. The point estimates vary around the true population proportion of 0.88, but most of their confidence intervals overlap the value p equals 0.88. One of the 25 intervals does not have a confidence interval that overlaps the population proportion, and this interval has been bolded. We might say that this confidence interval did not capture the parameter p equals 0.88.</description>
        </image>
      </figure>
      
      <example xml:id="ex-interval-doesnt-contain-p">
        <title>Interpreting when an interval doesn't capture the parameter</title>
        <statement>
          <p>
            In <xref ref="fig-25-confidence-intervals"/>,
            one interval does not contain <m>p = 0.88</m>.
            Does this imply that the population proportion used
            in the simulation could not have been
            <m>p = 0.88</m>?
          </p>
        </statement>
        <solution>
          <p>
            Just as some observations naturally
            occur more than 1.96 standard deviations
            from the mean, some point estimates will be more than
            1.96 standard errors from the parameter of interest.
            A confidence interval only provides a plausible range
            of values.
            While we might say other values are implausible
            based on the data, this does not mean they are impossible.
          </p>
        </solution>
      </example>
    </subsection>
    
    <subsection xml:id="subsec-95-ci">
      <title>Constructing a 95% Confidence Interval</title>
      
      <p>
        Our sample proportion <m>\hat{p}</m> is the most plausible
        value of the population proportion, so it makes sense
        to build a confidence interval around this point estimate.
        The standard error provides a guide for how
        large we should make the confidence interval.
      </p>
      
      <p>
        The standard error represents the standard deviation
        of the point estimate, and when the Central
        Limit Theorem conditions are satisfied,
        the point estimate closely follows a normal distribution.
        In a normal distribution, 95% of
        the data is within 1.96 standard deviations of the mean.
        Using this principle, we can construct a confidence
        interval that extends 1.96 standard errors from the sample
        proportion to be <term>95% confident</term>
        that the interval captures the population proportion:
      </p>
      
      <md>
        <mrow>\text{point estimate}\ \amp\pm\ 1.96 \times SE</mrow>
        <mrow>\hat{p}\ \amp\pm\ 1.96 \times \sqrt{\frac{p (1 - p)}{n}}</mrow>
      </md>
      
      <p>
        But what does <q>95% confident</q> mean? Suppose we took
        many samples and built a 95% confidence interval from
        each. Then about 95% of those intervals would
        contain the parameter, <m>p</m>.
        <xref ref="fig-25-confidence-intervals"/> shows the
        process of creating 25 intervals from 25 samples
        from the simulation in
        <xref ref="subsec-variability-point-estimate"/>,
        where 24 of the resulting confidence intervals contain
        the simulation's population proportion of
        <m>p = 0.88</m>, and one interval does not.
      </p>
      
      <assemblage xml:id="def-95-ci">
        <title>95% confidence interval for a parameter</title>
        <p>
          When the distribution of a point estimate qualifies for
          the Central Limit Theorem and
          therefore closely follows a normal distribution,
          we can construct a 95% confidence interval as
        </p>
        <md>
          <mrow>\text{point estimate} \amp\pm 1.96 \times SE</mrow>
        </md>
      </assemblage>
      
      <example xml:id="ex-solar-95-ci">
        <title>95% CI for solar energy support</title>
        <statement>
          <p>
            In <xref ref="sec-point-estimates"/> we learned about
            a Pew Research poll where
            88.7% of a random sample of
            1,000 American adults
            supported expanding the role of solar power.
            Compute and
            interpret a 95% confidence interval for the population
            proportion.
          </p>
        </statement>
        <solution>
          <p>
            We earlier confirmed that <m>\hat{p}</m> follows a normal
            distribution and has a standard error of
            <m>SE_{\hat{p}} = 0.0100</m>.
            To compute the 95% confidence interval, plug the
            point estimate <m>\hat{p} = 0.887</m> and
            standard error into the 95% confidence interval formula:
          </p>
          <md>
            <mrow>\hat{p} \pm 1.96 \times SE_{\hat{p}}
              \amp\quad\to\quad
              0.887 \pm 1.96 \times 0.0100</mrow>
            <mrow>\amp\quad\to\quad
              (0.8674, 0.9066)</mrow>
          </md>
          <p>
            We are 95% confident that the actual proportion of
            American adults who support expanding solar power is
            between 86.7% and 90.7%.
            (It's common to round to the nearest percentage point
            or nearest tenth of a percentage point when reporting
            a confidence interval.)
          </p>
        </solution>
      </example>
    </subsection>
    
    <subsection xml:id="subsec-changing-confidence">
      <title>Changing the Confidence Level</title>
      
      <p>
        Suppose we want to consider confidence intervals where the confidence
        level is higher than 95%, such as a confidence
        level of 99%. Think back to the analogy about trying to catch a fish:
        if we want to be more sure that we will catch the fish, we should use
        a wider net. To create a 99% confidence level, we must also widen our
        95% interval. On the other hand, if we want an interval with lower
        confidence, such as 90%, we could use a slightly narrower
        interval than our original 95% interval.
      </p>
      
      <p>
        The 95% confidence interval structure provides guidance in
        how to make intervals with different confidence levels.
        The general 95% confidence interval for a point estimate
        that follows a normal distribution is
      </p>
      <md>
        <mrow>\text{point estimate}\ \amp\pm\ 1.96 \times SE</mrow>
      </md>
      <p>
        There are three components to this interval: the point estimate,
        <q>1.96</q>, and the standard error. The choice of <m>1.96\times SE</m> was
        based on capturing 95% of the data since the estimate is within
        1.96 standard errors of the parameter about 95% of the time.
        The choice of 1.96 corresponds to a 95% confidence level.
      </p>
      
      <exercise>
        <statement>
          <p>
            If <m>X</m> is a normally distributed random variable, what is the
            probability of the value <m>X</m> being
            within 2.58 standard deviations of the mean?
          </p>
        </statement>
        <solution>
          <p>
            This is equivalent to asking how often the
            Z-score will be larger than -2.58 but less than 2.58.
            For a picture, see <xref ref="fig-choosing-z-star"/>.
            To determine this probability, we can use statistical software,
            a calculator, or a table to look up -2.58 and 2.58 for
            a normal distribution: 0.0049 and 0.9951.
            Thus, there is a <m>0.9951-0.0049 \approx 0.99</m> probability
            that an unobserved normal random variable
            <m>X</m> will be within 2.58 standard deviations of <m>\mu</m>.
          </p>
        </solution>
      </exercise>
      
      <p>
        This exercise highlights
        that 99% of the time a normal random variable will be within
        2.58 standard deviations of the mean.
        To create a 99% confidence interval, change 1.96 in the 95%
        confidence interval formula to be 2.58.
        That is, the formula
        for a 99% confidence interval is
      </p>
      <md>
        <mrow>\text{point estimate}\ \amp\pm\ 2.58 \times SE</mrow>
      </md>
      
      <figure xml:id="fig-choosing-z-star">
        <caption>The area between <m>-z^{\star}</m> and <m>z^{\star}</m> increases as
            <m>z^{\star}</m> becomes larger. If the confidence level is 99%,
            we choose <m>z^{\star}</m> such that 99% of a normal
            normal distribution is between <m>-z^{\star}</m> and <m>z^{\star}</m>,
            which corresponds to 0.5%
            in the lower tail and 0.5% in the upper tail:
            <m>z^{\star}=2.58</m>.</caption>
        <image source="images/ch_foundations_for_inf/figures/choosingZForCI/choosingZForCI.png" width="70%">
          <description>A standard normal distribution is shown, where standard is the term used to indicate that the normal distribution is centered at 0 and has a standard deviation of 1. Portions of the normal distribution have been shaded. First, the central 95% portion of the distribution has been shaded in a dark blue, and this region has an annotation stating 95%, extends from -1.96 to 1.96. Recall that the value of 1.96 closely matches our 68-95-99.7 rule for the normal distribution, which had stated that about 95% of the area under the normal distribution lied within 2 standard deviations of the mean. Second, a slightly broader region of the normal distribution is shaded, in this case from about -2.5 to positive 2.5, and this has an annotation stating, 99%, extends -2.58 to 2.58. The values described here -- 1.96 and 2.58 -- are the z-star values that we would use for 95% and 99% confidence intervals, respectively.</description>
        </image>
      </figure>
      
      <p>
        This approach -- using the Z-scores in the
        normal model to compute confidence levels --
        is appropriate when a point estimate such as <m>\hat{p}</m>
        is associated with a normal distribution.
        For some other point estimates, a normal model is not a good fit;
        in these cases, we'll use alternative distributions that better
        represent the sampling distribution.
      </p>
      
      <assemblage xml:id="def-general-ci">
        <title>Confidence interval using any confidence level</title>
        <p>
          If a point estimate closely follows a normal model
          with standard error <m>SE</m>, then a confidence interval
          for the population parameter is
        </p>
        <md>
          <mrow>\text{point estimate}\ \amp\pm\ z^{\star} \times SE</mrow>
        </md>
        <p>
          where <m>z^{\star}</m> corresponds to the confidence
          level selected.
        </p>
      </assemblage>
      
      <p>
        <xref ref="fig-choosing-z-star"/> provides a picture of how to identify
        <m>z^{\star}</m> based on a confidence level. We select <m>z^{\star}</m>
        so that the area between <m>-z^{\star}</m> and <m>z^{\star}</m> in the
        standard normal distribution,
        <m>N(0, 1)</m>, corresponds to the confidence level.
      </p>
      
      <assemblage xml:id="def-margin-of-error">
        <title>Margin of error</title>
        <p>
          In a confidence interval, <m>z^{\star}\times SE</m> is called the
          <term>margin of error</term>.
        </p>
      </assemblage>
      
      <example xml:id="ex-solar-90-ci">
        <title>90% CI for solar energy support</title>
        <statement>
          <p>
            Use the data in
            <xref ref="ex-solar-95-ci"/> to
            create a 90% confidence interval for the proportion of American
            adults that support expanding the use of solar power.
            We have already verified conditions for normality.
          </p>
        </statement>
        <solution>
          <p>
            We first find <m>z^{\star}</m> such that 90% of the distribution falls
            between <m>-z^{\star}</m> and <m>z^{\star}</m> in the
            standard normal distribution, <m>N(\mu = 0, \sigma = 1)</m>.
            We can do this using a graphing calculator,
            statistical software, or a probability table by looking for an
            upper tail of 5% (the other 5% is in the lower tail):
            <m>z^{\star}=1.65</m>.
            The 90% confidence interval can then be computed as
          </p>
          <md>
            <mrow>\hat{p}\ \amp\pm\ 1.6449 \times SE_{\hat{p}}</mrow>
            <mrow>\amp\quad\to\quad 0.887\ \pm\ 1.65 \times 0.0100</mrow>
            <mrow>\amp\quad\to\quad (0.8705, 0.9034)</mrow>
          </md>
          <p>
            That is, we are 90% confident that 87.1% to 90.3% of American
            adults supported the expansion of solar power in 2018.
          </p>
        </solution>
      </example>
    </subsection>
    
    <subsection xml:id="subsec-more-case-studies">
      <title>More Case Studies</title>
      
      <p>
        In New York City on October 23rd, 2014, a doctor who had recently been
        treating Ebola patients in Guinea went to the hospital with a slight fever
        and was subsequently diagnosed with Ebola. Soon thereafter,
        an NBC 4 New York/The Wall Street Journal/Marist Poll found that
        82% of New Yorkers favored a <q>mandatory 21-day
        quarantine for anyone who has come in contact with an Ebola
        patient</q>. This poll included responses
        of 1,042 New York adults between
        Oct 26th and 28th, 2014.
      </p>
      
      <example>
        <statement>
          <p>
            What is the point estimate in this case,
            and is it reasonable to
            use a normal distribution to model that point estimate?
          </p>
        </statement>
        <solution>
          <p>
            The point estimate, based on a sample of size <m>n = 1042</m>,
            is <m>\hat{p} = 0.82</m>.
            To check whether <m>\hat{p}</m> can be reasonably
            modeled using a normal distribution, we check independence
            (the poll is based on a simple random sample) and the
            success-failure condition
            (<m>1042 \times \hat{p} \approx 854</m>
            and <m>1042 \times (1 - \hat{p})
                \approx 188</m>,
            both easily greater than 10).
            With the conditions met, we are assured
            that the sampling distribution of <m>\hat{p}</m> can be
            reasonably modeled using a normal distribution.
          </p>
        </solution>
      </example>
      
      <example>
        <statement>
          <p>
            Estimate the standard error of
            <m>\hat{p} = 0.82</m> from the Ebola survey.
          </p>
        </statement>
        <solution>
          <p>
            We'll use the substitution approximation of
            <m>p \approx \hat{p} = 0.82</m> to compute
            the standard error:
          </p>
          <md>
            <mrow>SE_{\hat{p}}
              \amp= \sqrt{\frac{p(1-p)}{n}}</mrow>
            <mrow>\amp\approx \sqrt{\frac{0.82
                (1 - 0.82)}{1042}}</mrow>
            <mrow>\amp= 0.012</mrow>
          </md>
        </solution>
      </example>
      
      <example xml:id="ex-ci-ny-ebola-quarantine">
        <statement>
          <p>
            Construct a 95% confidence interval for <m>p</m>,
            the proportion of New York adults who supported a quarantine
            for anyone who has come into contact with an Ebola patient.
          </p>
        </statement>
        <solution>
          <p>
            Using the standard error <m>SE = 0.012</m> from
            the previous example,
            the point estimate 0.82, and <m>z^{\star} = 1.96</m>
            for a 95% confidence level, the confidence interval is
          </p>
          <md>
            <mrow>\text{point estimate} \ \amp\pm\ z^{\star} \times SE</mrow>
            <mrow>\amp\quad\to\quad 0.82 \ \pm\ 1.96\times 0.012</mrow>
            <mrow>\amp\quad\to\quad (0.796, 0.844)</mrow>
          </md>
          <p>
            We are 95% confident that the proportion of New York adults
            in October 2014 who supported a quarantine for anyone who had come
            into contact with an Ebola patient was between 0.796 and 0.844.
          </p>
        </solution>
      </example>
      
      <exercise>
        <statement>
          <p>
            Answer the following two questions about the confidence interval
            from the example above:
          </p>
          <ol>
            <li>
              What does 95% confident mean in this context?
            </li>
            <li>
              Do you think the confidence interval is still valid
              for the opinions of New Yorkers today?
            </li>
          </ol>
        </statement>
        <solution>
          <ol>
            <li>
              <p>
                If we took many such samples and computed
                a 95% confidence interval for each, then about 95% of those
                intervals would contain the actual proportion of New York
                adults who supported a quarantine for anyone who has come into
                contact with an Ebola patient.
              </p>
            </li>
            <li>
              <p>
                Not necessarily. The poll was taken at a
                time where there was a huge public safety concern.
                Now that people have had some time to step back,
                they may have changed their opinions.
                We would need to run a new poll if we wanted to get an
                estimate of the current proportion of New York adults who
                would support such a quarantine period.
              </p>
            </li>
          </ol>
        </solution>
      </exercise>
      
      <exercise xml:id="ex-pew-wind-turbine-support">
        <statement>
          <p>
            In the Pew Research poll about solar energy, they
            also inquired about other forms of energy,
            and 84.8% of the 1,000
            respondents supported expanding the use of wind
            turbines.
          </p>
          <ol>
            <li>
              Is it reasonable to model the proportion
              of US adults who support expanding wind turbines
              using a normal distribution?
            </li>
            <li>
              Create a 99% confidence interval for the level of American
              support for expanding the use of wind turbines for power
              generation.
            </li>
          </ol>
        </statement>
        <solution>
          <ol>
            <li>
              <p>
                The survey was a random sample
                and counts are both <m>\geq 10</m>
                (<m>1000 \times 0.848
                  = 848</m>
                and <m>1000 \times 0.152
                  = 152</m>),
                so independence and the success-failure condition
                are satisfied, and
                <m>\hat{p} = 0.848</m> can be
                modeled using a normal distribution.
              </p>
            </li>
            <li>
              <p>
                We already confirmed that <m>\hat{p}</m> closely follows
                a normal distribution, so we can use the C.I. formula:
              </p>
              <md>
                <mrow>\text{point estimate} \amp\pm z^{\star} \times SE</mrow>
              </md>
              <p>
                In this case, the point estimate is
                <m>\hat{p} = 0.848</m>.
                For a 99% confidence interval, <m>z^{\star} = 2.58</m>.
                Computing the standard error:
                <m>SE_{\hat{p}}
                  = \sqrt{\frac{0.848(1 - 0.848)}
                      {1000}}
                  = 0.0114</m>.
                Finally, we compute the interval as
                <m>0.848 \pm 2.58 \times 0.0114
                  \to (0.8186,   0.8774)</m>.
                It is also important to <em>always</em> provide an interpretation
                for the interval: we are 99% confident the proportion of
                American adults that support expanding the use of wind
                turbines in 2018 is between 81.9% and 87.7%.
              </p>
            </li>
          </ol>
        </solution>
      </exercise>
      
      <p>
        We can also construct confidence intervals for other
        parameters, such as a population mean.
        In these cases, a confidence interval would be computed
        in a similar way to that of a single proportion:
        a point estimate plus/minus some margin of error.
        We'll dive into these details in later chapters.
      </p>
      
      <assemblage xml:id="ci-procedure">
        <title>Confidence interval for a single proportion</title>
        <p>
          Once you've determined a one-proportion confidence interval would be helpful for an application, there are four steps to constructing the interval:
          <dl>
            <li>
              <title>Prepare.</title>
              <p>Identify <m>\hat{p}</m> and <m>n</m>, and determine what confidence level you wish to use.</p>
            </li>
            <li>
              <title>Check.</title>
              <p>Verify the conditions to ensure <m>\hat{p}</m> is nearly normal. For one-proportion confidence intervals, use <m>\hat{p}</m> in place of <m>p</m> to check the success-failure condition.</p>
            </li>
            <li>
              <title>Calculate.</title>
              <p>If the conditions hold, compute <m>SE</m> using <m>\hat{p}</m>, find <m>z^{\star}</m>, and construct the interval.</p>
            </li>
            <li>
              <title>Conclude.</title>
              <p>Interpret the confidence interval in the context of the problem.</p>
            </li>
          </dl>
        </p>
      </assemblage>
    </subsection>
    
    <subsection xml:id="subsec-interpreting-cis">
      <title>Interpreting Confidence Intervals</title>
      
      <p>
        In each of the examples, we described the confidence
        intervals by putting them into the context of the data and also
        using somewhat formal language:
      </p>
      <dl>
        <li>
          <title>Solar.</title>
          <p>
            We are 90% confident that 87.1% to 90.4% of
            American adults support the expansion of solar power in 2018.
          </p>
        </li>
        <li>
          <title>Ebola.</title>
          <p>
            We are 95% confident that the proportion
            of New York adults in October 2014 who supported a quarantine
            for anyone who had come into contact with an Ebola patient was
            between 0.796 and 0.844.
          </p>
        </li>
        <li>
          <title>Wind Turbine.</title>
          <p>
            We are 99% confident the proportion of
            Americans adults that support expanding the use of wind
            turbines is between 81.9% and 87.7% in 2018.
          </p>
        </li>
      </dl>
      <p>
        First, notice that the statements are always about the population
        parameter, which considers <em>all</em> American adults for the
        energy polls or <em>all</em> New York adults for the quarantine poll.
      </p>
      
      <p>
        We also avoided another common mistake:
        <em>incorrect</em> language might try to describe the confidence interval
        as capturing the population parameter with a certain probability.
        Making a probability interpretation is a common error:
        while it might be useful to think of it as a probability,
        the confidence level only quantifies how plausible
        it is that the parameter is in the given interval.
      </p>
      
      <p>
        Another important consideration of confidence intervals is that they
        are <em>only about the population parameter</em>.
        A confidence interval says nothing about individual
        observations or point estimates.
        Confidence intervals only provide a plausible range for
        population parameters.
      </p>
      
      <p>
        Lastly, keep in mind the methods we discussed only apply
        to sampling error, not to bias.
        If a data set is collected in a way that will tend to
        systematically under-estimate
        (or over-estimate) the population parameter, the techniques
        we have discussed will not address that problem.
        Instead, we rely on careful data collection procedures to
        help protect against bias in the examples we have considered,
        which is a common practice employed by data scientists
        to combat bias.
      </p>
      
      <exercise>
        <statement>
          <p>
            Consider the 90% confidence interval for the solar
            energy survey: 87.1% to 90.4%.
            If we ran the survey again, can we say that we're
            90% confident that the new survey's proportion
            will be between 87.1% and 90.4%?
          </p>
        </statement>
        <solution>
          <p>
            No, a confidence interval only provides a range of plausible
            values for a parameter,
            not future point estimates.
          </p>
        </solution>
      </exercise>
    </subsection>
    
    <exercises>
      <title>Section 5.2 Exercises</title>

      <exercise xml:id="ex-chronic-illness-intro">
       <title>Chronic illness, Part I.</title>
        <statement>
          <p>
            In 2013, the Pew Research Foundation reported that "45% of U.S. adults report
            that they live with one or more chronic conditions". However, this value was
            based on a sample, so it may not be a perfect estimate for the population
            parameter of interest on its own. The study reported a standard error of
            about 1.2%, and a normal model may reasonably be used in this setting. Create
            a 95% confidence interval for the proportion of U.S. adults who live with one
            or more chronic conditions. Also interpret the confidence interval in the
            context of the study.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-twitter-users-intro">
       <title>Twitter users and news. Part I.</title>
        <statement>
          <p>
            A poll conducted in 2013 found that 52% of U.S. adult Twitter users get at
            least some news on Twitter. The standard error for this estimate was 2.4%,
            and a normal distribution may be used to model the sample proportion.
            Construct a 99% confidence interval for the fraction of U.S. adult Twitter
            users who get some news on Twitter, and interpret the confidence interval in
            context.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-chronic-illness-tf">
       <title>Chronic illness, Part II.</title>
        <statement>
          <p>
            In 2013, the Pew Research Foundation reported that "45% of U.S. adults report
            that they live with one or more chronic conditions", and the standard error
            for this estimate is 1.2%. Identify each of the following statements as true
            or false. Provide an explanation to justify each of your answers.
          </p>
          <ol>
            <li>We can say with certainty that the confidence interval from the chronic
                illness exercise contains the true percentage of U.S. adults who suffer
                from a chronic illness.</li>
            <li>If we repeated this study 1,000 times and constructed a 95% confidence
                interval for each study, then approximately 950 of those confidence
                intervals would contain the true fraction of U.S. adults who suffer from
                chronic illnesses.</li>
            <li>The poll provides statistically significant evidence (at the
                <m>\alpha = 0.05</m> level) that the percentage of U.S. adults who
                suffer from chronic illnesses is below 50%.</li>
            <li>Since the standard error is 1.2%, only 1.2% of people in the study
                communicated uncertainty about their answer.</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-twitter-users-tf">
       <title>Twitter users and news, Part II.</title>
        <statement>
          <p>
            A poll conducted in 2013 found that 52% of U.S. adult Twitter users get at
            least some news on Twitter, and the standard error for this estimate was
            2.4%. Identify each of the following statements as true or false. Provide an
            explanation to justify each of your answers.
          </p>
          <ol>
            <li>The data provide statistically significant evidence that more than half
                of U.S. adult Twitter users get some news through Twitter. Use a
                significance level of <m>\alpha = 0.01</m>.
                (This part uses concepts from <xref ref="sec-hypothesis-testing"/> and will be
                corrected in a future edition.)</li>
            <li>Since the standard error is 2.4%, we can conclude that 97.6% of all
                U.S. adult Twitter users were included in the study.</li>
            <li>If we want to reduce the standard error of the estimate, we should
                collect less data.</li>
            <li>If we construct a 90% confidence interval for the percentage of U.S.
                adult Twitter users who get some news through Twitter, this confidence
                interval will be wider than a corresponding 99% confidence interval.</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-er-wait-intro">
       <title>Waiting at an ER, Part I.</title>
        <statement>
          <p>
            A hospital administrator hoping to improve wait times decides to estimate the
            average emergency room waiting time at her hospital. She collects a simple
            random sample of 64 patients and determines the time (in minutes) between
            when they checked in to the ER until they were first seen by a doctor. A 95%
            confidence interval based on this sample is (128 minutes, 147 minutes), which
            is based on the normal model for the mean. Determine whether the following
            statements are true or false, and explain your reasoning.
          </p>
          <ol>
            <li>We are 95% confident that the average waiting time of these 64 emergency
                room patients is between 128 and 147 minutes.</li>
            <li>We are 95% confident that the average waiting time of all patients at
                this hospital's emergency room is between 128 and 147 minutes.</li>
            <li>95% of random samples have a sample mean between 128 and 147 minutes.</li>
            <li>A 99% confidence interval would be narrower than the 95% confidence
                interval since we need to be more sure of our estimate.</li>
            <li>The margin of error is 9.5 and the sample mean is 137.5.</li>
            <li>In order to decrease the margin of error of a 95% confidence interval to
                half of what it is now, we would need to double the sample size.
                (Hint: the margin of error for a mean scales in the same way with sample size
                as the margin of error for a proportion.)</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-mental-health">
       <title>Mental health.</title>
        <statement>
          <p>
            The General Social Survey asked the question: "For how many days during the
            past 30 days was your mental health, which includes stress, depression, and
            problems with emotions, not good?" Based on responses from 1,151 US
            residents, the survey reported a 95% confidence interval of 3.40 to 4.24 days
            in 2010.
          </p>
          <ol>
            <li>Interpret this interval in context of the data.</li>
            <li>What does "95% confident" mean? Explain in the context of the
                application.</li>
            <li>Suppose the researchers think a 99% confidence level would be more
                appropriate for this interval. Will this new interval be smaller or wider
                than the 95% confidence interval?</li>
            <li>If a new survey were to be done with 500 Americans, do you think the
                standard error of the estimate would be larger, smaller, or about the same.</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-website-registration">
       <title>Website registration</title>
        <statement>
          <p>
            A website is trying to increase registration for first-time visitors,
            exposing 1% of these visitors to a new site design. Of 752 randomly sampled
            visitors over a month who saw the new design, 64 registered.
          </p>
          <ol>
            <li>Check any conditions required for constructing a confidence
                interval.</li>
            <li>Compute the standard error.</li>
            <li>Construct and interpret a 90% confidence interval for the fraction of
                first-time visitors of the site who would register under the new design
                (assuming stable behaviors by new visitors over time).</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-store-coupon">
       <title>Coupons driving visits</title>
        <statement>
          <p>
            A store randomly samples 603 shoppers over the course of a year and finds
            that 142 of them made their visit because of a coupon they'd received in the
            mail. Construct a 95% confidence interval for the fraction of all shoppers
            during the year whose visit was because of a coupon they'd received in the
            mail.
          </p>
        </statement>
      </exercise>
    </exercises>
  </section>

  <!-- Section 5.3: Hypothesis testing for a proportion -->
  <section xml:id="sec-hypothesis-testing">
    <title>Hypothesis Testing for a Proportion</title>
    
    <introduction>
      <p>
        The following question comes from a book written by
        Hans Rosling, Anna Rosling Rnnlund, and Ola Rosling
        called <em>Factfulness</em>:
      </p>
      <blockquote>
        <p>
          <em>How many of the world's 1 year old children today
          have been vaccinated against some disease:</em>
        </p>
        <ol marker="a.">
          <li>20%</li>
          <li>50%</li>
          <li>80%</li>
        </ol>
      </blockquote>
      <p>
        Write down what your answer (or guess),
        and when you're ready, find the answer in the
        footnote.<fn>The correct answer is (c):
        80% of the world's 1 year olds have been vaccinated
        against some disease.</fn>
      </p>
      
      <p>
        In this section,
        we'll be exploring how people with a 4-year college
        degree perform on this and other world health questions
        as we learn about hypothesis tests, which are
        a framework used to rigorously evaluate competing
        ideas and claims.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-ht-framework">
      <title>Hypothesis testing framework</title>
      
      <p>
        We're interested in understanding how much people know
        about world health and development.
        If we take a multiple choice
        world health question, then we might like to understand if
      </p>
      <dl>
        <li>
          <title><m>\mathbf{H_0}</m>:</title>
          <p>
            People never learn these particular topics and their
            responses are simply equivalent to random guesses.
          </p>
        </li>
        <li>
          <title><m>\mathbf{H_A}</m>:</title>
          <p>
            People have knowledge that helps them do better
            than random guessing, or perhaps, they have false knowledge
            that leads them to actually do worse than random guessing.
          </p>
        </li>
      </dl>
      <p>
        These competing ideas are called <term>hypotheses</term>.
        We call <m>H_0</m> the null hypothesis and <m>H_A</m> the alternative
        hypothesis.
        When there is a subscript 0 like in <m>H_0</m>,
        data scientists pronounce it as <q>nought</q>
        (e.g. <m>H_0</m> is pronounced <q>H-nought</q>).
      </p>
      
      <assemblage xml:id="def-null-alternative-hypotheses">
        <title>Null and alternative hypotheses</title>
        <p>
          The <term>null hypothesis (<m>H_0</m>)</term> often represents
          a skeptical perspective or a claim to be tested.
          The <term>alternative hypothesis (<m>H_A</m>)</term> represents an
          alternative claim under consideration and is often
          represented by a range of possible parameter values.
        </p>
        <p>
          Our job as data scientists is to play the role of a skeptic:
          before we buy into the alternative hypothesis, we need to
          see strong supporting evidence.
        </p>
      </assemblage>
      
      <p>
        The null hypothesis often represents a skeptical position
        or a perspective of <q>no difference</q>.
        In our first example, we'll consider whether
        the typical person does any different than random guessing
        on Roslings' question about infant vaccinations.
      </p>
      
      <p>
        The alternative hypothesis generally represents a new
        or stronger perspective. In the case of the question
        about infant vaccinations,
        it would certainly be interesting to learn whether
        people do better than random guessing, since that would
        mean that the typical person knows something about
        world health statistics.
        It would also be very interesting if we learned
        that people do <em>worse</em> than random guessing,
        which would suggest people believe
        incorrect information about world health.
      </p>
      
      <p>
        The hypothesis testing framework is a very general tool, and we often use it without a second thought. If a person makes a somewhat unbelievable claim, we are initially skeptical. However, if there is sufficient evidence that supports the claim, we set aside our skepticism and reject the null hypothesis in favor of the alternative. The hallmarks of hypothesis testing are also found in the US court system.
      </p>
      
      <exercise>
        <statement>
          <p>
            A US court considers two possible claims about a defendant: she is either innocent or guilty. If we set these claims up in a hypothesis framework, which would be the null hypothesis and which the alternative?
          </p>
        </statement>
        <solution>
          <p>
            The jury considers whether the evidence is so
            convincing (strong) that there is no reasonable doubt
            regarding the person's guilt;
            in such a case, the jury rejects innocence
            (the null hypothesis) and concludes the defendant
            is guilty (alternative hypothesis).
          </p>
        </solution>
      </exercise>
      
      <p>
        Jurors examine the evidence to see whether it convincingly
        shows a defendant is guilty.
        Even if the jurors leave unconvinced of guilt beyond
        a reasonable doubt, this does not mean they believe the
        defendant is innocent.
        This is also the case with hypothesis testing:
        <em>even if we fail to reject the null hypothesis,
        we typically do not accept the null hypothesis as true</em>.
        Failing to find strong evidence for the alternative
        hypothesis is not equivalent to accepting
        the null hypothesis.
      </p>
      
      <p>
        When considering Roslings' question about infant vaccination,
        the null hypothesis represents the notion that the people
        we will be considering -- college-educated adults --
        are as accurate as random guessing.
        That is, the proportion
        <m>p</m> of respondents who pick the correct
        answer, that 80% of 1 year olds have been vaccinated
        against some disease, is about 33.3%
        (or 1-in-3 if wanting to be perfectly precise).
        The alternative hypothesis is that this proportion is something
        other than 33.3%. While it's helpful to write these hypotheses
        in words, it can be useful to write them using mathematical
        notation:
      </p>
      <dl>
        <li>
          <title><m>H_0</m>:</title>
          <p><m>p = 0.333</m></p>
        </li>
        <li>
          <title><m>H_A</m>:</title>
          <p><m>p \neq 0.333</m></p>
        </li>
      </dl>
      <p>
        In this hypothesis setup, we want to make a conclusion about
        the population parameter <m>p</m>. The value we are comparing the
        parameter to is called the <term>null value</term>, which in this
        case is 0.333. It's common to label the null value with the
        same symbol as the parameter but with a subscript `0'.
        That is, in this case, the null value is <m>p_0 = 0.333</m>
        (pronounced <q>p-nought equals 0.333</q>).
      </p>
      
      <example>
        <statement>
          <p>
            It may seem impossible that the
            proportion of people who get the correct answer
            is <em>exactly</em> 33.3%. If we don't believe the
            null hypothesis, should we simply reject it?
          </p>
        </statement>
        <solution>
          <p>
            No. While we may not buy into the notion that
            the proportion is exactly 33.3%, the hypothesis testing
            framework requires that there be strong evidence before
            we reject the null hypothesis and conclude something
            more interesting.
          </p>
          
          <p>
            After all, even if we don't believe the proportion is
            <em>exactly</em> 33.3%, that doesn't really tell us anything
            useful! We would still be stuck with the original question:
            do people do better or worse than random guessing on
            Roslings' question?
            Without data that strongly
            points in one direction or the other, it is both
            uninteresting and pointless to reject <m>H_0</m>.
          </p>
        </solution>
      </example>
      
      <exercise>
        <statement>
          <p>
            Another example of a real-world hypothesis testing situation
            is evaluating whether a new drug is better or worse
            than an existing drug at treating a particular disease.
            What should we use for the null and alternative hypotheses in
            this case?
          </p>
        </statement>
        <solution>
          <p>
            The null hypothesis (<m>H_0</m>) in this case is
            the declaration of <em>no difference</em>: the drugs are equally
            effective. The alternative hypothesis (<m>H_A</m>) is that the
            new drug performs differently than the original,
            i.e. it could perform better or worse.
          </p>
        </solution>
      </exercise>
    </subsection>
    
    <subsection xml:id="subsec-ht-confidence-intervals">
      <title>Testing hypotheses using confidence intervals</title>
      
      <p>
        We will use the Rosling responses
        data set to evaluate
        the hypothesis test evaluating whether college-educated adults
        who get the question about infant vaccination correct is different
        from 33.3%.
        This data set summarizes the answers of 50
        college-educated adults.
        Of these 50 adults, 24% of
        respondents got the question correct that 80% of 1 year olds
        have been vaccinated against some disease.
      </p>
      
      <p>
        Up until now, our discussion has been philosophical.
        However, now that we have data, we might ask ourselves:
        does the data provide strong evidence that the proportion
        of all college-educated adults who would answer this
        question correctly is different than 33.3%?
      </p>
      
      <p>
        We learned in <xref ref="sec-point-estimates"/> that there is
        fluctuation from one sample to another, and it is unlikely
        that our sample proportion, <m>\hat{p}</m>,
        will exactly equal <m>p</m>, but we want to make
        a conclusion about <m>p</m>.
        We have a nagging concern:
        is this deviation of 24%
        from 33.3% simply due to chance,
        or does the data provide strong evidence that the
        population proportion is different from 33.3%?
      </p>
      
      <p>
        In <xref ref="sec-confidence-intervals"/>, we learned how to
        quantify the uncertainty in our estimate using confidence
        intervals.
        The same method for measuring variability can be useful
        for the hypothesis test.
      </p>
      
      <example>
        <statement>
          <p>
            Check whether it is reasonable to construct
            a confidence interval for <m>p</m> using the sample data, and
            if so, construct a 95% confidence interval.
          </p>
        </statement>
        <solution>
          <p>
            The conditions are met for <m>\hat{p}</m> to be approximately
            normal: the data come from a simple random sample (satisfies
            independence), and <m>n\hat{p} = 12</m> and
            <m>n(1 - \hat{p}) = 38</m> are both
            at least 10 (success-failure condition).
          </p>
          
          <p>
            To construct the confidence interval, we will need to identify
            the point estimate (<m>\hat{p} = 0.24</m>),
            the critical value for
            the 95% confidence level (<m>z^{\star} = 1.96</m>), and the standard
            error of <m>\hat{p}</m>
            (<m>SE_{\hat{p}} = \sqrt{\hat{p}(1 - \hat{p}) / n} = 0.060</m>).
            With those pieces, the confidence interval for <m>p</m> can be
            constructed:
          </p>
          <md>
            <mrow>\amp\hat{p} \pm z^{\star} \times SE_{\hat{p}}</mrow>
            <mrow>\amp 0.24 \pm 1.96 \times 0.060</mrow>
            <mrow>\amp (0.122, 0.358)</mrow>
          </md>
          <p>
            We are 95% confident that the proportion of all
            college-educated adults to correctly answer this
            particular question about infant vaccination is between
            12.2% and 35.8%.
          </p>
        </solution>
      </example>
      
      <p>
        Because the null value in the hypothesis test is <m>p_0 = 0.333</m>,
        which falls within the range of plausible values from the
        confidence interval, we cannot say the null value is
        implausible.<fn>Arguably this method is slightly imprecise.
        As we'll see in a few pages, the standard error is often
        computed slightly differently in the context of a hypothesis
        test for a proportion.</fn>
        That is, the data do not provide sufficient evidence to reject
        the notion that the performance of college-educated
        adults was different than random guessing,
        and we do not reject the null hypothesis, <m>H_0</m>.
      </p>
      
      <example>
        <statement>
          <p>
            Explain why we cannot conclude that
            college-educated adults simply guessed on the
            infant vaccination question.
          </p>
        </statement>
        <solution>
          <p>
            While we failed to reject <m>H_0</m>, that does not
            necessarily mean the null hypothesis is true.
            Perhaps there was an actual difference,
            but we were not able to detect it with the
            relatively small sample of 50.
          </p>
        </solution>
      </example>
      
      <assemblage>
        <title>Double negatives can sometimes be used in statistics</title>
        <p>
          In many statistical explanations, we use double negatives.
          For instance, we might say that the null hypothesis is
          <em>not implausible</em> or we <em>failed to reject</em>
          the null hypothesis.
          Double negatives are used to communicate that while we
          are not rejecting a position, we are also not saying it
          is correct.
        </p>
      </assemblage>
      
      <exercise>
        <statement>
          <p>
            Let's move onto a second question posed by the Roslings:
          </p>
          <blockquote>
            <p>
              <em>There are 2 billion children in the world today
              aged 0-15 years old, how many children will there
              be in year 2100 according to the United Nations?</em>
            </p>
            <ol marker="a.">
              <li>4 billion.</li>
              <li>3 billion.</li>
              <li>2 billion.</li>
            </ol>
          </blockquote>
          <p>
            Set up appropriate hypotheses to evaluate whether
            college-educated adults are better than random guessing
            on this question.
            Also, see if you can guess the correct answer before checking
            the answer in the solution!
          </p>
        </statement>
        <solution>
          <p>
            The appropriate hypotheses are:
          </p>
          <p>
            <m>H_0</m>: the proportion who get the answer correct is the same
            as random guessing: 1-in-3, or <m>p = 0.333</m>.
          </p>
          <p>
            <m>H_A</m>: the proportion who get the answer correct is different
            than random guessing, <m>p \neq 0.333</m>.
          </p>
          <p>
            The correct answer to the question is 2 billion.
            While the world population is projected to increase,
            the average age is also expected to rise.
            That is, the majority of the population growth will
            happen in older age groups, meaning people are projected
            to live longer in the future across much of the world.
          </p>
        </solution>
      </exercise>
      
      <exercise>
        <statement>
          <p>
            This time we took a larger sample of
            228 college-educated adults,
            34 (14.9%) selected the correct
            answer to the question above: 2 billion.
            Can we model the sample proportion using a normal distribution
            and construct a confidence interval?
          </p>
        </statement>
        <solution>
          <p>
            We check both conditions, which are satisfied,
            so it is reasonable to use
            a normal distribution for <m>\hat{p}</m>:
          </p>
          <p>
            <strong>Independence.</strong> Since the data are from a simple
            random sample, the observations are independent.
          </p>
          <p>
            <strong>Success-failure.</strong> We'll use <m>\hat{p}</m> in place of <m>p</m>
            to check: <m>n\hat{p} = 34</m>
            and <m>n(1 - \hat{p}) = 194</m>.
            Both are greater than 10, so the success-failure condition
            is satisfied.
          </p>
        </solution>
      </exercise>
      
      <example>
        <statement>
          <p>
            Compute a 95% confidence interval for the
            fraction of college-educated adults who answered the
            children-in-2100 question correctly, and evaluate the
            hypotheses from the previous exercise.
          </p>
        </statement>
        <solution>
          <p>
            To compute the standard error, we'll again use
            <m>\hat{p}</m>
            in place of <m>p</m> for the calculation:
          </p>
          <md>
            <mrow>SE_{\hat{p}}
                \amp= \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}</mrow>
            <mrow>\amp= \sqrt{\frac{0.149(1 - 0.149)}
                {228}}</mrow>
            <mrow>\amp= 0.024</mrow>
          </md>
          <p>
            In the previous exercise,
            we found that <m>\hat{p}</m> can be modeled using
            a normal distribution,
            which ensures a 95% confidence interval may be accurately
            constructed as
          </p>
          <md>
            <mrow>\hat{p}\ \amp\pm\ z^{\star} \times SE</mrow>
            <mrow>\amp\quad\to\quad
            0.149\ \pm\ 1.96 \times 0.024</mrow>
            <mrow>\amp\quad\to\quad
            (0.103, 0.195)</mrow>
          </md>
          <p>
            Because the null value, <m>p_0 = 0.333</m>, is not in the
            confidence interval, a population proportion of 0.333
            is implausible and we reject the null hypothesis.
            That is, the data provide statistically significant
            evidence that the actual proportion of college adults
            who get the children-in-2100 question correct is
            different from random guessing. Because the entire
            95% confidence interval
            is below 0.333, we can conclude college-educated adults
            do <em>worse</em> than random guessing on this question.
          </p>
          
          <p>
            One subtle consideration is that we used a
            95% confidence interval.
            What if we had used a 99% confidence level?
            Or even a 99.9% confidence level?
            It's possible to come to a different conclusion
            if using a different confidence level.
            Therefore, when we make a conclusion based
            on confidence interval, we should also be sure
            it is clear what confidence level we used.
          </p>
        </solution>
      </example>
      
      <p>
        The worse-than-random performance on this
        last question is not a fluke:
        there are many such world health questions where people
        do worse than random guessing.
        In general, the answers suggest that people tend to be
        more pessimistic about progress than reality suggests.
        This topic is discussed in much greater detail in
        the Roslings' book,
        <em>Factfulness</em>.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-decision-errors">
      <title>Decision errors</title>
      
      <p>
        Hypothesis tests are not flawless: we can make an incorrect
        decision in a statistical hypothesis test based on the data.
        For example, in the court system innocent people are
        sometimes wrongly convicted and the guilty sometimes walk free.
        One key distinction with statistical hypothesis tests is that
        we have the tools necessary to probabilistically quantify how
        often we make errors in our conclusions.
      </p>
      
      <p>
        Recall that there are two competing hypotheses:
        the null and the alternative.
        In a hypothesis test, we make a statement about which one might
        be true, but we might choose incorrectly. There are four possible
        scenarios, which are summarized in <xref ref="table-decision-errors"/>.
      </p>
      
      <table xml:id="table-decision-errors">
        <title>Four different scenarios for hypothesis tests.</title>
        <tabular>
          <row>
            <cell></cell>
            <cell></cell>
            <cell colspan="2" halign="center"><strong>Test conclusion</strong></cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell></cell>
            <cell>do not reject <m>H_0</m></cell>
            <cell>reject <m>H_0</m> in favor of <m>H_A</m></cell>
          </row>
          <row>
            <cell></cell>
            <cell><m>H_0</m> true</cell>
            <cell>okay</cell>
            <cell><em>Type 1 Error</em></cell>
          </row>
          <row>
            <cell><strong>Truth</strong></cell>
            <cell><m>H_A</m> true</cell>
            <cell><em>Type 2 Error</em></cell>
            <cell>okay</cell>
          </row>
        </tabular>
      </table>
      
      <p>
        A <term>Type 1 Error</term> is rejecting the null hypothesis when
        <m>H_0</m> is actually true.
        A <term>Type 2 Error</term> is failing to
        reject the null hypothesis when the alternative is actually
        true.
      </p>
      
      <exercise>
        <statement>
          <p>
            In a US court, the defendant is either innocent (<m>H_0</m>) or
            guilty (<m>H_A</m>).
            What does a Type 1 Error represent in this context?
            What does a Type 2 Error represent?
            <xref ref="table-decision-errors"/> may be useful.
          </p>
        </statement>
        <solution>
          <p>
            If
            the court makes a Type 1 Error, this means the defendant
            is innocent (<m>H_0</m> true) but wrongly convicted.
            Note that a Type 1 Error is only possible if we've rejected
            the null hypothesis.
          </p>
          
          <p>
            A Type 2 Error means the court failed to reject <m>H_0</m>
            (i.e. failed to convict the person) when she was
            in fact guilty (<m>H_A</m> true).
            Note that a Type 2 Error is only possible if we have
            failed to reject the null hypothesis.
          </p>
        </solution>
      </exercise>
      
      <example>
        <statement>
          <p>
            How could we reduce the Type 1 Error rate
            in US courts?
            What influence would this have on the Type 2 Error rate?
          </p>
        </statement>
        <solution>
          <p>
            To lower the Type 1 Error rate, we might
            raise our standard for conviction from
            <q>beyond a reasonable doubt</q> to
            <q>beyond a conceivable doubt</q> so fewer people would
            be wrongly convicted. However, this would also make
            it more difficult to convict the people who are
            actually guilty, so we would make more Type 2 Errors.
          </p>
        </solution>
      </example>
      
      <exercise>
        <statement>
          <p>
            How could we reduce the Type 2 Error rate in US courts?
            What influence would this have on the Type 1 Error
            rate?
          </p>
        </statement>
        <solution>
          <p>
            To lower the Type 2 Error rate, we want
            to convict more guilty people. We could lower the
            standards for conviction from <q>beyond a reasonable
            doubt</q> to <q>beyond a little doubt</q>. Lowering the bar
            for guilt will also result in more wrongful convictions,
            raising the Type 1 Error rate.
          </p>
        </solution>
      </exercise>
      
      <p>
        The previous exercises provide
        an important lesson: if we reduce how often we make
        one type of error, we generally make more of the
        other type.
      </p>
      
      <p>
        Hypothesis testing is built around rejecting or failing
        to reject the null hypothesis.
        That is, we do not reject <m>H_0</m> unless we have strong evidence.
        But what precisely does <em>strong evidence</em> mean?
        As a general rule of thumb, for those cases where the null
        hypothesis is actually true, we do not want to incorrectly
        reject <m>H_0</m> more than 5% of the time.
        This corresponds to a <term>significance level</term>
        of 0.05.
        That is, if the null hypothesis is true,
        the significance level indicates how often
        the data lead us to incorrectly reject <m>H_0</m>.
        We often write the significance level using <m>\alpha</m>
        (the Greek letter <em>alpha</em>):
        <m>\alpha = 0.05</m>.
        We discuss the appropriateness of different significance
        levels in <xref ref="subsec-significance-level"/>.
      </p>
      
      <p>
        If we use a 95% confidence interval to evaluate a
        hypothesis test and the null hypothesis happens to be true,
        we will make an error whenever the point estimate is
        at least 1.96 standard errors away from the population
        parameter.
        This happens about 5% of the time (2.5% in each tail).
        Similarly, using a 99% confidence interval to evaluate
        a hypothesis is equivalent to a significance level of
        <m>\alpha = 0.01</m>.
      </p>
      
      <p>
        A confidence interval is very helpful in determining
        whether or not to reject the null hypothesis.
        However, the confidence interval approach isn't always
        sustainable.
        In several sections, we will encounter situations where
        a confidence interval cannot be constructed.
        For example, if we wanted to evaluate the hypothesis
        that several proportions are equal, it isn't clear how
        to construct and compare many confidence intervals
        altogether.
      </p>
      
      <p>
        Next we will introduce a statistic called the <em>p-value</em>
        to help us expand our statistical toolkit, which will
        enable us to both better understand the strength of
        evidence and work in more complex data scenarios in
        later sections.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-p-values">
      <title>Formal Testing Using P-values</title>
      
      <p>
        The p-value is a way of quantifying the strength of the evidence against the null
        hypothesis and in favor of the alternative hypothesis. Statistical hypothesis
        testing typically uses the p-value method rather than making a decision based on
        confidence intervals.
      </p>
      
      <definition xml:id="def-p-value">
        <title>P-value</title>
        <statement>
          <p>
            The <term>p-value</term> is the probability of observing data at least as
            favorable to the alternative hypothesis as our current data set, if the null
            hypothesis were true. We typically use a summary statistic of the data, in
            this section the sample proportion, to help compute the p-value and evaluate
            the hypotheses.
          </p>
        </statement>
      </definition>
      
      <example xml:id="ex-coal-hypotheses">
        <title>Setting up hypotheses for coal energy support</title>
        <statement>
          <p>
            Pew Research asked a random sample of 1000 American adults whether they
            supported the increased usage of coal to produce energy. Set up hypotheses to
            evaluate whether a majority of American adults support or oppose the increased
            usage of coal.
          </p>
        </statement>
        <solution>
          <p>
            The uninteresting result is that there is no majority either way: half of
            Americans support and the other half oppose expanding the use of coal to
            produce energy. The alternative hypothesis would be that there is a majority
            support or oppose (though we do not known which one!) expanding the use of
            coal. If <m>p</m> represents the proportion supporting, then we can write the
            hypotheses as
            <md>
              <mrow>H_0: \amp\  p = 0.5</mrow>
              <mrow>H_A: \amp\  p \neq 0.5</mrow>
            </md>
            In this case, the null value is <m>p_0 = 0.5</m>.
          </p>
        </solution>
      </example>
      
      <p>
        When evaluating hypotheses for proportions using the p-value method, we will
        slightly modify how we check the success-failure condition and compute the
        standard error for the single proportion case. These changes aren't dramatic, but
        pay close attention to how we use the null value, <m>p_0</m>.
      </p>
      
      <example xml:id="ex-coal-null-distribution">
        <title>Constructing the null distribution for coal energy support</title>
        <statement>
          <p>
            Pew Research's sample show that 37% of American adults support increased usage
            of coal. We now wonder, does 37% represent a real difference from the null
            hypothesis of 50%? What would the sampling distribution of <m>\hat{p}</m> look
            like if the null hypothesis were true?
          </p>
        </statement>
        <solution>
          <p>
            If the null hypothesis were true, the population proportion would be the null
            value, 0.5. We previously learned that the sampling distribution of
            <m>\hat{p}</m> will be normal when two conditions are met:
          </p>
          <p>
            <dl>
              <li>
                <title>Independence.</title>
                <p>
                  The poll was based on a simple random sample, so independence is
                  satisfied.
                </p>
              </li>
              <li>
                <title>Success-failure.</title>
                <p>
                  Based on the poll's sample size of <m>n = 1000</m>, the success-failure
                  condition is met, since
                  <md>
                    <mrow>np ~ \stackrel{H_0}{=} ~ 1000 \times 0.5 = 500 \qquad\qquad
                          n(1-p) ~ \stackrel{H_0}{=} ~ 1000 \times (1-0.5) = 500</mrow>
                  </md>
                  are both at least 10. Note that the success-failure condition was
                  checked using the null value, <m>p_0 = 0.5</m>; this is the first
                  procedural difference from confidence intervals.
                </p>
              </li>
            </dl>
          </p>
          <p>
            If the null hypothesis were true, the sampling distribution indicates that a
            sample proportion based on <m>n = 1000</m> observations would be normally
            distributed. Next, we can compute the standard error, where we will again use
            the null value <m>p_0 = 0.5</m> in the calculation:
            <me>
              SE_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}} \quad \stackrel{H_0}{=} \quad
              \sqrt{\frac{0.5 \times (1-0.5)}{1000}} = 0.016
            </me>
            This marks the other procedural difference from confidence intervals: since
            the sampling distribution is determined under the null proportion, the null
            value <m>p_0</m> was used for the proportion in the calculation rather than
            <m>\hat{p}</m>.
          </p>
          <p>
            Ultimately, if the null hypothesis were true, then the sample proportion
            should follow a normal distribution with mean 0.5 and a standard error of
            0.016. This distribution is shown in <xref ref="fig-coal-null-distribution"/>.
          </p>
        </solution>
      </example>
      
      <figure xml:id="fig-coal-null-distribution">
        <caption>If the null hypothesis were true, this normal distribution describes the distribution of <m>\hat{p}</m>.</caption>
        <image source="images/ch_foundations_for_inf/figures/normal_dist_mean_500_se_016/normal_dist_mean_500_se_016.png" width="70%">
          <description>A normal distribution centered at 0.5 with a standard deviation of 0.016 is shown. Additionally, an annotation is located at 0.37 that states, "Observed p-hat equals 0.37".</description>
        </image>
      </figure>
      
      <assemblage>
        <title>Checking success-failure and computing <m>SE_{\hat{p}}</m> for a hypothesis test</title>
        <p>
          When using the p-value method to evaluate a hypothesis test, we check the
          conditions for <m>\hat{p}</m> and construct the standard error using the null
          value, <m>p_0</m>, instead of using the sample proportion.
        </p>
        <p>
          In a hypothesis test with a p-value, we are supposing the null hypothesis is
          true, which is a different mindset than when we compute a confidence interval.
          This is why we use <m>p_0</m> instead of <m>\hat{p}</m> when we check
          conditions and compute the standard error in this context.
        </p>
      </assemblage>
      
      <p>
        When we identify the sampling distribution under the null hypothesis, it has a
        special name: the <term>null distribution</term>. The p-value represents the
        probability of the observed <m>\hat{p}</m>, or a <m>\hat{p}</m> that is more
        extreme, if the null hypothesis were true. To find the p-value, we generally find
        the null distribution, and then we find a tail area in that distribution
        corresponding to our point estimate.
      </p>
      
      <example xml:id="ex-coal-p-value">
        <title>Computing the p-value for coal energy support</title>
        <statement>
          <p>
            If the null hypothesis were true, determine the chance of finding
            <m>\hat{p}</m> at least as far into the tails as 0.37 under the null
            distribution, which is a normal distribution with mean <m>\mu = 0.5</m> and
            <m>SE = 0.016</m>.
          </p>
        </statement>
        <solution>
          <p>
            This is a normal probability problem where <m>x = 0.37</m>. First, we draw a
            simple graph to represent the situation, similar to what is shown in <xref
            ref="fig-coal-null-distribution"/>. Since <m>\hat{p}</m> is so far out in the
            tail, we know the tail area is going to be very small. To find it, we start by
            computing the Z-score using the mean of 0.5 and the standard error of 0.016:
            <me>Z = \frac{0.37 - 0.5}{0.016} = -8.125</me>
          </p>
          <p>
            We can use software to find the tail area: <m>2.2 \times 10^{-16}</m>
            (0.00000000000000022). If using the normal probability table in the appendix,
            we'd find that <m>Z = -8.125</m> is off the table, so we would use the
            smallest area listed: 0.0002.
          </p>
          <p>
            The potential <m>\hat{p}</m>'s in the upper tail beyond 0.63, which are shown
            in <xref ref="fig-coal-p-value-both-tails"/>, also represent observations at
            least as extreme as the observed value of 0.37. To account for these values
            that are also more extreme under the hypothesis setup, we double the lower tail
            to get an estimate of the p-value: <m>4.4 \times 10^{-16}</m> (or if using the
            table method, 0.0004).
          </p>
          <p>
            The p-value represents the probability of observing such an extreme sample
            proportion by chance, if the null hypothesis were true.
          </p>
        </solution>
      </example>
      
      <figure xml:id="fig-coal-p-value-both-tails">
        <caption>If <m>H_0</m> were true, then the values above 0.63 are just as unlikely as values below 0.37.</caption>
        <image source="images/ch_foundations_for_inf/figures/normal_dist_mean_500_se_016/normal_dist_mean_500_se_016.png" width="70%">
          <description>A normal distribution centered at 0.5 with a standard deviation of 0.016 is shown. Additionally, the tail areas below 0.37 and above 0.63 are emphasized -- the regions under the normal distribution are nearly zero. Two annotations also appear. First, an annotation located at 0.37 states, "Tail area for p-hat". Second, an annotation located at 0.68 states, "Equally unlikely if H-sub-zero (the null hypothesis) is true".</description>
        </image>
      </figure>
      
      <example xml:id="ex-coal-decision">
        <title>Evaluating the coal energy hypothesis test</title>
        <statement>
          <p>
            How should we evaluate the hypotheses using the p-value of <m>4.4 \times
            10^{-16}</m>? Use the standard significance level of <m>\alpha = 0.05</m>.
          </p>
        </statement>
        <solution>
          <p>
            If the null hypothesis were true, there's only an incredibly small chance of
            observing such an extreme deviation of <m>\hat{p}</m> from 0.5. This means one
            of the following must be true:
            <ol>
              <li>The null hypothesis is true, and we just happened to observe something so
                  extreme that it only happens about once in every 23 quadrillion times (1
                  quadrillion = 1 million <m>\times</m> 1 billion).</li>
              <li>The alternative hypothesis is true, which would be consistent with
                  observing a sample proportion far from 0.5.</li>
            </ol>
            The first scenario is laughably improbable, while the second scenario seems
            much more plausible.
          </p>
          <p>
            Formally, when we evaluate a hypothesis test, we compare the p-value to the
            significance level, which in this case is <m>\alpha = 0.05</m>. Since the
            p-value is less than <m>\alpha</m>, we reject the null hypothesis. That is, the
            data provide strong evidence against <m>H_0</m>. The data indicate the
            direction of the difference: a majority of Americans do not support expanding
            the use of coal-powered energy.
          </p>
        </solution>
      </example>
      
      <assemblage>
        <title>Compare the p-value to <m>\alpha</m> to evaluate <m>H_0</m></title>
        <p>
          When the p-value is less than the significance level, <m>\alpha</m>, reject
          <m>H_0</m>. We would report a conclusion that the data provide strong evidence
          supporting the alternative hypothesis.
        </p>
        <p>
          When the p-value is greater than <m>\alpha</m>, do not reject <m>H_0</m>, and
          report that we do not have sufficient evidence to reject the null hypothesis.
        </p>
        <p>
          In either case, it is important to describe the conclusion in the context of the
          data.
        </p>
      </assemblage>
      
      <example xml:id="ex-nuclear-arms-reduction">
        <title>Nuclear arms reduction support</title>
        <statement>
          <p>
            A simple random sample of 1,028 US adults in March 2013 show that 56% support
            nuclear arms reduction. Does this provide convincing evidence that a majority
            of Americans supported nuclear arms reduction at the 5% significance level?
          </p>
        </statement>
        <solution>
          <p>
            First, check conditions:
          </p>
          <p>
            <dl>
              <li>
                <title>Independence.</title>
                <p>
                  The poll was of a simple random sample of US adults, meaning the
                  observations are independent.
                </p>
              </li>
              <li>
                <title>Success-failure.</title>
                <p>
                  In a one-proportion hypothesis test, this condition is checked using the
                  null proportion, which is <m>p_0 = 0.5</m> in this context: <m>np_0 =
                  n(1-p_0) = 1028 \times 0.5 = 514 \geq 10</m>.
                </p>
              </li>
            </dl>
          </p>
          <p>
            With these conditions verified, we can model <m>\hat{p}</m> using a normal
            model.
          </p>
          <p>
            Next the standard error can be computed. The null value <m>p_0</m> is used
            again here, because this is a hypothesis test for a single proportion.
            <me>
              SE_{\hat{p}} = \sqrt{\frac{p_0(1-p_0)}{n}} = \sqrt{\frac{0.5 \times
              (1-0.5)}{1028}} = 0.0156
            </me>
          </p>
          <p>
            Based on the normal model, the test statistic can be computed as the Z-score
            of the point estimate:
            <me>
              Z = \frac{\text{point estimate} - \text{null value}}{SE} = \frac{0.56 -
              0.50}{0.0156} = 3.85
            </me>
          </p>
          <p>
            It's generally helpful to draw null distribution and the tail areas of interest
            for computing the p-value, as shown in <xref
            ref="fig-nuclear-arms-p-value"/>. The upper tail area is about 0.0001, and we
            double this tail area to get the p-value: 0.0002.
          </p>
          <p>
            Because the p-value is smaller than 0.05, we reject <m>H_0</m>. The poll
            provides convincing evidence that a majority of Americans supported nuclear
            arms reduction efforts in March 2013.
          </p>
        </solution>
      </example>
      
      <figure xml:id="fig-nuclear-arms-p-value">
        <caption>A normal distribution centered at 0.5 is shown, which has a standard deviation of about 0.0156. Two tails several standard deviations away from the center are emphasized. The first, at and above 0.56, is annotated with the text "upper tail". The second, which appears to be at and below 0.44, is annotated with the text "lower tail".</caption>
        <image source="images/ch_foundations_for_inf/figures/nuclearArmsReduction/nuclearArmsReductionPValue.png" width="48%">
          <description>Normal distribution centered at 0.5 showing tail areas for nuclear arms test</description>
        </image>
      </figure>
      
      <assemblage xml:id="ht-procedure">
        <title>Hypothesis testing for a single proportion</title>
        <p>
          Once you've determined a one-proportion hypothesis test is the correct
          procedure, there are four steps to completing the test:
        </p>
        <p>
          <dl>
            <li>
              <title>Prepare.</title>
              <p>Identify the parameter of interest, list hypotheses, identify the significance level, and identify <m>\hat{p}</m> and <m>n</m>.</p>
            </li>
            <li>
              <title>Check.</title>
              <p>Verify conditions to ensure <m>\hat{p}</m> is nearly normal under <m>H_0</m>. For one-proportion hypothesis tests, use the null value to check the success-failure condition.</p>
            </li>
            <li>
              <title>Calculate.</title>
              <p>If the conditions hold, compute the standard error, again using <m>p_0</m>, compute the Z-score, and identify the p-value.</p>
            </li>
            <li>
              <title>Conclude.</title>
              <p>Evaluate the hypothesis test by comparing the p-value to <m>\alpha</m>, and provide a conclusion in the context of the problem.</p>
            </li>
          </dl>
        </p>
      </assemblage>
    </subsection>
    
    <subsection xml:id="subsec-significance-level">
      <title>Choosing a Significance Level</title>
      
      <p>
        Choosing a significance level for a test is important in many contexts,
        and the traditional level is <m>\alpha = 0.05</m>.
        However, it can be helpful to adjust the significance level based on the application.
        We may select a level that is smaller or larger than 0.05 depending on the
        consequences of any conclusions reached from the test.
      </p>
      
      <p>
        If making a Type 1 Error is dangerous or especially costly,
        we should choose a small significance level (e.g. 0.01).
        Under this scenario we want to be very cautious about rejecting the null hypothesis,
        so we demand very strong evidence favoring <m>H_A</m> before we would reject <m>H_0</m>.
      </p>
      
      <p>
        If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error,
        then we might choose a higher significance level (e.g. 0.10).
        Here we want to be cautious about failing to reject <m>H_0</m> when the alternative
        hypothesis is actually true.
      </p>
      
      <p>
        Additionally, if the cost of collecting data is small relative to the cost of a
        Type 2 Error, then it may also be a good strategy to collect more data.
        Under this strategy, the Type 2 Error can be reduced while not affecting the
        Type 1 Error rate.
        Of course, collecting extra data is often costly, so there is typically a
        cost-benefit analysis to be considered.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-statistical-vs-practical">
      <title>Statistical Significance versus Practical Significance</title>

      <p>
        When the sample size becomes larger, point estimates become more precise and any
        real differences in the mean and null value become easier to detect and
        recognize. Even a very small difference would likely be detected if we took a
        large enough sample. Sometimes researchers will take such large samples that even
        the slightest difference is detected, even differences where there is no
        practical value. In such cases, we still say the difference is
        <term>statistically significant</term>, but it is not <term>practically
        significant</term>.
      </p>

      <p>
        For example, an online experiment might identify that placing additional ads on a
        movie review website statistically significantly increases viewership of a TV
        show by 0.001%, but this increase might not have any practical value.
      </p>

      <p>
        One role of a data scientist in conducting a study often includes planning the
        size of the study. The data scientist might first consult experts or scientific
        literature to learn what would be the smallest meaningful difference from the
        null value. She also would obtain other information, such as a very rough
        estimate of the true proportion <m>p</m>, so that she could roughly estimate the
        standard error. From here, she can suggest a sample size that is sufficiently
        large that, if there is a real difference that is meaningful, we could detect it.
        While larger sample sizes may still be used, these calculations are especially
        helpful when considering costs or potential risks, such as possible health
        impacts to volunteers in a medical study.
      </p>
    </subsection>

 <subsection xml:id="subsec-one-sided-tests">
      <title>One-Sided Hypothesis Tests (Special Topic)</title>

      <p>
        So far we've only considered what are called <term>two-sided hypothesis
        tests</term>, where we care about detecting whether <m>p</m> is either above or
        below some null value <m>p_0</m>. There is a second type of hypothesis test
        called a <term>one-sided hypothesis test</term>.
      </p>

      <p>
        For a one-sided hypothesis test, the hypotheses take one of the following forms:
      </p>

      <ol>
        <li>There's only value in detecting if the population parameter is
            <em>less than</em> some value <m>p_0</m>. In this case, the alternative
            hypothesis is written as <m>p &lt; p_0</m> for some null value <m>p_0</m>.</li>
        <li>There's only value in detecting if the population parameter is
            <em>more than</em> some value <m>p_0</m>: In this case, the alternative
            hypothesis is written as <m>p &gt; p_0</m>.</li>
      </ol>

      <p>
        While we adjust the form of the alternative hypothesis, we continue to write the
        null hypothesis using an equals-sign in the one-sided hypothesis test case.
      </p>

      <p>
        In the entire hypothesis testing procedure, there is only one difference in
        evaluating a one-sided hypothesis test vs a two-sided hypothesis test: how to
        compute the p-value. In a one-sided hypothesis test, we compute the p-value as
        the tail area in the <em>direction of the alternative hypothesis only</em>,
        meaning it is represented by a single tail area. Herein lies the reason why
        one-sided tests are sometimes interesting: if we don't have to double the tail
        area to get the p-value, then the p-value is smaller and the level of evidence
        required to identify an interesting finding in the direction of the alternative
        hypothesis goes down. However, one-sided tests aren't all sunshine and rainbows:
        the heavy price paid is that any interesting findings in the opposite direction
        must be disregarded.
      </p>

      <example xml:id="ex-stents-two-sided-important">
        <statement>
          <p>
            In Section <xref ref="subsec-ht-framework"/>, we encountered an example where
            doctors were interested in determining whether stents would help people who
            had a high risk of stroke. The researchers believed the stents would help.
            Unfortunately, the data showed the opposite: patients who received stents
            actually did worse. Why was using a two-sided test so important in this
            context?
          </p>
        </statement>
        <solution>
          <p>
            Before the study, researchers had reason to believe that stents would help
            patients since existing research suggested stents helped in patients with
            heart attacks. It would surely have been tempting to use a one-sided test in
            this situation, and had they done this, they would have limited their ability
            to identify potential harm to patients.
          </p>
        </solution>
      </example>

      <p>
        This example highlights that using a one-sided hypothesis creates a risk of
        overlooking data supporting the opposite conclusion. We could have made a similar
        error when reviewing the Roslings' question data in this section; if we had a
        pre-conceived notion that college-educated people wouldn't do worse than random
        guessing and so used a one-sided test, we would have missed the really
        interesting finding that many people have incorrect knowledge about global public
        health.
      </p>

      <p>
        When might a one-sided test be appropriate to use? <em>Very rarely.</em> Should
        you ever find yourself considering using a one-sided test, carefully answer the
        following question:
      </p>

      <blockquote>
        <p>
          <em>What would I, or others, conclude if the data happens to go clearly in the
          opposite direction than my alternative hypothesis?</em>
        </p>
      </blockquote>

      <p>
        If you or others would find any value in making a conclusion about the data that
        goes in the opposite direction of a one-sided test, then a two-sided hypothesis
        test should actually be used. These considerations can be subtle, so exercise
        caution. We will only apply two-sided tests in the rest of this book.
      </p>

      <example xml:id="ex-why-not-choose-direction-after">
        <statement>
          <p>
            Why can't we simply run a one-sided test that goes in the direction of the
            data?
          </p>
        </statement>
        <solution>
          <p>
            We've been building a careful framework that controls for the Type 1 Error,
            which is the significance level <m>\alpha</m> in a hypothesis test. We'll use
            the <m>\alpha = 0.05</m> below to keep things simple.
          </p>
          <p>
            Imagine we could pick the one-sided test after we saw the data. What will go
            wrong?
          </p>
          <ul>
            <li>If <m>\hat{p}</m> is <em>smaller</em> than the null value, then a
                one-sided test where <m>p &lt; p_0</m> would mean that any observation in
                the <em>lower</em> 5% tail of the null distribution would lead to us
                rejecting <m>H_0</m>.</li>
            <li>If <m>\hat{p}</m> is <em>larger</em> than the null value, then a
                one-sided test where <m>p &gt; p_0</m> would mean that any observation in
                the <em>upper</em> 5% tail of the null distribution would lead to us
                rejecting <m>H_0</m>.</li>
          </ul>
          <p>
            Then if <m>H_0</m> were true, there's a 10% chance of being in one of the two
            tails, so our testing error is actually <m>\alpha = 0.10</m>, not 0.05. That
            is, not being careful about when to use one-sided tests effectively
            undermines the methods we're working so hard to develop and utilize.
          </p>
        </solution>
      </example>

      <warning>
        <p>
          Use one-sided tests only when you have a strong reason to care about
          deviations in only one direction. If you're unsure, use a two-sided test.
        </p>
      </warning>
    </subsection>

    <exercises>
      <title>Section 5.3 Exercises</title>

      <exercise xml:id="ex-identify-hypotheses-1">
       <title>Identify hypotheses, Part I.</title>
        <statement>
          <p>
            Write the null and alternative hypotheses in words and then symbols for each
            of the following situations.
          </p>
          <ol>
            <li>A tutoring company would like to understand if most students tend to
                improve their grades (or not) after they use their services. They sample
                200 of the students who used their service in the past year and ask them
                if their grades have improved or declined from the previous year.</li>
            <li>Employers at a firm are worried about the effect of March Madness, a
                basketball championship held each spring in the US, on employee
                productivity. They estimate that on a regular business day employees
                spend on average 15 minutes of company time checking personal email,
                making personal phone calls, etc. They also collect data on how much
                company time employees spend on such non-business activities during March
                Madness. They want to determine if these data provide convincing evidence
                that employee productivity changed during March Madness.</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-identify-hypotheses-2">
       <title>Identify hypotheses, Part II.</title>
        <statement>
          <p>
            Write the null and alternative hypotheses in words and using symbols for
            each of the following situations.
          </p>
          <ol>
            <li>Since 2008, chain restaurants in California have been required to
                display calorie counts of each menu item. Prior to menus displaying
                calorie counts, the average calorie intake of diners at a restaurant was
                1100 calories. After calorie counts started to be displayed on menus, a
                nutritionist collected data on the number of calories consumed at this
                restaurant from a random sample of diners. Do these data provide
                convincing evidence of a difference in the average calorie intake of
                diners at this restaurant?</li>
            <li>The state of Wisconsin would like to understand the fraction of its
                adult residents that consumed alcohol in the last year, specifically if
                the rate is different from the national rate of 70%. To help them answer
                this question, they conduct a random sample of 852 residents and ask them
                about their alcohol consumption.</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-online-communication">
       <title>Online communication.</title>
        <statement>
          <p>
            A study suggests that 60% of college students spend 10 or more hours per week
            communicating with others online. You believe that this is incorrect and
            decide to collect your own sample for a hypothesis test. You randomly sample
            160 students from your dorm and find that 70% spent 10 or more hours a week
            communicating with others online. A friend of yours, who offers to help you
            with the hypothesis test, comes up with the following set of hypotheses.
            Indicate any errors you see.
          </p>
          <md>
            <mrow>H_0: \amp\ \hat{p} &lt; 0.6</mrow>
            <mrow>H_A: \amp\ \hat{p} &gt; 0.7</mrow>
          </md>
        </statement>
      </exercise>

      <exercise xml:id="ex-married-at-25">
       <title>Married at 25.</title>
        <statement>
          <p>
            A study suggests that 25% of 25 year olds have gotten married. You
            believe that this is incorrect and decide to collect your own sample for a
            hypothesis test. From a random sample of 25 year olds in census data with
            size 776, you find that 24% of them are married. A friend of yours offers to
            help you with setting up the hypothesis test and comes up with the following
            hypotheses. Indicate any errors you see.
          </p>
          <md>
            <mrow>H_0: \amp\ \hat{p} = 0.24</mrow>
            <mrow>H_A: \amp\ \hat{p} \neq 0.24</mrow>
          </md>
        </statement>
      </exercise>

      <exercise xml:id="ex-cyberbullying-rates">
       <title>Cyberbullying rates.</title>
        <statement>
          <p>
            Teens were surveyed about cyberbullying, and 54% to 64% reported
            experiencing cyberbullying (95% confidence interval). Answer the following
            questions based on this interval.
          </p>
          <ol>
            <li>A newspaper claims that a majority of teens have experienced
                cyberbullying. Is this claim supported by the confidence interval?
                Explain your reasoning.</li>
            <li>A researcher conjectured that 70% of teens have experienced
                cyberbullying. Is this claim supported by the confidence interval?
                Explain your reasoning.</li>
            <li>Without actually calculating the interval, determine if the claim of the
                researcher from part (b) would be supported based on a 90% confidence
                interval?</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-er-wait-ci-ht">
       <title>Waiting at an ER, Part II.</title>
        <statement>
          <p>
            The ER wait time exercise provides a 95% confidence interval for the mean
            waiting time at an emergency room (ER) of (128 minutes, 147 minutes). Answer
            the following questions based on this interval.
          </p>
          <ol>
            <li>A local newspaper claims that the average waiting time at this ER
                exceeds 3 hours. Is this claim supported by the confidence interval?
                Explain your reasoning.</li>
            <li>The Dean of Medicine at this hospital claims the average wait time is
                2.2 hours. Is this claim supported by the confidence interval? Explain
                your reasoning.</li>
            <li>Without actually calculating the interval, determine if the claim of the
                Dean from part (b) would be supported based on a 99% confidence
                interval?</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-minimum-wage-1">
       <title>Minimum wage, Part I.</title>
        <statement>
          <p>
            Do a majority of US adults believe raising the minimum wage will help the
            economy, or is there a majority who do not believe this? A Rasmussen Reports
            survey of a random sample of 1,000 US adults found that 42% believe it will
            help the economy. Conduct an appropriate hypothesis test to help answer the
            research question.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-getting-enough-sleep">
       <title>Getting enough sleep.</title>
        <statement>
          <p>
            400 students were randomly sampled from a large university, and 289 said they
            did not get enough sleep. Conduct a hypothesis test to check whether this
            represents a statistically significant difference from 50%, and use a
            significance level of 0.01.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-working-backwards-1">
       <title>Working backwards, Part I.</title>
        <statement>
          <p>
            You are given the following hypotheses:
          </p>
          <md>
            <mrow>H_0: \amp\ p = 0.3</mrow>
            <mrow>H_A: \amp\ p \neq 0.3</mrow>
          </md>
          <p>
            We know the sample size is 90. For what sample proportion would the p-value
            be equal to 0.05? Assume that all conditions necessary for inference are
            satisfied.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-working-backwards-2">
       <title>Working backwards, Part II.</title>
        <statement>
          <p>
            You are given the following hypotheses:
          </p>
          <md>
            <mrow>H_0: \amp\ p = 0.9</mrow>
            <mrow>H_A: \amp\ p \neq 0.9</mrow>
          </md>
          <p>
            We know that the sample size is 1,429. For what sample proportion would the
            p-value be equal to 0.01? Assume that all conditions necessary for inference
            are satisfied.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-testing-fibromyalgia">
       <title>Testing for Fibromyalgia.</title>
        <statement>
          <p>
            A patient named Diana was diagnosed with Fibromyalgia, a long-term syndrome
            of body pain, and was prescribed anti-depressants. Being the skeptic that she
            is, Diana didn't initially believe that anti-depressants would help her
            symptoms. However after a couple months of being on the medication she
            decides that the anti-depressants are working, because she feels like her
            symptoms are in fact getting better.
          </p>
          <ol>
            <li>Write the hypotheses in words for Diana's skeptical position when she
                started taking the anti-depressants.</li>
            <li>What is a Type 1 Error in this context?</li>
            <li>What is a Type 2 Error in this context?</li>
          </ol>
        </statement>
      </exercise>

      <exercise xml:id="ex-which-is-higher">
       <title>Which is higher?</title>
        <statement>
          <p>
            In each part below, there is a value of interest and two scenarios (I and
            II). For each part, report if the value of interest is larger under scenario
            I, scenario II, or whether the value is equal under the scenarios.
          </p>
          <ol>
            <li>The standard error of <m>\hat{p}</m> when (I) <m>n = 125</m> or
                (II) <m>n = 500</m>.</li>
            <li>The margin of error of a confidence interval when the confidence level is
                (I) 90% or (II) 80%.</li>
            <li>The p-value for a Z-statistic of 2.5 calculated based on a (I) sample
                with <m>n = 500</m> or based on a (II) sample with <m>n = 1000</m>.</li>
            <li>The probability of making a Type 2 Error when the alternative hypothesis
                is true and the significance level is (I) 0.05 or (II) 0.10.</li>
          </ol>
        </statement>
      </exercise>
    </exercises>
  </section>

  <!-- Section 5.4: Chapter 5 review exercises -->
  <section xml:id="sec-ch05-review">    
      <title>Review Exercises</title>
    <exercises>      
      <exercise xml:id="ex-relax-after-work">
       <title>Relaxing after work.</title>
        <statement>
          <p>
            The General Social Survey asked the question:
            <q>After an average work day, about how many hours do you have to relax or pursue 
            activities that you enjoy?</q> to a random sample of 1,155 Americans. A 95% confidence interval for the mean number of hours spent 
            relaxing or pursuing activities they enjoy was (1.38, 1.92).
          </p>
          <ol>
            <li>Interpret this interval in context of the data.</li>
            <li>Suppose another set of researchers reported a confidence interval with a 
                larger margin of error based on the same sample of 1,155 Americans. How does 
                their confidence level compare to the confidence level of the interval stated 
                above?</li>
            <li>Suppose next year a new survey asking the same question is conducted, and 
                this time the sample size is 2,500. Assuming that the population 
                characteristics, with respect to how much time people spend relaxing after work, 
                have not changed much within a year, how will the margin of error of the 95% 
                confidence interval constructed based on data from the new survey compare to the 
                margin of error of the interval stated above?</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-minimum-wage-2">
       <title>Minimum wage, Part II.</title>
        <statement>
          <p>
            In <xref ref="ex-minimum-wage-1"/>,
            we learned that a Rasmussen Reports survey
            of 1,000 US adults found that 42% believe
            raising the minimum wage will help the economy.
            Construct a 99% confidence interval for the
            true proportion of US adults who believe this.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-testing-food-safety">
       <title>Testing for food safety.</title>
        <statement>
          <p>
            A food safety inspector 
            is called upon to investigate a restaurant with a few customer reports of poor 
            sanitation practices. The food safety inspector uses a hypothesis testing 
            framework to evaluate whether regulations are not being met. If he decides 
            the restaurant is in gross violation, its license to serve food will be revoked.
          </p>
          <ol>
            <li>Write the hypotheses in words.</li>
            <li>What is a Type 1 Error in this context?</li>
            <li>What is a Type 2 Error in this context?</li>
            <li>Which error is more problematic for the restaurant owner? Why?</li>
            <li>Which error is more problematic for the diners? Why?</li>
            <li>As a diner, would you prefer that the food safety inspector requires 
                strong evidence or very strong evidence of health concerns before revoking a 
                restaurant's license? Explain your reasoning.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-true-false-review">
       <title>True or false?</title>
        <statement>
          <p>
            Determine if the following statements are true or false, and 
            explain your reasoning. If false, state how it could be corrected.
          </p>
          <ol>
            <li>If a given value (for example, the null hypothesized value of a parameter) 
                is within a 95% confidence interval, it will also be within a 99% confidence 
                interval.</li>
            <li>Decreasing the significance level (<m>\alpha</m>) will increase the probability 
                of making a Type 1 Error.</li>
            <li>Suppose the null hypothesis is <m>p = 0.5</m> and we fail to reject <m>H_0</m>. 
                Under this scenario, the true population proportion is 0.5.</li>
            <li>With large sample sizes, even small differences between the null value and 
                the observed point estimate, a difference often called the
                effect size, will be identified as statistically significant.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-unemployment-relationship">
       <title>Unemployment and relationship problems.</title>
        <statement>
          <p>
            A USA Today/Gallup poll asked a group of
            unemployed and underemployed Americans if they have
            had major problems in their relationships with their
            spouse or another close family member as a result of
            not having a job (if unemployed) or not having
            a full-time job (if underemployed).
            27% of the 1,145 unemployed respondents and
            25% of the 675 underemployed respondents said they had
            major problems in relationships as a result of their
            employment status.
          </p>
          <ol>
            <li>What are the hypotheses for evaluating if the proportions
                of unemployed and underemployed people who had relationship
                problems were different?</li>
            <li>The p-value for this hypothesis test is approximately 0.35.
                Explain what this means in context of the hypothesis test
                and the data.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-nearsighted">
       <title>Nearsighted.</title>
        <statement>
          <p>
            It is believed that nearsightedness affects about 8% of 
            all children.
            In a random sample of 194 children, 21 are nearsighted.
            Conduct a hypothesis test for the following question:
            do these data provide evidence that the 8% value is inaccurate?
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-nutrition-labels">
       <title>Nutrition labels.</title>
        <statement>
          <p>
            The nutrition label on a bag of potato chips says
            that a one ounce (28 gram) serving of potato chips
            has 130 calories and contains ten grams of fat,
            with three grams of saturated fat.
            A random sample of 35 bags yielded
            a confidence interval for the number of calories
            per bag of 128.2 to 139.8 calories.
            Is there evidence that the nutrition label does not 
            provide an accurate measure of calories in the bags
            of potato chips?
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-clt-proportions">
       <title>CLT for proportions.</title>
        <statement>
          <p>
            Define the term <q>sampling distribution</q> of the sample proportion,
            and describe how the shape, center, and spread of the sampling
            distribution change as the sample size increases when <m>p = 0.1</m>.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-practical-vs-statistical">
       <title>Practical vs statistical significance.</title>
        <statement>
          <p>
            Determine whether the following statement is true
            or false, and explain your reasoning:
            <q>With large sample sizes, even small differences
            between the null value and the observed point
            estimate can be statistically significant.</q>
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-same-obs-diff-n">
       <title>Same observation, different sample size.</title>
        <statement>
          <p>
            Suppose you 
            conduct a hypothesis test based on a sample where the sample size is <m>n = 50</m>, 
            and arrive at a p-value of 0.08. You then refer back to your notes and discover 
            that you made a careless mistake, the sample size should have been <m>n = 500</m>. 
            Will your p-value increase, decrease, or stay the same? Explain.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="ex-gender-pay-gap">
       <title>Gender pay gap in medicine.</title>
        <statement>
          <p>
            A study examined the average pay for men and women
            entering the workforce as doctors for 21 different
            positions.
          </p>
          <ol>
            <li>If each gender was equally paid, then we would expect
                about half of those positions to have men paid more
                than women and women would be paid more than men in
                the other half of positions.
                Write appropriate hypotheses to test this scenario.</li>
            <li>Men were, on average, paid more in 19 of those
                21 positions.
                Supposing these 21 positions represent a simple random sample,
                complete a hypothesis test using your hypotheses
                from part (a).</li>
          </ol>
        </statement>
      </exercise>
    </exercises>
  </section>
</chapter>
