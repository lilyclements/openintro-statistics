<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-probability" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Probability</title>

  <introduction>
    <p>
      Probability forms the foundation of statistics, and you're probably already
      aware of many of the ideas presented in this chapter. However, formalization 
      of probability concepts is likely new for most readers.
    </p>
    <p>
      While this chapter provides a theoretical foundation for the ideas in later 
      chapters and provides a path to a deeper understanding, mastery of the concepts 
      introduced in this chapter is not required for applying the methods introduced 
      in the rest of this book.
    </p>
  </introduction>

  <!-- Section 3.1: Defining probability -->
  <section xml:id="sec-defining-probability">
    <title>Defining probability</title>

    <introduction>
      <p>
        Statistics is based on probability, and while probability is not required 
        for the applied techniques in this book, it may help you gain a deeper 
        understanding of the methods and set a better foundation for future courses.
      </p>
    </introduction>

    <subsection xml:id="subsec-introductory-examples">
      <title>Introductory examples</title>
      <p>
        Before we get into technical ideas, let's walk through some basic examples that may feel more familiar.
      </p>

      <example xml:id="ex-prob-of-1">
        <statement>
          <p>
            A <q>die</q>, the singular of dice, is a cube with six faces numbered 1, 2, 3, 4, 5, and 6. 
            What is the chance of getting 1 when rolling a die?
          </p>
        </statement>
        <solution>
          <p>
            If the die is fair, then the chance of a 1 is as good as the chance of any other number. 
            Since there are six outcomes, the chance must be 1-in-6 or, equivalently, <m>1/6</m>.
          </p>
        </solution>
      </example>

      <example xml:id="ex-prob-of-1-or-2">
        <statement>
          <p>What is the chance of getting a 1 or 2 in the next roll?</p>
        </statement>
        <solution>
          <p>
            1 and 2 constitute two of the six equally likely possible outcomes, so the chance of getting 
            one of these two outcomes must be <m>2/6 = 1/3</m>.
          </p>
        </solution>
      </example>

      <example xml:id="ex-prob-of-123456">
        <statement>
          <p>
            What is the chance of getting either 1, 2, 3, 4, 5, or 6 on the next roll?
          </p>
        </statement>
        <solution>
          <p>
            100%. The outcome must be one of these numbers.
          </p>
        </solution>
      </example>

      <example xml:id="ex-prob-not-2">
        <statement>
          <p>What is the chance of not rolling a 2?</p>
        </statement>
        <solution>
          <p>
            Since the chance of rolling a 2 is <m>1/6</m> or <m>16.\overline{6}\%</m>, the chance of not 
            rolling a 2 must be <m>100\% - 16.\overline{6}\% = 83.\overline{3}\%</m> or <m>5/6</m>.
          </p>
          <p>
            Alternatively, we could have noticed that not rolling a 2 is the same as getting a 1, 3, 4, 5, 
            or 6, which makes up five of the six equally likely outcomes and has probability <m>5/6</m>.
          </p>
        </solution>
      </example>

      <example xml:id="ex-prob-of-2-ones">
        <statement>
          <p>
            Consider rolling two dice. If <m>1/6</m> of the time the first die is a 1 and <m>1/6</m> of 
            those times the second die is a 1, what is the chance of getting two 1s?
          </p>
        </statement>
        <solution>
          <p>
            If <m>16.\overline{6}\%</m> of the time the first die is a 1 and <m>1/6</m> of <em>those</em> 
            times the second die is also a 1, then the chance that both dice are 1 is 
            <m>(1/6)\times (1/6)</m> or <m>1/36</m>.
          </p>
        </solution>
      </example>
    </subsection>

    <subsection xml:id="subsec-probability-definition">
      <title>Probability</title>
      
      <p>
        We use probability to build tools to describe and understand apparent randomness. We often frame 
        probability in terms of a <term>random process</term> giving rise to an <term>outcome</term>.
      </p>
      
      <p>
        Rolling a die or flipping a coin is a seemingly random process and each gives rise to an outcome: 
        Roll a die <m>\\rightarrow</m> 1, 2, 3, 4, 5, or 6; Flip a coin <m>\\rightarrow</m> H or T.
      </p>

      <assemblage xml:id="def-probability">
        <title>Probability</title>
        <p>
          The <term>probability</term> of an outcome is the proportion of times the outcome would occur if 
          we observed the random process an infinite number of times.
        </p>
      </assemblage>

      <p>
        Probability is defined as a proportion, and it always takes values between 0 and 1 (inclusively). 
        It may also be displayed as a percentage between 0% and 100%.
      </p>

      <p>
        Probability can be illustrated by rolling a die many times. Let <m>\\hat{p}_n</m> be the proportion 
        of outcomes that are 1 after the first <m>n</m> rolls. As the number of rolls increases, 
        <m>\\hat{p}_n</m> will converge to the probability of rolling a 1, <m>p = 1/6</m>. 
        <xref ref="fig-die-prop"/> shows this convergence for 100,000 die rolls. The tendency of 
        <m>\\hat{p}_n</m> to stabilize around <m>p</m> is described by the <term>Law of Large Numbers</term>.
      </p>

      <figure xml:id="fig-die-prop">
        <caption>The fraction of die rolls that are 1 at each stage in a simulation. The proportion tends to 
          get closer to the probability <m>1/6 \\approx 0.167</m> as the number of rolls increases.</caption>
        <image source="ch_probability/figures/dieProp"/>
      </figure>

      <assemblage xml:id="law-large-numbers">
        <title>Law of Large Numbers</title>
        <p>
          As more observations are collected, the proportion <m>\\hat{p}_n</m> of occurrences with a 
          particular outcome converges to the probability <m>p</m> of that outcome.
        </p>
      </assemblage>

      <p>
        Occasionally the proportion will veer off from the probability and appear to defy the Law of Large 
        Numbers, as <m>\\hat{p}_n</m> does many times in <xref ref="fig-die-prop"/>. However, these 
        deviations become smaller as the number of rolls increases.
      </p>

      <p>
        Above we write <m>p</m> as the probability of rolling a 1. We can also write this probability as
        <me>P(\\text{rolling a 1})</me>
        As we become more comfortable with this notation, we will abbreviate it further. For instance, if 
        it is clear that the process is <q>rolling a die</q>, we could abbreviate <m>P(</m>rolling a 1<m>)</m> 
        as <m>P(1)</m>.
      </p>

      <exercise xml:id="ex-random-process">
        <title>Random processes</title>
        <statement>
          <p>
            Random processes include rolling a die and flipping a coin. (a) Think of another random process. 
            (b) Describe all the possible outcomes of that process. For instance, rolling a die is a random 
            process with possible outcomes 1, 2, <ellipsis />, 6.
          </p>
        </statement>
        <solution>
          <p>
            Here are four examples. (i) Whether someone gets sick in the next month or not is an apparently 
            random process with outcomes <q>sick</q> and <q>not</q>. (ii) We can <em>generate</em> a random 
            process by randomly picking a person and measuring that person's height. The outcome of this 
            process will be a positive number. (iii) Whether the stock market goes up or down next week is a 
            seemingly random process with possible outcomes <q>up</q>, <q>down</q>, and <q>no_change</q>. 
            Alternatively, we could have used the percent change in the stock market as a numerical outcome. 
            (iv) Whether your roommate cleans her dishes tonight probably seems like a random process with 
            possible outcomes <q>cleans_dishes</q> and <q>leaves_dishes</q>.
          </p>
        </solution>
      </exercise>

      <p>
        What we think of as random processes are not necessarily random, but they may just be too difficult 
        to understand exactly. The fourth example in the solution to <xref ref="ex-random-process"/> 
        suggests a roommate's behavior is a random process. However, even if a roommate's behavior is not 
        truly random, modeling her behavior as a random process can still be useful.
      </p>
    </subsection>

    <subsection xml:id="subsec-disjoint-outcomes">
      <title>Disjoint or mutually exclusive outcomes</title>
      
      <p>
        Two outcomes are called <term>disjoint</term> or <term>mutually exclusive</term> if they cannot both happen. 
        For instance, if we roll a die, the outcomes 1 and 2 are disjoint since they cannot both occur. On the other 
        hand, the outcomes 1 and <q>rolling an odd number</q> are not disjoint since both occur if the outcome of the 
        roll is a 1. The terms <em>disjoint</em> and <em>mutually exclusive</em> are equivalent and interchangeable.
      </p>

      <p>
        Calculating the probability of disjoint outcomes is easy. When rolling a die, the outcomes 1 and 2 are disjoint, 
        and we compute the probability that one of these outcomes will occur by adding their separate probabilities:
        <me>P(\\text{1 or 2}) = P(\\text{1})+P(\\text{2}) = 1/6 + 1/6 = 1/3</me>
        What about the probability of rolling a 1, 2, 3, 4, 5, or 6? Here again, all of the outcomes are disjoint so we 
        add the probabilities:
        <md>
          <mrow>P(\\text{1 or 2 or 3 or 4 or 5 or 6}) \\amp= P(\\text{1})+P(\\text{2}) + P(\\text{3})+P(\\text{4}) + P(\\text{5})+P(\\text{6})</mrow>
          <mrow>\\amp= 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1</mrow>
        </md>
        The <term>Addition Rule</term> guarantees the accuracy of this approach when the outcomes are disjoint.
      </p>

      <assemblage xml:id="addition-rule-disjoint">
        <title>Addition Rule of disjoint outcomes</title>
        <p>
          If <m>A_1</m> and <m>A_2</m> represent two disjoint outcomes, then the probability that one of them occurs is given by
          <me>P(A_1\\text{ or } A_2) = P(A_1) + P(A_2)</me>
          If there are many disjoint outcomes <m>A_1</m>, <ellipsis />, <m>A_k</m>, then the probability that one of these 
          outcomes will occur is
          <me>P(A_1) + P(A_2) + \\cdots + P(A_k)</me>
        </p>
      </assemblage>

      <exercise xml:id="ex-disjoint-145">
        <statement>
          <p>
            We are interested in the probability of rolling a 1, 4, or 5. (a) Explain why the outcomes 1, 4, and 5 are 
            disjoint. (b) Apply the Addition Rule for disjoint outcomes to determine <m>P(</m>1 or 4 or 5<m>)</m>.
          </p>
        </statement>
        <solution>
          <p>
            (a) The random process is a die roll, and at most one of these outcomes can come up. This means they are disjoint 
            outcomes. (b) <m>P(</m>1 or 4 or 5<m>) = P(1)+P(4)+P(5) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{3}{6} = \\frac{1}{2}</m>
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-loans-homeownership">
        <statement>
          <p>
            In the loans data set in Chapter 1, the homeownership variable described whether the borrower rents, has a mortgage, 
            or owns her property. Of the 10,000 borrowers, 3858 rented, 4789 had a mortgage, and 1353 owned their home.
          </p>
          <p>
            <ol marker="(a)">
              <li><p>Are the outcomes <q>rent</q>, <q>mortgage</q>, and <q>own</q> disjoint?</p></li>
              <li><p>Determine the proportion of loans with value <q>mortgage</q> and <q>own</q> separately.</p></li>
              <li><p>Use the Addition Rule for disjoint outcomes to compute the probability a randomly selected loan from the 
                data set is for someone who has a mortgage or owns her home.</p></li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            (a) Yes. Each loan is categorized in only one level of homeownership. (b) Mortgage: <m>\\frac{4789}{10000} = 0.479</m>. 
            Own: <m>\\frac{1353}{10000} = 0.135</m>. (c) <m>P(</m>mortgage or own<m>) = P(</m>mortgage<m>) + P(</m>own<m>) = 0.479 + 0.135 = 0.614</m>.
          </p>
        </solution>
      </exercise>

      <p>
        Data scientists rarely work with individual outcomes and instead consider <em>sets</em> or <em>collections</em> of outcomes. 
        Let <m>A</m> represent the event where a die roll results in 1 or 2 and <m>B</m> represent the event that the die roll is 
        a 4 or a 6. We write <m>A</m> as the set of outcomes <m>\\{1, 2\\}</m> and <m>B=\\{4, 6\\}</m>. These sets are commonly called 
        <term>events</term>. Because <m>A</m> and <m>B</m> have no elements in common, they are disjoint events. <m>A</m> and 
        <m>B</m> are represented in <xref ref="fig-disjoint-sets"/>.
      </p>

      <figure xml:id="fig-disjoint-sets">
        <caption>Three events, <m>A</m>, <m>B</m>, and <m>D</m>, consist of outcomes from rolling a die. <m>A</m> and 
          <m>B</m> are disjoint since they do not have any outcomes in common.</caption>
        <image source="ch_probability/figures/disjointSets"/>
      </figure>

      <p>
        The Addition Rule applies to both disjoint outcomes and disjoint events. The probability that one of the disjoint events 
        <m>A</m> or <m>B</m> occurs is the sum of the separate probabilities:
        <me>P(A\\text{ or }B) = P(A) + P(B) = 1/3 + 1/3 = 2/3</me>
      </p>

      <exercise xml:id="ex-verify-prob-a-b">
        <statement>
          <p>
            (a) Verify the probability of event <m>A</m>, <m>P(A)</m>, is <m>1/3</m> using the Addition Rule. (b) Do the same 
            for event <m>B</m>.
          </p>
        </statement>
        <solution>
          <p>
            (a) <m>P(A) = P(</m>1 or 2<m>) = P(1) + P(2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} = \\frac{1}{3}</m>. 
            (b) Similarly, <m>P(B) = 1/3</m>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-examining-disjoint-sets-abd">
        <statement>
          <p>
            (a) Using <xref ref="fig-disjoint-sets"/> as a reference, what outcomes are represented by event <m>D</m>? (b) Are 
            events <m>B</m> and <m>D</m> disjoint? (c) Are events <m>A</m> and <m>D</m> disjoint?
          </p>
        </statement>
        <solution>
          <p>
            (a) Outcomes 2 and 3. (b) Yes, events <m>B</m> and <m>D</m> are disjoint because they share no outcomes. (c) The 
            events <m>A</m> and <m>D</m> share an outcome in common, 2, and so are not disjoint.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-prob-b-or-d">
        <statement>
          <p>
            In <xref ref="ex-examining-disjoint-sets-abd"/>, you confirmed <m>B</m> and <m>D</m> from <xref ref="fig-disjoint-sets"/> 
            are disjoint. Compute the probability that event <m>B</m> or event <m>D</m> occurs.
          </p>
        </statement>
        <solution>
          <p>
            Since <m>B</m> and <m>D</m> are disjoint events, use the Addition Rule: <m>P(B</m> or <m>D) = P(B) + P(D) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}</m>.
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-probabilities-not-disjoint">
      <title>Probabilities when events are not disjoint</title>
      
      <p>
        Let's consider calculations for two events that are not disjoint in the context of a regular deck 
        of 52 cards, represented in <xref ref="fig-deck-of-cards"/>. If you are unfamiliar with the cards 
        in a regular deck, please see the footnote.<fn>The 52 cards are split into four <term>suits</term>: 
        <m>\clubsuit</m> (club), <m>\diamondsuit</m> (diamond), <m>\heartsuit</m> (heart), <m>\spadesuit</m> (spade). 
        Each suit has its 13 cards labeled: 2, 3, <ellipsis/>, 10, J (jack), Q (queen), K (king), and 
        A (ace). Thus, each card is a unique combination of a suit and a label, e.g. <m>4\heartsuit</m> and 
        <m>J\clubsuit</m>. The 12 cards represented by the jacks, queens, and kings are called <term>face cards</term>. 
        The cards that are <m>\diamondsuit</m> or <m>\heartsuit</m> are typically colored red while the other 
        two suits are typically colored black.</fn>
      </p>

      <figure xml:id="fig-deck-of-cards">
        <caption>Representations of the 52 unique cards in a deck.</caption>
        <image source="ch_probability/figures/deckOfCards" width="85%"/>
      </figure>

      <exercise xml:id="ex-diamond-face-card-prob">
        <statement>
          <p>
            (a) What is the probability that a randomly selected card is a diamond? (b) What is the 
            probability that a randomly selected card is a face card?
          </p>
        </statement>
        <solution>
          <p>
            (a) There are 52 cards and 13 diamonds. If the cards are thoroughly shuffled, each card has 
            an equal chance of being drawn, so the probability that a randomly selected card is a diamond 
            is <m>P(\diamondsuit) = \frac{13}{52} = 0.250</m>. (b) Likewise, there are 12 face cards, so 
            <m>P(\text{face card}) = \frac{12}{52} = \frac{3}{13} = 0.231</m>.
          </p>
        </solution>
      </exercise>

      <p>
        <term>Venn diagrams</term> are useful when outcomes can be categorized as <q>in</q> or <q>out</q> 
        for two or three variables, attributes, or random processes. The Venn diagram in 
        <xref ref="fig-cards-diamond-face-venn"/> uses a circle to represent diamonds and another to 
        represent face cards. If a card is both a diamond and a face card, it falls into the intersection 
        of the circles. If it is a diamond but not a face card, it will be in part of the left circle that 
        is not in the right circle (and so on). The total number of cards that are diamonds is given by 
        the total number of cards in the diamonds circle: <m>10+3=13</m>. The probabilities are also shown 
        (e.g. <m>10/52 = 0.1923</m>).
      </p>

      <figure xml:id="fig-cards-diamond-face-venn">
        <caption>A Venn diagram for diamonds and face cards.</caption>
        <image source="ch_probability/figures/cardsDiamondFaceVenn" width="65%"/>
      </figure>

      <p>
        Let <m>A</m> represent the event that a randomly selected card is a diamond and <m>B</m> represent 
        the event that it is a face card. How do we compute <m>P(A \text{ or } B)</m>? Events <m>A</m> and 
        <m>B</m> are not disjoint <mdash/> the cards <m>J\diamondsuit</m>, <m>Q\diamondsuit</m>, and 
        <m>K\diamondsuit</m> fall into both categories <mdash/> so we cannot use the Addition Rule for 
        disjoint events. Instead we use the Venn diagram. We start by adding the probabilities of the two events:
        <me>P(A) + P(B) = P(\diamondsuit) + P(\text{face card}) = 13/52 + 12/52</me>
        However, the three cards that are in both events were counted twice, once in each probability. We 
        must correct this double counting:
        <md>
          <mrow>P(A\text{ or } B) \amp= P(\diamondsuit\text{ or face card})</mrow>
          <mrow>\amp= P(\diamondsuit) + P(\text{face card}) - P(\diamondsuit\text{ and face card})</mrow>
          <mrow>\amp= 13/52 + 12/52 - 3/52</mrow>
          <mrow>\amp= 22/52 = 11/26</mrow>
        </md>
        This equation is an example of the <term>General Addition Rule</term>.
      </p>

      <assemblage xml:id="general-addition-rule">
        <title>General Addition Rule</title>
        <p>
          If <m>A</m> and <m>B</m> are any two events, disjoint or not, then the probability that at least 
          one of them will occur is
          <me>P(A\text{ or }B) = P(A) + P(B) - P(A\text{ and }B)</me>
          where <m>P(A \text{ and } B)</m> is the probability that both events occur.
        </p>
      </assemblage>

      <note>
        <title><q>or</q> is inclusive</title>
        <p>
          When we write <q>or</q> in statistics, we mean <q>and/or</q> unless we explicitly state otherwise. 
          Thus, <m>A</m> or <m>B</m> occurs means <m>A</m>, <m>B</m>, or both <m>A</m> and <m>B</m> occur.
        </p>
      </note>

      <exercise xml:id="ex-disjoint-implies-empty-intersection">
        <statement>
          <p>
            (a) If <m>A</m> and <m>B</m> are disjoint, describe why this implies <m>P(A \text{ and } B) = 0</m>. 
            (b) Using part (a), verify that the General Addition Rule simplifies to the simpler Addition Rule 
            for disjoint events if <m>A</m> and <m>B</m> are disjoint.
          </p>
        </statement>
        <solution>
          <p>
            (a) If <m>A</m> and <m>B</m> are disjoint, <m>A</m> and <m>B</m> can never occur simultaneously. 
            (b) If <m>A</m> and <m>B</m> are disjoint, then the last <m>P(A\text{ and }B)</m> term in the 
            General Addition Rule formula is 0 (see part (a)) and we are left with the Addition Rule for 
            disjoint events.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-loans-venn">
        <statement>
          <p>
            In the <c>loans</c> data set describing 10,000 loans, 1495 loans were from joint applications 
            (e.g. a couple applied together), 4789 applicants had a mortgage, and 950 had both of these 
            characteristics. Create a Venn diagram for this setup.
          </p>
        </statement>
        <solution>
          <p>
            Both the counts and corresponding probabilities (e.g. <m>3839/10000 = 0.384</m>) are shown. 
            Notice that the number of loans represented in the left circle corresponds to <m>3839 + 950 = 4789</m>, 
            and the number represented in the right circle is <m>950 + 545 = 1495</m>.
          </p>
          <figure>
            <image source="ch_probability/figures/loans_app_type_home_venn" width="50%"/>
          </figure>
        </solution>
      </exercise>

      <exercise xml:id="ex-loans-venn-prob">
        <statement>
          <p>
            (a) Use your Venn diagram from the previous exercise to determine the probability a randomly 
            drawn loan from the <c>loans</c> data set is from a joint application where the couple had a 
            mortgage. (b) What is the probability that the loan had either of these attributes?
          </p>
        </statement>
        <solution>
          <p>
            (a) The solution is represented by the intersection of the two circles: 0.095. (b) This is the 
            sum of the three disjoint probabilities shown in the circles: <m>0.384 + 0.095 + 0.055 = 0.534</m> 
            (off by 0.001 due to a rounding error).
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-probability-distributions">
      <title>Probability distributions</title>
      
      <p>
        A <term>probability distribution</term> is a table of all disjoint outcomes and their associated 
        probabilities. <xref ref="fig-dice-prob"/> shows the probability distribution for the sum of two dice.
      </p>

      <figure xml:id="fig-dice-prob">
        <caption>Probability distribution for the sum of two dice.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell>Dice sum</cell>
            <cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell>
            <cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell>
          </row>
          <row>
            <cell>Probability</cell>
            <cell><m>\frac{1}{36}</m></cell><cell><m>\frac{2}{36}</m></cell><cell><m>\frac{3}{36}</m></cell>
            <cell><m>\frac{4}{36}</m></cell><cell><m>\frac{5}{36}</m></cell><cell><m>\frac{6}{36}</m></cell>
            <cell><m>\frac{5}{36}</m></cell><cell><m>\frac{4}{36}</m></cell><cell><m>\frac{3}{36}</m></cell>
            <cell><m>\frac{2}{36}</m></cell><cell><m>\frac{1}{36}</m></cell>
          </row>
        </tabular>
      </figure>

      <assemblage xml:id="rules-probability-distributions">
        <title>Rules for probability distributions</title>
        <p>
          A probability distribution is a list of the possible outcomes with corresponding probabilities that 
          satisfies three rules:
        </p>
        <p>
          <ol>
            <li><p>The outcomes listed must be disjoint.</p></li>
            <li><p>Each probability must be between 0 and 1.</p></li>
            <li><p>The probabilities must total 1.</p></li>
          </ol>
        </p>
      </assemblage>

      <exercise xml:id="ex-us-household-income-dists">
        <statement>
          <p>
            <xref ref="fig-us-household-income-dists"/> suggests three distributions for household income 
            in the United States. Only one is correct. Which one must it be? What is wrong with the other two?
          </p>
        </statement>
        <solution>
          <p>
            The probabilities of (a) do not sum to 1. The second probability in (b) is negative. This leaves 
            (c), which sure enough satisfies the requirements of a distribution. One of the three was said 
            to be the actual distribution of US household incomes, so it must be (c).
          </p>
        </solution>
      </exercise>

      <figure xml:id="fig-us-household-income-dists">
        <caption>Proposed distributions of US household incomes.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell>Income Range</cell>
            <cell>$0-25k</cell><cell>$25k-50k</cell><cell>$50k-100k</cell><cell>$100k+</cell>
          </row>
          <row>
            <cell>(a)</cell>
            <cell>0.18</cell><cell>0.39</cell><cell>0.33</cell><cell>0.16</cell>
          </row>
          <row>
            <cell>(b)</cell>
            <cell>0.38</cell><cell>-0.27</cell><cell>0.52</cell><cell>0.37</cell>
          </row>
          <row>
            <cell>(c)</cell>
            <cell>0.28</cell><cell>0.27</cell><cell>0.29</cell><cell>0.16</cell>
          </row>
        </tabular>
      </figure>

      <p>
        Chapter 1 emphasized the importance of plotting data to provide quick summaries. Probability 
        distributions can also be summarized in a bar plot. For instance, the distribution of US household 
        incomes is shown in <xref ref="fig-us-household-income-dist-bar"/> as a bar plot. The probability 
        distribution for the sum of two dice is shown in <xref ref="fig-dice-prob"/> and plotted in 
        <xref ref="fig-dice-sum-dist"/>.
      </p>

      <figure xml:id="fig-us-household-income-dist-bar">
        <caption>The probability distribution of US household income.</caption>
        <image source="ch_probability/figures/usHouseholdIncomeDistBar" width="65%"/>
      </figure>

      <figure xml:id="fig-dice-sum-dist">
        <caption>The probability distribution of the sum of two dice.</caption>
        <image source="ch_probability/figures/diceSumDist" width="67%"/>
      </figure>

      <p>
        In these bar plots, the bar heights represent the probabilities of outcomes. If the outcomes are 
        numerical and discrete, it is usually (visually) convenient to make a bar plot that resembles a 
        histogram, as in the case of the sum of two dice. Another example of plotting the bars at their 
        respective locations is shown in <xref ref="fig-book-cost-dist"/> in <xref ref="sec-random-variables"/>.
      </p>
    </subsection>

    <subsection xml:id="subsec-complement">
      <title>Complement of an event</title>
      
      <p>
        Rolling a die produces a value in the set <m>\{1, 2, 3, 4, 5, 6\}</m>. This set of all possible 
        outcomes is called the <term>sample space</term> (<m>S</m>) for rolling a die. We often use the 
        sample space to examine the scenario where an event does not occur.
      </p>

      <p>
        Let <m>D=\{2, 3\}</m> represent the event that the outcome of a die roll is 2 or 3. Then the 
        <term>complement</term> of <m>D</m> represents all outcomes in our sample space that are not in 
        <m>D</m>, which is denoted by <m>D^c = \{1, 4, 5, 6\}</m>. That is, <m>D^c</m> is the set of all 
        possible outcomes not already included in <m>D</m>. <xref ref="fig-complement-of-d"/> shows the 
        relationship between <m>D</m>, <m>D^c</m>, and the sample space <m>S</m>.
      </p>

      <figure xml:id="fig-complement-of-d">
        <caption>Event <m>D=\{2, 3\}</m> and its complement, <m>D^c = \{1, 4, 5, 6\}</m>. <m>S</m> 
          represents the sample space, which is the set of all possible outcomes.</caption>
        <image source="ch_probability/figures/complementOfD" width="55%"/>
      </figure>

      <exercise xml:id="ex-complement-dice">
        <statement>
          <p>
            (a) Compute <m>P(D^c) = P(\text{rolling a } 1, 4, 5, \text{ or } 6)</m>. (b) What is <m>P(D) + P(D^c)</m>?
          </p>
        </statement>
        <solution>
          <p>
            (a) The outcomes are disjoint and each has probability <m>1/6</m>, so the total probability is 
            <m>4/6=2/3</m>. (b) We can also see that <m>P(D)=\frac{1}{6} + \frac{1}{6} = 1/3</m>. Since <m>D</m> 
            and <m>D^c</m> are disjoint, <m>P(D) + P(D^c) = 1</m>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-complement-ab">
        <statement>
          <p>
            Events <m>A=\{1, 2\}</m> and <m>B=\{4, 6\}</m> are shown in <xref ref="fig-disjoint-sets"/>. 
            (a) Write out what <m>A^c</m> and <m>B^c</m> represent. (b) Compute <m>P(A^c)</m> and <m>P(B^c)</m>. 
            (c) Compute <m>P(A)+P(A^c)</m> and <m>P(B)+P(B^c)</m>.
          </p>
        </statement>
        <solution>
          <p>
            Brief solutions: (a) <m>A^c=\{3, 4, 5, 6\}</m> and <m>B^c=\{1, 2, 3, 5\}</m>. (b) Noting that 
            each outcome is disjoint, add the individual outcome probabilities to get <m>P(A^c)=2/3</m> and 
            <m>P(B^c)=2/3</m>. (c) <m>A</m> and <m>A^c</m> are disjoint, and the same is true of <m>B</m> 
            and <m>B^c</m>. Therefore, <m>P(A) + P(A^c) = 1</m> and <m>P(B) + P(B^c) = 1</m>.
          </p>
        </solution>
      </exercise>

      <p>
        A complement of an event <m>A</m> is constructed to have two very important properties: (i) every 
        possible outcome not in <m>A</m> is in <m>A^c</m>, and (ii) <m>A</m> and <m>A^c</m> are disjoint. 
        Property (i) implies
        <me>P(A\text{ or }A^c) = 1</me>
        That is, if the outcome is not in <m>A</m>, it must be represented in <m>A^c</m>. We use the 
        Addition Rule for disjoint events to apply Property (ii):
        <me>P(A\text{ or }A^c) = P(A) + P(A^c)</me>
        Combining the last two equations yields a very useful relationship between the probability of an 
        event and its complement.
      </p>

      <assemblage xml:id="complement-rule">
        <title>Complement</title>
        <p>
          The complement of event <m>A</m> is denoted <m>A^c</m>, and <m>A^c</m> represents all outcomes 
          not in <m>A</m>. <m>A</m> and <m>A^c</m> are mathematically related:
          <me>P(A) + P(A^c) = 1, \quad\text{i.e.}\quad P(A) = 1-P(A^c)</me>
        </p>
      </assemblage>

      <p>
        In simple examples, computing <m>A</m> or <m>A^c</m> is feasible in a few steps. However, using 
        the complement can save a lot of time as problems grow in complexity.
      </p>

      <exercise xml:id="ex-dice-sum-less-than-12">
        <statement>
          <p>
            Let <m>A</m> represent the event where we roll two dice and their total is less than 12. (a) What 
            does the event <m>A^c</m> represent? (b) Determine <m>P(A^c)</m> from <xref ref="fig-dice-prob"/>. 
            (c) Determine <m>P(A)</m>.
          </p>
        </statement>
        <solution>
          <p>
            (a) The complement of <m>A</m>: when the total is equal to 12. (b) <m>P(A^c) = 1/36</m>. (c) Use 
            the probability of the complement from part (b), <m>P(A^c) = 1/36</m>, and the equation for the 
            complement: <m>P(\text{less than } 12) = 1 - P(12) = 1 - 1/36 = 35/36</m>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-dice-sum-complements">
        <statement>
          <p>
            Find the following probabilities for rolling two dice:
          </p>
          <p>
            <ol marker="(a)">
              <li><p>The sum of the dice is <em>not</em> 6.</p></li>
              <li><p>The sum is at least 4. That is, determine the probability of the event <m>B = \{4, 5, \ldots, 12\}</m>.</p></li>
              <li><p>The sum is no more than 10. That is, determine the probability of the event <m>D=\{2, 3, \ldots, 10\}</m>.</p></li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            (a) First find <m>P(6)=5/36</m>, then use the complement: <m>P(\text{not } 6) = 1 - P(6) = 31/36</m>. 
            (b) First find the complement, which requires much less effort: <m>P(2 \text{ or } 3)=1/36+2/36=1/12</m>. 
            Then calculate <m>P(B) = 1-P(B^c) = 1-1/12 = 11/12</m>. (c) As before, finding the complement is 
            the clever way to determine <m>P(D)</m>. First find <m>P(D^c) = P(11 \text{ or } 12)=2/36 + 1/36=1/12</m>. 
            Then calculate <m>P(D) = 1 - P(D^c) = 11/12</m>.
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-independence">
      <title>Independence</title>
      
      <p>
        Just as variables and observations can be independent, random processes can be independent, too. Two 
        processes are <term>independent</term> if knowing the outcome of one provides no useful information 
        about the outcome of the other. For instance, flipping a coin and rolling a die are two independent 
        processes <mdash/> knowing the coin was heads does not help determine the outcome of a die roll. On 
        the other hand, stock prices usually move up or down together, so they are not independent.
      </p>

      <p>
        <xref ref="ex-prob-of-2-ones"/> provides a basic example of two independent processes: rolling two 
        dice. We want to determine the probability that both will be 1. Suppose one of the dice is red and 
        the other white. If the outcome of the red die is a 1, it provides no information about the outcome 
        of the white die. We first encountered this same question in <xref ref="ex-prob-of-2-ones"/>, where 
        we calculated the probability using the following reasoning: <m>1/6</m> of the time the red die is a 
        1, and <m>1/6</m> of <em>those</em> times the white die will also be a 1. This is illustrated in 
        <xref ref="fig-indep-for-rolling-two-1s"/>. Because the rolls are independent, the probabilities of 
        the corresponding outcomes can be multiplied to get the final answer: <m>(1/6)\times(1/6)=1/36</m>. 
        This can be generalized to many independent processes.
      </p>

      <figure xml:id="fig-indep-for-rolling-two-1s">
        <caption><m>1/6</m> of the time, the first roll is a 1. Then <m>1/6</m> of <em>those</em> times, 
          the second roll will also be a 1.</caption>
        <image source="ch_probability/figures/indepForRollingTwo1s" width="60%"/>
      </figure>

      <example xml:id="ex-three-dice">
        <statement>
          <p>
            What if there was also a blue die independent of the other two? What is the probability of 
            rolling the three dice and getting all 1s?
          </p>
        </statement>
        <solution>
          <p>
            The same logic applies from <xref ref="ex-prob-of-2-ones"/>. If <m>1/36</m> of the time the 
            white and red dice are both 1, then <m>1/6</m> of <em>those</em> times the blue die will also 
            be 1, so multiply:
            <md>
              <mrow>P(\text{white}=1 \text{ and } \text{red}=1 \text{ and } \text{blue}=1)</mrow>
              <mrow>\amp= P(\text{white}=1)\times P(\text{red}=1)\times P(\text{blue}=1)</mrow>
              <mrow>\amp= (1/6)\times (1/6)\times (1/6) = 1/216</mrow>
            </md>
          </p>
        </solution>
      </example>

      <p>
        <xref ref="ex-three-dice"/> illustrates what is called the Multiplication Rule for independent processes.
      </p>

      <assemblage xml:id="multiplication-rule-independent">
        <title>Multiplication Rule for independent processes</title>
        <p>
          If <m>A</m> and <m>B</m> represent events from two different and independent processes, then the 
          probability that both <m>A</m> and <m>B</m> occur can be calculated as the product of their 
          separate probabilities:
          <me>P(A \text{ and }B) = P(A) \times  P(B)</me>
          Similarly, if there are <m>k</m> events <m>A_1, \ldots, A_k</m> from <m>k</m> independent processes, 
          then the probability they all occur is
          <me>P(A_1) \times  P(A_2)\times  \cdots \times  P(A_k)</me>
        </p>
      </assemblage>

      <exercise xml:id="ex-two-handedness">
        <statement>
          <p>
            About 9% of people are left-handed. Suppose 2 people are selected at random from the U.S. 
            population. Because the sample size of 2 is very small relative to the population, it is 
            reasonable to assume these two people are independent. (a) What is the probability that both 
            are left-handed? (b) What is the probability that both are right-handed?
          </p>
        </statement>
        <solution>
          <p>
            (a) The probability the first person is left-handed is 0.09, which is the same for the second 
            person. We apply the Multiplication Rule for independent processes to determine the probability 
            that both will be left-handed: <m>0.09\times 0.09 = 0.0081</m>. (b) It is reasonable to assume 
            the proportion of people who are ambidextrous (both right- and left-handed) is nearly 0, which 
            results in <m>P(\text{right-handed})=1-0.09=0.91</m>. Using the same reasoning as in part (a), 
            the probability that both will be right-handed is <m>0.91\times 0.91 = 0.8281</m>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-five-handedness">
        <statement>
          <p>
            Suppose 5 people are selected at random.
          </p>
          <p>
            <ol marker="(a)">
              <li><p>What is the probability that all are right-handed?</p></li>
              <li><p>What is the probability that all are left-handed?</p></li>
              <li><p>What is the probability that not all of the people are right-handed?</p></li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            (a) The abbreviations RH and LH are used for right-handed and left-handed, respectively. Since 
            each are independent, we apply the Multiplication Rule for independent processes:
            <md>
              <mrow>P(\text{all five are RH}) \amp= P(\text{first = RH, second = RH, ..., fifth = RH})</mrow>
              <mrow>\amp= P(\text{first = RH})\times P(\text{second = RH})\times  \cdots \times P(\text{fifth = RH})</mrow>
              <mrow>\amp= 0.91\times 0.91\times 0.91\times 0.91\times 0.91 = 0.624</mrow>
            </md>
            (b) Using the same reasoning as in (a), <m>0.09\times 0.09\times 0.09\times 0.09\times 0.09 = 0.0000059</m>. 
            (c) Use the complement, <m>P(\text{all five are RH})</m>, to answer this question:
            <me>P(\text{not all RH}) = 1 - P(\text{all RH}) = 1 - 0.624 = 0.376</me>
          </p>
        </solution>
      </exercise>

      <p>
        Suppose the variables <c>handedness</c> and <c>sex</c> are independent, i.e. knowing someone's sex 
        provides no useful information about their handedness and vice-versa. Then we can compute whether a 
        randomly selected person is right-handed and female<fn>The actual proportion of the U.S. population 
        that is female is about 50%, and so we use 0.5 for the probability of sampling a woman. However, 
        this probability does differ in other countries.</fn> using the Multiplication Rule:
        <md>
          <mrow>P(\text{right-handed and female}) \amp= P(\text{right-handed}) \times  P(\text{female})</mrow>
          <mrow>\amp= 0.91 \times  0.50 = 0.455</mrow>
        </md>
      </p>

      <exercise xml:id="ex-three-people-sex-handedness">
        <statement>
          <p>
            Three people are selected at random.
          </p>
          <p>
            <ol marker="(a)">
              <li><p>What is the probability that the first person is male and right-handed?</p></li>
              <li><p>What is the probability that the first two people are male and right-handed?</p></li>
              <li><p>What is the probability that the third person is female and left-handed?</p></li>
              <li><p>What is the probability that the first two people are male and right-handed and the 
                third person is female and left-handed?</p></li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            Brief answers are provided. (a) This can be written in probability notation as <m>P(\text{a randomly 
            selected person is male and right-handed})=0.455</m>. (b) 0.207. (c) 0.045. (d) 0.0093.
          </p>
        </solution>
      </exercise>

      <p>
        Sometimes we wonder if one outcome provides useful information about another outcome. The question we 
        are asking is, are the occurrences of the two events independent? We say that two events <m>A</m> and 
        <m>B</m> are independent if they satisfy <m>P(A \text{ and }B) = P(A) \times  P(B)</m>.
      </p>

      <example xml:id="ex-heart-ace-independent">
        <statement>
          <p>
            If we shuffle up a deck of cards and draw one, is the event that the card is a heart independent 
            of the event that the card is an ace?
          </p>
        </statement>
        <solution>
          <p>
            The probability the card is a heart is <m>1/4</m> and the probability that it is an ace is 
            <m>1/13</m>. The probability the card is the ace of hearts is <m>1/52</m>. We check whether 
            <m>P(A \text{ and }B) = P(A) \times  P(B)</m> is satisfied:
            <me>P(\heartsuit)\times P(\text{ace}) = \frac{1}{4}\times \frac{1}{13} = \frac{1}{52} = P(\heartsuit\text{ and ace})</me>
            Because the equation holds, the event that the card is a heart and the event that the card is 
            an ace are independent events.
          </p>
        </solution>
      </example>
    </subsection>
  </section>

  <!-- Section 3.2: Conditional probability -->
  <section xml:id="sec-conditional-probability">
    <title>Conditional probability</title>

    <introduction>
      <p>
        There can be rich relationships between two or more variables that are useful to understand. 
        For example a car insurance company will consider information about a person's driving history to 
        assess the risk that they will be responsible for an accident. These types of relationships are 
        the realm of conditional probabilities.
      </p>
    </introduction>

    <subsection xml:id="subsec-exploring-contingency-table">
      <title>Exploring probabilities with a contingency table</title>
      
      <p>
        The <c>photo_classify</c> data set represents a classifier for a sample of 1822 photos from a photo 
        sharing website. Data scientists have been working to improve a classifier for whether the photo is 
        about fashion or not, and these 1822 photos represent a test for their classifier. Each photo gets 
        two classifications: the first is called <c>mach_learn</c> and gives a classification from a machine 
        learning (ML) system of either <q>pred_fashion</q> or <q>pred_not</q>. Each of these 1822 photos have 
        also been classified carefully by a team of people, which we take to be the source of truth; this 
        variable is called <c>truth</c> and takes values <q>fashion</q> and <q>not</q>. 
        <xref ref="fig-contingency-fashion"/> summarizes the results.
      </p>

      <figure xml:id="fig-contingency-fashion">
        <caption>Contingency table summarizing the <c>photo_classify</c> data set.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell></cell>
            <cell colspan="2">truth</cell>
            <cell></cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell>fashion</cell>
            <cell>not</cell>
            <cell>Total</cell>
          </row>
          <row>
            <cell>mach_learn: pred_fashion</cell>
            <cell>197</cell>
            <cell>22</cell>
            <cell>219</cell>
          </row>
          <row>
            <cell>mach_learn: pred_not</cell>
            <cell>112</cell>
            <cell>1491</cell>
            <cell>1603</cell>
          </row>
          <row bottom="minor">
            <cell>Total</cell>
            <cell>309</cell>
            <cell>1513</cell>
            <cell>1822</cell>
          </row>
        </tabular>
      </figure>

      <figure xml:id="fig-photo-classify-venn">
        <caption>A Venn diagram using boxes for the <c>photo_classify</c> data set.</caption>
        <image source="ch_probability/figures/photoClassifyVenn" width="65%"/>
      </figure>

      <example xml:id="ex-ml-classifier-accuracy">
        <statement>
          <p>
            If a photo is actually about fashion, what is the chance the ML classifier correctly identified 
            the photo as being about fashion?
          </p>
        </statement>
        <solution>
          <p>
            We can estimate this probability using the data. Of the 309 fashion photos, the ML algorithm 
            correctly classified 197 of the photos:
            <me>P(\text{mach_learn is pred_fashion} \mid \text{truth is fashion}) = \frac{197}{309} = 0.638</me>
          </p>
        </solution>
      </example>

      <example xml:id="ex-ml-false-negative">
        <statement>
          <p>
            We sample a photo from the data set and learn the ML algorithm predicted this photo was not about 
            fashion. What is the probability that it was incorrect and the photo is about fashion?
          </p>
        </statement>
        <solution>
          <p>
            If the ML classifier suggests a photo is not about fashion, then it comes from the second row in 
            the data set. Of these 1603 photos, 112 were actually about fashion:
            <me>P(\text{truth is fashion} \mid \text{mach_learn is pred_not}) = \frac{112}{1603} = 0.070</me>
          </p>
        </solution>
      </example>
    </subsection>

    <subsection xml:id="subsec-marginal-joint-probabilities">
      <title>Marginal and joint probabilities</title>
      
      <p>
        <xref ref="fig-contingency-fashion"/> includes row and column totals for each variable separately 
        in the <c>photo_classify</c> data set. These totals represent <term>marginal probabilities</term> 
        for the sample, which are the probabilities based on a single variable without regard to any other 
        variables. For instance, a probability based solely on the <c>mach_learn</c> variable is a marginal 
        probability:
        <me>P(\text{mach_learn is pred_fashion}) = \frac{219}{1822} = 0.12</me>
        A probability of outcomes for two or more variables or processes is called a <term>joint probability</term>:
        <me>P(\text{mach_learn is pred_fashion and truth is fashion}) = \frac{197}{1822} = 0.11</me>
        It is common to substitute a comma for <q>and</q> in a joint probability, although using either the 
        word <q>and</q> or a comma is acceptable.
      </p>

      <assemblage xml:id="marginal-joint-def">
        <title>Marginal and joint probabilities</title>
        <p>
          If a probability is based on a single variable, it is a <term>marginal probability</term>. 
          The probability of outcomes for two or more variables or processes is called a 
          <term>joint probability</term>.
        </p>
      </assemblage>

      <p>
        We use <term>table proportions</term> to summarize joint probabilities for the <c>photo_classify</c> 
        sample. These proportions are computed by dividing each count by the table's total, 1822, to obtain 
        the probabilities in <xref ref="fig-photo-classify-prob-table"/>. The joint probability distribution 
        of the <c>mach_learn</c> and <c>truth</c> variables is shown in <xref ref="fig-photo-classify-dist"/>.
      </p>

      <figure xml:id="fig-photo-classify-prob-table">
        <caption>Probability table summarizing the <c>photo_classify</c> data set.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell></cell>
            <cell>truth: fashion</cell>
            <cell>truth: not</cell>
            <cell>Total</cell>
          </row>
          <row>
            <cell>mach_learn: pred_fashion</cell>
            <cell>0.1081</cell>
            <cell>0.0121</cell>
            <cell>0.1202</cell>
          </row>
          <row bottom="minor">
            <cell>mach_learn: pred_not</cell>
            <cell>0.0615</cell>
            <cell>0.8183</cell>
            <cell>0.8798</cell>
          </row>
          <row>
            <cell>Total</cell>
            <cell>0.1696</cell>
            <cell>0.8304</cell>
            <cell>1.00</cell>
          </row>
        </tabular>
      </figure>

      <figure xml:id="fig-photo-classify-dist">
        <caption>Joint probability distribution for the <c>photo_classify</c> data set.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell>Joint outcome</cell>
            <cell>Probability</cell>
          </row>
          <row>
            <cell>mach_learn is pred_fashion and truth is fashion</cell>
            <cell>0.1081</cell>
          </row>
          <row>
            <cell>mach_learn is pred_fashion and truth is not</cell>
            <cell>0.0121</cell>
          </row>
          <row>
            <cell>mach_learn is pred_not and truth is fashion</cell>
            <cell>0.0615</cell>
          </row>
          <row>
            <cell>mach_learn is pred_not and truth is not</cell>
            <cell>0.8183</cell>
          </row>
          <row bottom="minor">
            <cell>Total</cell>
            <cell>1.0000</cell>
          </row>
        </tabular>
      </figure>

      <exercise xml:id="ex-verify-photo-dist">
        <statement>
          <p>
            Verify <xref ref="fig-photo-classify-dist"/> represents a probability distribution: events are 
            disjoint, all probabilities are non-negative, and the probabilities sum to 1.
          </p>
        </statement>
        <solution>
          <p>
            Each of the four outcome combinations are disjoint, all probabilities are indeed non-negative, 
            and the sum of the probabilities is <m>0.1081 + 0.0121 + 0.0615 + 0.8183 = 1.00</m>.
          </p>
        </solution>
      </exercise>

      <p>
        We can compute marginal probabilities using joint probabilities in simple cases. For example, the 
        probability a randomly selected photo from the data set is about fashion is found by summing the 
        outcomes where <c>truth</c> takes value <q>fashion</q>:
        <md>
          <mrow>P(\text{truth is fashion}) \amp= P(\text{mach_learn is pred_fashion and truth is fashion})</mrow>
          <mrow>\amp\quad + P(\text{mach_learn is pred_not and truth is fashion})</mrow>
          <mrow>\amp= 0.1081 + 0.0615 = 0.1696</mrow>
        </md>
      </p>
    </subsection>

    <subsection xml:id="subsec-defining-conditional-probability">
      <title>Defining conditional probability</title>
      
      <p>
        The ML classifier predicts whether a photo is about fashion, even if it is not perfect. We would 
        like to better understand how to use information from a variable like <c>mach_learn</c> to improve 
        our probability estimation of a second variable, which in this example is <c>truth</c>.
      </p>

      <p>
        The probability that a random photo from the data set is about fashion is about 0.17. If we knew 
        the machine learning classifier predicted the photo was about fashion, could we get a better 
        estimate of the probability the photo is actually about fashion? Absolutely. To do so, we limit 
        our view to only those 219 cases where the ML classifier predicted that the photo was about 
        fashion and look at the fraction where the photo was actually about fashion:
        <me>P(\text{truth is fashion} \mid \text{mach_learn is pred_fashion}) = \frac{197}{219} = 0.900</me>
        We call this a <term>conditional probability</term> because we computed the probability under a 
        condition: the ML classifier prediction said the photo was about fashion.
      </p>

      <p>
        There are two parts to a conditional probability, the <term>outcome of interest</term> and the 
        <term>condition</term>. It is useful to think of the condition as information we know to be true, 
        and this information usually can be described as a known outcome or event. We generally separate 
        the text inside our probability notation into the outcome of interest and the condition with a 
        vertical bar:
        <me>P(\text{truth is fashion} \mid \text{mach_learn is pred_fashion}) = \frac{197}{219} = 0.900</me>
        The vertical bar <q><m>|</m></q> is read as <em>given</em>.
      </p>

      <p>
        In this equation, we computed the probability a photo was about fashion based on the condition that 
        the ML algorithm predicted it was about fashion as a fraction:
        <me>\frac{\text{number of cases where truth is fashion and mach_learn is pred_fashion}}{\text{number of cases where mach_learn is pred_fashion}} = \frac{197}{219} = 0.900</me>
        We considered only those cases that met the condition (<c>mach_learn</c> is <q>pred_fashion</q>) and 
        then we computed the ratio of those cases that satisfied our outcome of interest (photo was actually 
        about fashion).
      </p>

      <p>
        Frequently, marginal and joint probabilities are provided instead of count data. For example, disease 
        rates are commonly listed in percentages rather than in a count format. We would like to be able to 
        compute conditional probabilities even when no counts are available, and we use the previous equation 
        as a template to understand this technique.
      </p>

      <p>
        If we were provided only the information in <xref ref="fig-photo-classify-prob-table"/>, i.e. only 
        probability data, then if we took a sample of 1000 photos, we would anticipate about 12.0% or 
        <m>0.120 \times 1000 = 120</m> would be predicted to be about fashion. Similarly, we would expect 
        about 10.8% or <m>0.108 \times 1000 = 108</m> to meet both the information criteria and represent 
        our outcome of interest. Then the conditional probability can be computed as:
        <me>\frac{108}{120} = \frac{0.108}{0.120} = 0.90</me>
        Here we are examining exactly the fraction of two probabilities, 0.108 and 0.120, which we can write as:
        <me>P(\text{truth is fashion and mach_learn is pred_fashion}) \quad\text{and}\quad P(\text{mach_learn is pred_fashion})</me>
        The fraction of these probabilities is an example of the general formula for conditional probability.
      </p>

      <assemblage xml:id="conditional-probability-def">
        <title>Conditional probability</title>
        <p>
          The conditional probability of outcome <m>A</m> given condition <m>B</m> is computed as:
          <me>P(A \mid B) = \frac{P(A \text{ and } B)}{P(B)}</me>
        </p>
      </assemblage>

      <exercise xml:id="ex-conditional-ml-correct">
        <statement>
          <p>
            (a) Write out the following statement in conditional probability notation: <q>The probability 
            that the ML prediction was correct, if the photo was about fashion</q>. Here the condition is now 
            based on the photo's <c>truth</c> status, not the ML algorithm. (b) Determine the probability 
            from part (a). <xref ref="fig-photo-classify-prob-table"/> may be helpful.
          </p>
        </statement>
        <solution>
          <p>
            (a) <me>P(\text{mach_learn is pred_fashion} \mid \text{truth is fashion})</me>
            (b) The equation for conditional probability indicates we should first find 
            <m>P(\text{mach_learn is pred_fashion and truth is fashion}) = 0.1081</m> and 
            <m>P(\text{truth is fashion}) = 0.1696</m>. Then the ratio represents the conditional probability: 
            <m>0.1081 / 0.1696 = 0.6374</m>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-cond-prob-sum-to-1">
        <statement>
          <p>
            (a) Determine the probability that the algorithm is incorrect if it is known the photo is about 
            fashion. (b) Using part (a) and the previous exercise, compute the sum of the two conditional 
            probabilities. (c) Provide an intuitive argument to explain why this sum is 1.
          </p>
        </statement>
        <solution>
          <p>
            (a) <me>P(\text{mach_learn is pred_not} \mid \text{truth is fashion}) = \frac{0.0615}{0.1696} = 0.3626</me>
            (b) The total equals 1. (c) Under the condition the photo is about fashion, the ML algorithm must 
            have either predicted it was about fashion or predicted it was not about fashion. The complement 
            still works for conditional probabilities, provided the probabilities are conditioned on the same 
            information.
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-smallpox-boston">
      <title>Smallpox in Boston, 1721</title>
      
      <p>
        The <c>smallpox</c> data set provides a sample of 6,224 individuals from the year 1721 who were 
        exposed to smallpox in Boston. Doctors at the time believed that inoculation, which involves 
        exposing a person to the disease in a controlled form, could reduce the likelihood of death.
      </p>

      <p>
        Each case represents one person with two variables: <c>inoculated</c> and <c>result</c>. The variable 
        <c>inoculated</c> takes two levels: <q>yes</q> or <q>no</q>, indicating whether the person was 
        inoculated or not. The variable <c>result</c> has outcomes <q>lived</q> or <q>died</q>. These data 
        are summarized in <xref ref="fig-smallpox-contingency"/> and <xref ref="fig-smallpox-prob"/>.
      </p>

      <figure xml:id="fig-smallpox-contingency">
        <caption>Contingency table for the <c>smallpox</c> data set.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell></cell>
            <cell colspan="2">inoculated</cell>
            <cell></cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell>yes</cell>
            <cell>no</cell>
            <cell>Total</cell>
          </row>
          <row>
            <cell>lived</cell>
            <cell>238</cell>
            <cell>5136</cell>
            <cell>5374</cell>
          </row>
          <row bottom="minor">
            <cell>died</cell>
            <cell>6</cell>
            <cell>844</cell>
            <cell>850</cell>
          </row>
          <row>
            <cell>Total</cell>
            <cell>244</cell>
            <cell>5980</cell>
            <cell>6224</cell>
          </row>
        </tabular>
      </figure>

      <figure xml:id="fig-smallpox-prob">
        <caption>Table proportions for the <c>smallpox</c> data, computed by dividing each count by the table 
          total, 6224.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell></cell>
            <cell colspan="2">inoculated</cell>
            <cell></cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell>yes</cell>
            <cell>no</cell>
            <cell>Total</cell>
          </row>
          <row>
            <cell>lived</cell>
            <cell>0.0382</cell>
            <cell>0.8252</cell>
            <cell>0.8634</cell>
          </row>
          <row bottom="minor">
            <cell>died</cell>
            <cell>0.0010</cell>
            <cell>0.1356</cell>
            <cell>0.1366</cell>
          </row>
          <row>
            <cell>Total</cell>
            <cell>0.0392</cell>
            <cell>0.9608</cell>
            <cell>1.0000</cell>
          </row>
        </tabular>
      </figure>

      <exercise xml:id="ex-prob-died-not-inoculated">
        <statement>
          <p>
            Write out, in formal notation, the probability a randomly selected person who was not inoculated 
            died from smallpox, and find this probability.
          </p>
        </statement>
        <solution>
          <p>
            <me>P(\text{result} = \text{died} \mid \text{inoculated} = \text{no}) = \frac{P(\text{result} = \text{died and inoculated} = \text{no})}{P(\text{inoculated} = \text{no})} = \frac{0.1356}{0.9608} = 0.1411</me>
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-prob-died-inoculated">
        <statement>
          <p>
            Determine the probability that an inoculated person died from smallpox. How does this result 
            compare with the previous exercise?
          </p>
        </statement>
        <solution>
          <p>
            <me>P(\text{result} = \text{died} \mid \text{inoculated} = \text{yes}) = \frac{0.0010}{0.0392} = 0.0255</me>
            (If we avoided rounding errors, we'd get <m>6 / 244 = 0.0246</m>). The death rate for individuals 
            who were inoculated is only about 1 in 40 while the death rate is about 1 in 7 for those who were 
            not inoculated.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-smallpox-obs-exp">
        <statement>
          <p>
            The people of Boston self-selected whether or not to be inoculated. (a) Is this study observational 
            or was this an experiment? (b) Can we infer any causal connection using these data? (c) What are 
            some potential confounding variables that might influence whether someone lived or died and also 
            affect whether that person was inoculated?
          </p>
        </statement>
        <solution>
          <p>
            Brief answers: (a) Observational. (b) No, we cannot infer causation from this observational study. 
            (c) Accessibility to the latest and best medical care. There are other valid answers for part (c).
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-general-multiplication-rule">
      <title>General multiplication rule</title>
      
      <p>
        Section 3.1 on Independence introduced the Multiplication Rule for independent processes. Here we 
        provide the <term>General Multiplication Rule</term> for events that might not be independent.
      </p>

      <assemblage xml:id="general-mult-rule">
        <title>General Multiplication Rule</title>
        <p>
          If <m>A</m> and <m>B</m> represent two outcomes or events, then
          <me>P(A \text{ and } B) = P(A \mid B)\times P(B)</me>
          It is useful to think of <m>A</m> as the outcome of interest and <m>B</m> as the condition.
        </p>
      </assemblage>

      <p>
        This General Multiplication Rule is simply a rearrangement of the conditional probability equation.
      </p>

      <example xml:id="ex-smallpox-mult-rule">
        <statement>
          <p>
            Consider the <c>smallpox</c> data set. Suppose we are given only two pieces of information: 96.08% 
            of residents were not inoculated, and 85.88% of the residents who were not inoculated ended up 
            surviving. How could we compute the probability that a resident was not inoculated and lived?
          </p>
        </statement>
        <solution>
          <p>
            We will compute our answer using the General Multiplication Rule and then verify it using 
            <xref ref="fig-smallpox-prob"/>. We want to determine <m>P(\text{result} = \text{lived and inoculated} = \text{no})</m> 
            and we are given that <m>P(\text{result} = \text{lived} \mid \text{inoculated} = \text{no}) = 0.8588</m> and 
            <m>P(\text{inoculated} = \text{no}) = 0.9608</m>.
          </p>
          <p>
            Among the 96.08% of people who were not inoculated, 85.88% survived:
            <me>P(\text{result} = \text{lived and inoculated} = \text{no}) = 0.8588 \times 0.9608 = 0.8251</me>
            This is equivalent to the General Multiplication Rule. We can confirm this probability in 
            <xref ref="fig-smallpox-prob"/> at the intersection of <q>no</q> and <q>lived</q> (with a small 
            rounding error).
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-inoculated-lived">
        <statement>
          <p>
            Use <m>P(\text{inoculated} = \text{yes}) = 0.0392</m> and <m>P(\text{result} = \text{lived} \mid \text{inoculated} = \text{yes}) = 0.9754</m> 
            to determine the probability that a person was both inoculated and lived.
          </p>
        </statement>
        <solution>
          <p>
            The answer is 0.0382, which can be verified using <xref ref="fig-smallpox-prob"/>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-inoculated-died-pct">
        <statement>
          <p>
            If 97.54% of the inoculated people lived, what proportion of inoculated people must have died?
          </p>
        </statement>
        <solution>
          <p>
            There were only two possible outcomes: lived or died. This means that <m>100\% - 97.54\% = 2.46\%</m> 
            of the people who were inoculated died.
          </p>
        </solution>
      </exercise>

      <assemblage xml:id="sum-conditional-probs">
        <title>Sum of conditional probabilities</title>
        <p>
          Let <m>A_1, \ldots, A_k</m> represent all the disjoint outcomes for a variable or process. Then if 
          <m>B</m> is an event, possibly for another variable or process, we have:
          <me>P(A_1|B) + \cdots + P(A_k|B) = 1</me>
          The rule for complements also holds when an event and its complement are conditioned on the same 
          information:
          <me>P(A \mid B) = 1 - P(A^c \mid B)</me>
        </p>
      </assemblage>

      <exercise xml:id="ex-inoculation-effective">
        <statement>
          <p>
            Based on the probabilities computed above, does it appear that inoculation is effective at reducing 
            the risk of death from smallpox?
          </p>
        </statement>
        <solution>
          <p>
            The samples are large relative to the difference in death rates for the <q>inoculated</q> and 
            <q>not inoculated</q> groups, so it seems there is an association between <c>inoculated</c> and 
            <c>result</c>. However, this is an observational study and we cannot be sure if there is a causal 
            connection. (Further research has shown that inoculation is effective at reducing death rates.)
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-independence-conditional">
      <title>Independence considerations in conditional probability</title>
      
      <p>
        If two events are independent, then knowing the outcome of one should provide no information about 
        the other. We can show this is mathematically true using conditional probabilities.
      </p>

      <exercise xml:id="ex-dice-independence">
        <statement>
          <p>
            Let <m>X</m> and <m>Y</m> represent the outcomes of rolling two dice.
          </p>
          <p>
            <ol marker="(a)">
              <li><p>What is the probability that the first die, <m>X</m>, is 1?</p></li>
              <li><p>What is the probability that both <m>X</m> and <m>Y</m> are 1?</p></li>
              <li><p>Use the formula for conditional probability to compute <m>P(Y = 1 \mid X = 1)</m>.</p></li>
              <li><p>What is <m>P(Y=1)</m>? Is this different from the answer from part (c)? Explain.</p></li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            Brief solutions: (a) <m>1/6</m>. (b) <m>1/36</m>. (c) <m>\frac{P(Y = 1 \text{ and } X = 1)}{P(X = 1)} = \frac{1/36}{1/6} = 1/6</m>. 
            (d) The probability is the same as in part (c): <m>P(Y=1)=1/6</m>. The probability that <m>Y=1</m> 
            was unchanged by knowledge about <m>X</m>, which makes sense as <m>X</m> and <m>Y</m> are independent.
          </p>
        </solution>
      </exercise>

      <p>
        We can show in the above exercise that the conditioning information has no influence by using the 
        Multiplication Rule for independence processes:
        <md>
          <mrow>P(Y=1 \mid X=1) \amp= \frac{P(Y=1 \text{ and } X=1)}{P(X=1)}</mrow>
          <mrow>\amp= \frac{P(Y=1) \times P(X=1)}{P(X=1)}</mrow>
          <mrow>\amp= P(Y=1)</mrow>
        </md>
      </p>

      <exercise xml:id="ex-gamblers-fallacy">
        <statement>
          <p>
            Ron is watching a roulette table in a casino and notices that the last five outcomes were black. 
            He figures that the chances of getting black six times in a row is very small (about 1/64) and 
            puts his paycheck on red. What is wrong with his reasoning?
          </p>
        </statement>
        <solution>
          <p>
            He has forgotten that the next roulette spin is independent of the previous spins. This fallacy, 
            known as the <term>gambler's fallacy</term>, is commonly exploited by casinos which post the last 
            several outcomes of betting games.
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-tree-diagrams">
      <title>Tree diagrams</title>
      
      <p>
        <term>Tree diagrams</term> are a tool to organize outcomes and probabilities around the structure 
        of the data. They are most useful when two or more processes occur in a sequence and each process 
        is conditioned on its predecessors.
      </p>

      <p>
        The <c>smallpox</c> data fit this description. We see the population as split by <c>inoculation</c>: 
        yes and no. Following this split, survival rates were observed for each group. This structure is 
        reflected in the tree diagram shown in <xref ref="fig-smallpox-tree"/>.
      </p>

      <figure xml:id="fig-smallpox-tree">
        <caption>A tree diagram of the <c>smallpox</c> data set.</caption>
        <image source="ch_probability/figures/smallpoxTreeDiagram" width="93%"/>
      </figure>

      <p>
        Tree diagrams are annotated with marginal and conditional probabilities. This tree diagram splits 
        the smallpox data by <c>inoculation</c> into the yes and no groups with respective marginal 
        probabilities 0.0392 and 0.9608. The secondary branches are conditioned on the first, so we assign 
        conditional probabilities to these branches. We may construct joint probabilities at the end of each 
        branch in our tree by multiplying the numbers we come across as we move from left to right. These 
        joint probabilities are computed using the General Multiplication Rule:
        <md>
          <mrow>P(\text{inoculated} = \text{yes and result} = \text{lived})</mrow>
          <mrow>\amp= P(\text{inoculated} = \text{yes}) \times P(\text{result} = \text{lived} \mid \text{inoculated} = \text{yes})</mrow>
          <mrow>\amp= 0.0392 \times 0.9754 = 0.0382</mrow>
        </md>
      </p>

      <example xml:id="ex-test-scores-tree">
        <statement>
          <p>
            Consider the midterm and final for a statistics class. Suppose 13% of students earned an A on the 
            midterm. Of those students who earned an A on the midterm, 47% received an A on the final, and 11% 
            of the students who earned lower than an A on the midterm received an A on the final. You randomly 
            pick up a final exam and notice the student received an A. What is the probability that this 
            student earned an A on the midterm?
          </p>
        </statement>
        <solution>
          <p>
            The end-goal is to find <m>P(\text{midterm} = A \mid \text{final} = A)</m>. To calculate this 
            conditional probability, we need <m>P(\text{midterm} = A \text{ and final} = A)</m> and 
            <m>P(\text{final} = A)</m>. Since we aren't sure how to proceed, it is useful to organize the 
            information into a tree diagram (shown below).
          </p>
          <figure>
            <image source="ch_probability/figures/testTree" width="85%"/>
          </figure>
          <p>
            Variables provided with marginal probabilities are often used to create the tree's primary branches; 
            in this case, the marginal probabilities are provided for midterm grades. The final grades, which 
            correspond to the conditional probabilities provided, will be shown on the secondary branches.
          </p>
          <p>
            With the tree diagram constructed, we may compute the required probabilities:
            <md>
              <mrow>P(\text{midterm} = A \text{ and final} = A) \amp= 0.0611</mrow>
              <mrow>P(\text{final} = A) \amp= P(\text{midterm} = \text{other and final} = A) + P(\text{midterm} = A \text{ and final} = A)</mrow>
              <mrow>\amp= 0.0957 + 0.0611 = 0.1568</mrow>
            </md>
            The marginal probability <m>P(\text{final} = A)</m> was calculated by adding up all the joint 
            probabilities corresponding to final = A. We may now take the ratio of the two probabilities:
            <md>
              <mrow>P(\text{midterm} = A \mid \text{final} = A) \amp= \frac{P(\text{midterm} = A \text{ and final} = A)}{P(\text{final} = A)}</mrow>
              <mrow>\amp= \frac{0.0611}{0.1568} = 0.3897</mrow>
            </md>
            The probability the student also earned an A on the midterm is about 0.39.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-tree-diagram-pass">
        <statement>
          <p>
            After an introductory statistics course, 78% of students can successfully construct tree diagrams. 
            Of those who can construct tree diagrams, 97% passed, while only 57% of those students who could 
            not construct tree diagrams passed. (a) Organize this information into a tree diagram. (b) What is 
            the probability that a randomly selected student passed? (c) Compute the probability a student is 
            able to construct a tree diagram if it is known that she passed.
          </p>
        </statement>
        <solution>
          <p>
            (a) The tree diagram is provided below. (b) Identify which two joint probabilities represent 
            students who passed, and add them: <m>P(\text{passed}) = 0.7566+0.1254= 0.8820</m>. (c) 
            <m>P(\text{construct tree diagram} \mid \text{passed}) = \frac{0.7566}{0.8820} = 0.8578</m>.
          </p>
          <figure>
            <image source="ch_probability/figures/treeDiagramAndPass" width="70%"/>
          </figure>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-bayes-theorem">
      <title>Bayes' Theorem</title>
      
      <p>
        In many instances, we are given a conditional probability of the form <m>P(\text{statement about 
        variable 1} \mid \text{statement about variable 2})</m> but we would really like to know the inverted 
        conditional probability: <m>P(\text{statement about variable 2} \mid \text{statement about variable 1})</m>. 
        Tree diagrams can be used to find the second conditional probability when given the first. However, 
        sometimes it is not possible to draw the scenario in a tree diagram. In these cases, we can apply a 
        very useful and general formula: Bayes' Theorem.
      </p>

      <example xml:id="ex-mammogram">
        <statement>
          <p>
            In Canada, about 0.35% of women over 40 will develop breast cancer in any given year. A common 
            screening test for cancer is the mammogram, but this test is not perfect. In about 11% of 
            patients with breast cancer, the test gives a <term>false negative</term>: it indicates a woman 
            does not have breast cancer when she does have breast cancer. Similarly, the test gives a 
            <term>false positive</term> in 7% of patients who do not have breast cancer: it indicates these 
            patients have breast cancer when they actually do not. If we tested a random woman over 40 for 
            breast cancer using a mammogram and the test came back positive  that is, the test suggested 
            the patient has cancer  what is the probability that the patient actually has breast cancer?
          </p>
        </statement>
        <solution>
          <p>
            Notice that we are given sufficient information to quickly compute the probability of testing 
            positive if a woman has breast cancer (<m>1.00 - 0.11 = 0.89</m>). However, we seek the inverted 
            probability of cancer given a positive test result.
          </p>
          <p>
            This inverted probability can be broken into two pieces:
            <me>P(\text{has cancer} \mid \text{mammogram+}) = \frac{P(\text{has cancer and mammogram+})}{P(\text{mammogram+})}</me>
          </p>
          <p>
            We can construct a tree diagram to organize these probabilities:
          </p>
          <figure>
            <image source="ch_probability/figures/BreastCancerTreeDiagram" width="90%"/>
          </figure>
          <p>
            The probability the patient has breast cancer and the mammogram is positive is:
            <md>
              <mrow>P(\text{has cancer and mammogram+}) \amp= P(\text{mammogram+} \mid \text{has cancer})P(\text{has cancer})</mrow>
              <mrow>\amp= 0.89 \times 0.0035 = 0.00312</mrow>
            </md>
          </p>
          <p>
            The probability of a positive test result is the sum of the two corresponding scenarios:
            <md>
              <mrow>P(\text{mammogram+}) \amp= P(\text{mammogram+ and has cancer}) + P(\text{mammogram+ and no cancer})</mrow>
              <mrow>\amp= P(\text{has cancer})P(\text{mammogram+} \mid \text{has cancer})</mrow>
              <mrow>\amp\quad + P(\text{no cancer})P(\text{mammogram+} \mid \text{no cancer})</mrow>
              <mrow>\amp= 0.0035 \times 0.89 + 0.9965 \times 0.07 = 0.07288</mrow>
            </md>
          </p>
          <p>
            Then if the mammogram screening is positive for a patient, the probability the patient has 
            breast cancer is:
            <md>
              <mrow>P(\text{has cancer} \mid \text{mammogram+}) \amp= \frac{P(\text{has cancer and mammogram+})}{P(\text{mammogram+})}</mrow>
              <mrow>\amp= \frac{0.00312}{0.07288} \approx 0.0428</mrow>
            </md>
            That is, even if a patient has a positive mammogram screening, there is still only about a 4% 
            chance that she has breast cancer.
          </p>
        </solution>
      </example>

      <p>
        <xref ref="ex-mammogram"/> highlights why doctors often run more tests regardless of a first positive 
        test result. When a medical condition is rare, a single positive test isn't generally definitive.
      </p>

      <p>
        Consider again the last equation of <xref ref="ex-mammogram"/>. Using the tree diagram, we can see 
        that the numerator is:
        <me>P(\text{has cancer and mammogram+}) = P(\text{mammogram+} \mid \text{has cancer})P(\text{has cancer})</me>
        The denominatorthe probability the screening was positiveis the sum of probabilities for each 
        positive screening scenario:
        <md>
          <mrow>P(\text{mammogram+}) \amp= P(\text{mammogram+ and no cancer}) + P(\text{mammogram+ and has cancer})</mrow>
          <mrow>\amp= P(\text{mammogram+} \mid \text{no cancer})P(\text{no cancer})</mrow>
          <mrow>\amp\quad + P(\text{mammogram+} \mid \text{has cancer})P(\text{has cancer})</mrow>
        </md>
      </p>

      <assemblage xml:id="bayes-theorem">
        <title>Bayes' Theorem: inverting probabilities</title>
        <p>
          Consider the following conditional probability for variable 1 and variable 2:
          <me>P(\text{outcome } A_1 \text{ of variable 1} \mid \text{outcome } B \text{ of variable 2})</me>
          Bayes' Theorem states that this conditional probability can be identified as:
          <me>\frac{P(B \mid A_1) P(A_1)}{P(B \mid A_1) P(A_1) + P(B \mid A_2) P(A_2) + \cdots + P(B \mid A_k) P(A_k)}</me>
          where <m>A_2, A_3, \ldots, A_k</m> represent all other possible outcomes of the first variable.
        </p>
      </assemblage>

      <p>
        Bayes' Theorem is a generalization of what we have done using tree diagrams. The numerator identifies 
        the probability of getting both <m>A_1</m> and <m>B</m>. The denominator is the marginal probability 
        of getting <m>B</m>. This bottom component of the fraction appears long and complicated since we have 
        to add up probabilities from all of the different ways to get <m>B</m>. We always completed this step 
        when using tree diagrams. However, we usually did it in a separate step so it didn't seem as complex.
      </p>

      <p>
        To apply Bayes' Theorem correctly, there are two preparatory steps:
      </p>
      <p>
        <ol>
          <li><p>First identify the marginal probabilities of each possible outcome of the first variable: 
            <m>P(A_1), P(A_2), \ldots, P(A_k)</m>.</p></li>
          <li><p>Then identify the probability of the outcome <m>B</m>, conditioned on each possible scenario 
            for the first variable: <m>P(B \mid A_1), P(B \mid A_2), \ldots, P(B \mid A_k)</m>.</p></li>
        </ol>
      </p>
      <p>
        Once each of these probabilities are identified, they can be applied directly within the formula. 
        Bayes' Theorem tends to be a good option when there are so many scenarios that drawing a tree diagram 
        would be complex.
      </p>

      <exercise xml:id="ex-parking-garage-tree">
        <statement>
          <p>
            Jose visits campus every Thursday evening. However, some days the parking garage is full, often 
            due to college events. There are academic events on 35% of evenings, sporting events on 20% of 
            evenings, and no events on 45% of evenings. When there is an academic event, the garage fills up 
            about 25% of the time, and it fills up 70% of evenings with sporting events. On evenings when 
            there are no events, it only fills up about 5% of the time. If Jose comes to campus and finds 
            the garage full, what is the probability that there is a sporting event? Use a tree diagram to 
            solve this problem.
          </p>
        </statement>
        <solution>
          <p>
            The tree diagram with three primary branches is shown below. Next, we identify two probabilities 
            from the tree diagram: (1) The probability that there is a sporting event and the garage is full: 
            0.14. (2) The probability the garage is full: <m>0.0875 + 0.14 + 0.0225 = 0.25</m>. Then the 
            solution is the ratio of these probabilities: <m>\frac{0.14}{0.25} = 0.56</m>. If the garage is 
            full, there is a 56% probability that there is a sporting event.
          </p>
          <figure>
            <image source="ch_probability/figures/treeDiagramGarage" width="65%"/>
          </figure>
        </solution>
      </exercise>

      <example xml:id="ex-bayes-parking">
        <statement>
          <p>
            Here we solve the same problem presented in the previous exercise, except this time we use 
            Bayes' Theorem.
          </p>
        </statement>
        <solution>
          <p>
            The outcome of interest is whether there is a sporting event (call this <m>A_1</m>), and the 
            condition is that the lot is full (<m>B</m>). Let <m>A_2</m> represent an academic event and 
            <m>A_3</m> represent there being no event on campus. Then the given probabilities can be written as:
            <md>
              <mrow>P(A_1) = 0.2 \quad P(A_2) = 0.35 \quad P(A_3) = 0.45</mrow>
              <mrow>P(B \mid A_1) = 0.7 \quad P(B \mid A_2) = 0.25 \quad P(B \mid A_3) = 0.05</mrow>
            </md>
          </p>
          <p>
            Bayes' Theorem can be used to compute the probability of a sporting event (<m>A_1</m>) under the 
            condition that the parking lot is full (<m>B</m>):
            <md>
              <mrow>P(A_1 \mid B) \amp= \frac{P(B \mid A_1) P(A_1)}{P(B \mid A_1) P(A_1) + P(B \mid A_2) P(A_2) + P(B \mid A_3) P(A_3)}</mrow>
              <mrow>\amp= \frac{(0.7)(0.2)}{(0.7)(0.2) + (0.25)(0.35) + (0.05)(0.45)}</mrow>
              <mrow>\amp= 0.56</mrow>
            </md>
            Based on the information that the garage is full, there is a 56% probability that a sporting 
            event is being held on campus that evening.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-parking-academic-event">
        <statement>
          <p>
            Use the information in <xref ref="ex-parking-garage-tree"/> and <xref ref="ex-bayes-parking"/> 
            to verify the probability that there is an academic event conditioned on the parking lot being 
            full is 0.35.
          </p>
        </statement>
        <solution>
          <p>
            Short answer:
            <md>
              <mrow>P(A_2 \mid B) \amp= \frac{P(B \mid A_2) P(A_2)}{P(B \mid A_1) P(A_1) + P(B \mid A_2) P(A_2) + P(B \mid A_3) P(A_3)}</mrow>
              <mrow>\amp= \frac{(0.25)(0.35)}{(0.7)(0.2) + (0.25)(0.35) + (0.05)(0.45)}</mrow>
              <mrow>\amp= 0.35</mrow>
            </md>
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-parking-no-event">
        <statement>
          <p>
            In <xref ref="ex-parking-garage-tree"/> and <xref ref="ex-bayes-parking"/>, you found that if 
            the parking lot is full, the probability there is a sporting event is 0.56 and the probability 
            there is an academic event is 0.35. Using this information, compute <m>P(\text{no event} \mid 
            \text{the lot is full})</m>.
          </p>
        </statement>
        <solution>
          <p>
            Each probability is conditioned on the same information that the garage is full, so the 
            complement may be used: <m>1.00 - 0.56 - 0.35 = 0.09</m>.
          </p>
        </solution>
      </exercise>

      <p>
        The last several exercises offered a way to update our belief about whether there is a sporting event, 
        academic event, or no event going on at the school based on the information that the parking lot was 
        full. This strategy of <em>updating beliefs</em> using Bayes' Theorem is actually the foundation of 
        an entire section of statistics called <term>Bayesian statistics</term>. While Bayesian statistics is 
        very important and useful, we will not have time to cover much more of it in this book.
      </p>
    </subsection>
  </section>

  <!-- Section 3.3: Sampling from a small population -->
  <section xml:id="sec-sampling-small-population">
    <title>Sampling from a small population</title>

    <introduction>
      <p>
        When we sample observations from a population, usually we're only sampling a small fraction of 
        the possible individuals or cases. However, sometimes our sample size is large enough or the 
        population is small enough that we sample more than 10% of a population without replacement 
        (meaning we do not have a chance of sampling the same cases twice). Sampling such a notable 
        fraction of a population can be important for how we analyze the sample.
      </p>
    </introduction>

    <example xml:id="ex-professor-select-student">
      <title>Selecting a student at random</title>
      <statement>
        <p>
          Professors sometimes select a student at random to answer a question. If each student has 
          an equal chance of being selected and there are 15 people in your class, what is the chance 
          that she will pick you for the next question?
        </p>
      </statement>
      <solution>
        <p>
          If there are 15 people to ask and none are skipping class, then the probability is <m>1/15</m>, 
          or about 0.067.
        </p>
      </solution>
    </example>

    <example xml:id="ex-3-questions-wo-replacement">
      <title>Three questions without replacement</title>
      <statement>
        <p>
          If the professor asks 3 questions, what is the probability that you will not be selected? 
          Assume that she will not pick the same person twice in a given lecture.
        </p>
      </statement>
      <solution>
        <p>
          For the first question, she will pick someone else with probability <m>14/15</m>. When she 
          asks the second question, she only has 14 people who have not yet been asked. Thus, if you 
          were not picked on the first question, the probability you are again not picked is <m>13/14</m>. 
          Similarly, the probability you are again not picked on the third question is <m>12/13</m>, 
          and the probability of not being picked for any of the three questions is
          <md>
            <mrow>P(\text{not picked in 3 questions}) \amp= P(\text{Q1} = \text{not picked, Q2} = \text{not picked, Q3} = \text{not picked})</mrow>
            <mrow>\amp= \frac{14}{15} \times \frac{13}{14} \times \frac{12}{13} = \frac{12}{15} = 0.80</mrow>
          </md>
        </p>
      </solution>
    </example>

    <exercise xml:id="ex-rule-multiply-probs">
      <statement>
        <p>
          What rule permitted us to multiply the probabilities in <xref ref="ex-3-questions-wo-replacement"/>?
        </p>
      </statement>
      <solution>
        <p>
          The three probabilities we computed were actually one marginal probability, <m>P(\text{Q1} = \text{not picked})</m>, 
          and two conditional probabilities:
          <md>
            <mrow>P(\text{Q2} = \text{not picked} \mid \text{Q1} = \text{not picked})</mrow>
            <mrow>P(\text{Q3} = \text{not picked} \mid \text{Q1} = \text{not picked, Q2} = \text{not picked})</mrow>
          </md>
          Using the General Multiplication Rule, the product of these three probabilities is the 
          probability of not being picked in 3 questions.
        </p>
      </solution>
    </exercise>

    <example xml:id="ex-3-questions-w-replacement">
      <title>Three questions with replacement</title>
      <statement>
        <p>
          Suppose the professor randomly picks without regard to who she already selected, i.e. students 
          can be picked more than once. What is the probability that you will not be picked for any of 
          the three questions?
        </p>
      </statement>
      <solution>
        <p>
          Each pick is independent, and the probability of not being picked for any individual question 
          is <m>14/15</m>. Thus, we can use the Multiplication Rule for independent processes.
          <md>
            <mrow>P(\text{not picked in 3 questions}) \amp= P(\text{Q1} = \text{not picked, Q2} = \text{not picked, Q3} = \text{not picked})</mrow>
            <mrow>\amp= \frac{14}{15} \times \frac{14}{15} \times \frac{14}{15} = 0.813</mrow>
          </md>
          You have a slightly higher chance of not being picked compared to when she picked a new person 
          for each question. However, you now may be picked more than once.
        </p>
      </solution>
    </example>

    <exercise xml:id="ex-picked-all-three">
      <statement>
        <p>
          Under the setup of <xref ref="ex-3-questions-w-replacement"/>, what is the probability of being 
          picked to answer all three questions?
        </p>
      </statement>
      <solution>
        <p>
          <m>P(\text{being picked to answer all three questions}) = \left(\frac{1}{15}\right)^3 = 0.00030</m>
        </p>
      </solution>
    </exercise>

    <p>
      If we sample from a small population without replacement, we no longer have independence between 
      our observations. In <xref ref="ex-3-questions-wo-replacement"/>, the probability of not being 
      picked for the second question was conditioned on the event that you were not picked for the 
      first question. In <xref ref="ex-3-questions-w-replacement"/>, the professor sampled her students 
      with replacement: she repeatedly sampled the entire class without regard to who she already picked.
    </p>

    <exercise xml:id="ex-raffle-30-tickets">
      <statement>
        <p>
          Your department is holding a raffle. They sell 30 tickets and offer seven prizes. 
          <ol marker="(a)">
            <li><p>They place the tickets in a hat and draw one for each prize. The tickets are sampled 
              without replacement, i.e. the selected tickets are not placed back in the hat. What is 
              the probability of winning a prize if you buy one ticket?</p></li>
            <li><p>What if the tickets are sampled with replacement?</p></li>
          </ol>
        </p>
      </statement>
      <solution>
        <p>
          (a) First determine the probability of not winning. The tickets are sampled without replacement, 
          which means the probability you do not win on the first draw is <m>29/30</m>, <m>28/29</m> for the 
          second, <m>\ldots</m>, and <m>23/24</m> for the seventh. The probability you win no prize is the 
          product of these separate probabilities: <m>23/30</m>. That is, the probability of winning a 
          prize is <m>1 - 23/30 = 7/30 = 0.233</m>.
        </p>
        <p>
          (b) When the tickets are sampled with replacement, there are seven independent draws. Again we 
          first find the probability of not winning a prize: <m>(29/30)^7 = 0.789</m>. Thus, the 
          probability of winning (at least) one prize when drawing with replacement is 0.211.
        </p>
      </solution>
    </exercise>

    <exercise xml:id="ex-raffle-comparison">
      <statement>
        <p>
          Compare your answers in <xref ref="ex-raffle-30-tickets"/>. How much influence does the 
          sampling method have on your chances of winning a prize?
        </p>
      </statement>
      <solution>
        <p>
          There is about a 10% larger chance of winning a prize when using sampling without replacement. 
          However, at most one prize may be won under this sampling procedure.
        </p>
      </solution>
    </exercise>

    <p>
      Had we repeated <xref ref="ex-raffle-30-tickets"/> with 300 tickets instead of 30, we would have 
      found something interesting: the results would be nearly identical. The probability would be 0.0233 
      without replacement and 0.0231 with replacement. When the sample size is only a small fraction of 
      the population (under 10%), observations are nearly independent even when sampling without replacement.
    </p>
  </section>

  <!-- Section 3.4: Random variables -->
  <section xml:id="sec-random-variables">
    <title>Random variables</title>

    <introduction>
      <p>
        It's often useful to model a process using what's called a <term>random variable</term>. 
        Such a model allows us to apply a mathematical framework and statistical principles for 
        better understanding and predicting outcomes in the real world.
      </p>
    </introduction>

    <example xml:id="ex-bookstore-sales">
      <title>Bookstore sales expectation</title>
      <statement>
        <p>
          Two books are assigned for a statistics class: a textbook and its corresponding study guide. 
          The university bookstore determined 20% of enrolled students do not buy either book, 55% buy 
          the textbook only, and 25% buy both books. If there are 100 students enrolled, how many 
          books should the bookstore expect to sell to this class?
        </p>
      </statement>
      <solution>
        <p>
          Around 20 students will not buy either book (0 books total), about 55 will buy one book 
          (55 books total), and approximately 25 will buy two books (totaling 50 books for these 25 students). 
          The bookstore should expect to sell about 105 books for this class.
        </p>
      </solution>
    </example>

    <exercise xml:id="ex-bookstore-variability">
      <statement>
        <p>
          Would you be surprised if the bookstore sold slightly more or less than 105 books?
        </p>
      </statement>
      <solution>
        <p>
          If they sell a little more or a little less, this should not be a surprise. There is natural 
          variability in observed data. For example, if we flip a coin 100 times, it will not usually 
          come up heads exactly half the time, but it will probably be close.
        </p>
      </solution>
    </exercise>

    <subsection xml:id="subsec-expectation">
      <title>Expectation</title>
      
      <p>
        We call a variable or process with a numerical outcome a <term>random variable</term>, and we 
        usually represent this random variable with a capital letter such as <m>X</m>, <m>Y</m>, or <m>Z</m>. 
        The amount of money a single student will spend on her statistics books is a random variable, 
        and we represent it by <m>X</m>.
      </p>

      <assemblage xml:id="def-random-variable">
        <title>Random variable</title>
        <p>
          A random process or variable with a numerical outcome.
        </p>
      </assemblage>

      <p>
        The possible outcomes of <m>X</m> are labeled with a corresponding lower case letter <m>x</m> and 
        subscripts. For example, we write <m>x_1 = \$0</m>, <m>x_2 = \$137</m>, and <m>x_3 = \$170</m>, 
        which occur with probabilities 0.20, 0.55, and 0.25. The distribution of <m>X</m> is summarized 
        in the table below.
      </p>

      <figure xml:id="fig-stat-spend-dist">
        <caption>The probability distribution for the random variable <m>X</m>, representing the bookstore's 
          revenue from a single student.</caption>
        <tabular halign="center">
          <row bottom="minor">
            <cell><m>i</m></cell>
            <cell>1</cell>
            <cell>2</cell>
            <cell>3</cell>
            <cell>Total</cell>
          </row>
          <row>
            <cell><m>x_i</m></cell>
            <cell>$0</cell>
            <cell>$137</cell>
            <cell>$170</cell>
            <cell>--</cell>
          </row>
          <row bottom="minor">
            <cell><m>P(X=x_i)</m></cell>
            <cell>0.20</cell>
            <cell>0.55</cell>
            <cell>0.25</cell>
            <cell>1.00</cell>
          </row>
        </tabular>
      </figure>

      <figure xml:id="fig-book-cost-dist">
        <caption>Probability distribution for the bookstore's revenue from one student. 
          The triangle represents the average revenue per student.</caption>
        <image source="ch_probability/figures/bookCostDist" width="60%"/>
      </figure>

      <example xml:id="ex-rev-per-student">
        <statement>
          <p>
            What is the average revenue per student for this course?
          </p>
        </statement>
        <solution>
          <p>
            The expected total revenue is $11,785, and there are 100 students. Therefore the expected 
            revenue per student is <m>\$11,785/100 = \$117.85</m>.
          </p>
        </solution>
      </example>

      <p>
        We computed the average outcome of <m>X</m> as <m>\$117.85</m> in <xref ref="ex-rev-per-student"/>. 
        We call this average the <term>expected value</term> of <m>X</m>, denoted by <m>E(X)</m>. 
        The expected value of a random variable is computed by adding each outcome weighted by its probability:
        <md>
          <mrow>E(X) \amp= 0 \times P(X=0) + 137 \times P(X=137) + 170 \times P(X=170)</mrow>
          <mrow>\amp= 0 \times 0.20 + 137 \times 0.55 + 170 \times 0.25 = 117.85</mrow>
        </md>
      </p>

      <assemblage xml:id="def-expected-value">
        <title>Expected value of a discrete random variable</title>
        <p>
          If <m>X</m> takes outcomes <m>x_1, \ldots, x_k</m> with probabilities <m>P(X=x_1), \ldots, P(X=x_k)</m>, 
          the expected value of <m>X</m> is the sum of each outcome multiplied by its corresponding probability:
          <md>
            <mrow>E(X) \amp= x_1 \times P(X = x_1) + \cdots + x_k \times P(X = x_k)</mrow>
            <mrow>\amp= \sum_{i=1}^{k} x_i P(X = x_i)</mrow>
          </md>
          The Greek letter <m>\mu</m> may be used in place of the notation <m>E(X)</m>.
        </p>
      </assemblage>

      <p>
        The expected value for a random variable represents the average outcome. For example, <m>E(X) = 117.85</m> 
        represents the average amount the bookstore expects to make from a single student, which we could also 
        write as <m>\mu = 117.85</m>.
      </p>

      <p>
        It is also possible to compute the expected value of a continuous random variable (see <xref ref="sec-continuous-distributions"/>). 
        However, it requires calculus and we save it for a later class.
      </p>

      <p>
        In physics, the expectation holds the same meaning as the center of gravity. The distribution can be 
        represented by a series of weights at each outcome, and the mean represents the balancing point.
      </p>
    </subsection>

    <subsection xml:id="subsec-variability-random-variables">
      <title>Variability in random variables</title>
      
      <p>
        Suppose you ran the university bookstore. Besides how much revenue you expect to generate, you might 
        also want to know the volatility (variability) in your revenue.
      </p>

      <p>
        The <term>variance</term> and <term>standard deviation</term> can be used to describe the variability 
        of a random variable. We first compute deviations from the mean (<m>x_i - \mu</m>), square those deviations, 
        and take an average to get the variance. In the case of a random variable, we again compute squared 
        deviations. However, we take their sum weighted by their corresponding probabilities, just like we did 
        for the expectation. This weighted sum of squared deviations equals the variance, and we calculate the 
        standard deviation by taking the square root of the variance.
      </p>

      <assemblage xml:id="def-variance-random-variable">
        <title>Variance of a discrete random variable</title>
        <p>
          If <m>X</m> takes outcomes <m>x_1, \ldots, x_k</m> with probabilities <m>P(X=x_1), \ldots, P(X=x_k)</m> 
          and expected value <m>\mu = E(X)</m>, then the variance of <m>X</m>, denoted by <m>\text{Var}(X)</m> 
          or the symbol <m>\sigma^2</m>, is
          <md>
            <mrow>\sigma^2 \amp= (x_1-\mu)^2 \times P(X=x_1) + \cdots + (x_k-\mu)^2 \times P(X=x_k)</mrow>
            <mrow>\amp= \sum_{j=1}^{k} (x_j - \mu)^2 P(X=x_j)</mrow>
          </md>
          The standard deviation of <m>X</m>, labeled <m>\sigma</m>, is the square root of the variance.
        </p>
      </assemblage>

      <example xml:id="ex-revenue-variance">
        <statement>
          <p>
            Compute the expected value, variance, and standard deviation of <m>X</m>, the revenue of a 
            single statistics student for the bookstore.
          </p>
        </statement>
        <solution>
          <p>
            It is useful to construct a table that holds computations for each outcome separately, then 
            add up the results.
          </p>
          <figure>
            <tabular halign="center">
              <row bottom="minor">
                <cell><m>i</m></cell>
                <cell>1</cell>
                <cell>2</cell>
                <cell>3</cell>
                <cell>Total</cell>
              </row>
              <row>
                <cell><m>x_i</m></cell>
                <cell>$0</cell>
                <cell>$137</cell>
                <cell>$170</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>P(X=x_i)</m></cell>
                <cell>0.20</cell>
                <cell>0.55</cell>
                <cell>0.25</cell>
                <cell></cell>
              </row>
              <row bottom="minor">
                <cell><m>x_i \times P(X=x_i)</m></cell>
                <cell>0</cell>
                <cell>75.35</cell>
                <cell>42.50</cell>
                <cell>117.85</cell>
              </row>
            </tabular>
          </figure>
          <p>
            Thus, the expected value is <m>\mu = 117.85</m>, which we computed earlier. The variance can 
            be constructed by extending this table:
          </p>
          <figure>
            <tabular halign="center">
              <row bottom="minor">
                <cell><m>i</m></cell>
                <cell>1</cell>
                <cell>2</cell>
                <cell>3</cell>
                <cell>Total</cell>
              </row>
              <row>
                <cell><m>x_i</m></cell>
                <cell>$0</cell>
                <cell>$137</cell>
                <cell>$170</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>P(X=x_i)</m></cell>
                <cell>0.20</cell>
                <cell>0.55</cell>
                <cell>0.25</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>x_i \times P(X=x_i)</m></cell>
                <cell>0</cell>
                <cell>75.35</cell>
                <cell>42.50</cell>
                <cell>117.85</cell>
              </row>
              <row>
                <cell><m>x_i - \mu</m></cell>
                <cell>-117.85</cell>
                <cell>19.15</cell>
                <cell>52.15</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>(x_i-\mu)^2</m></cell>
                <cell>13888.62</cell>
                <cell>366.72</cell>
                <cell>2719.62</cell>
                <cell></cell>
              </row>
              <row bottom="minor">
                <cell><m>(x_i-\mu)^2 \times P(X=x_i)</m></cell>
                <cell>2777.7</cell>
                <cell>201.7</cell>
                <cell>679.9</cell>
                <cell>3659.3</cell>
              </row>
            </tabular>
          </figure>
          <p>
            The variance of <m>X</m> is <m>\sigma^2 = 3659.3</m>, which means the standard deviation is 
            <m>\sigma = \sqrt{3659.3} = \$60.49</m>.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-chemistry-books-revenue">
        <statement>
          <p>
            The bookstore also offers a chemistry textbook for $159 and a book supplement for $41. From past 
            experience, they know about 25% of chemistry students just buy the textbook while 60% buy both the 
            textbook and supplement.
          </p>
          <p>
            <ol marker="(a)">
              <li><p>What proportion of students don't buy either book? Assume no students buy the supplement 
                without the textbook.</p></li>
              <li><p>Let <m>Y</m> represent the revenue from a single student. Write out the probability 
                distribution of <m>Y</m>, i.e. a table for each outcome and its associated probability.</p></li>
              <li><p>Compute the expected revenue from a single chemistry student.</p></li>
              <li><p>Find the standard deviation to describe the variability associated with the revenue from 
                a single student.</p></li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            (a) <m>100\% - 25\% - 60\% = 15\%</m> of students do not buy any books for the class.
          </p>
          <p>
            (b) The probability distribution is represented in the table below.
          </p>
          <p>
            (c) The expectation is given as the total on the line <m>y_i \times P(Y=y_i)</m>: <m>E(Y) = 159.75</m>.
          </p>
          <p>
            (d) The result is the square root of the variance listed in the total on the last line: 
            <m>\sigma = \sqrt{\text{Var}(Y)} = \$69.28</m>.
          </p>
          <figure>
            <tabular halign="center">
              <row bottom="minor">
                <cell><m>i</m></cell>
                <cell>1</cell>
                <cell>2</cell>
                <cell>3</cell>
                <cell>Total</cell>
              </row>
              <row>
                <cell>Scenario</cell>
                <cell>no book</cell>
                <cell>textbook</cell>
                <cell>both</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>y_i</m></cell>
                <cell>0.00</cell>
                <cell>159.00</cell>
                <cell>200.00</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>P(Y=y_i)</m></cell>
                <cell>0.15</cell>
                <cell>0.25</cell>
                <cell>0.60</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>y_i \times P(Y=y_i)</m></cell>
                <cell>0.00</cell>
                <cell>39.75</cell>
                <cell>120.00</cell>
                <cell>159.75</cell>
              </row>
              <row>
                <cell><m>y_i - E(Y)</m></cell>
                <cell>-159.75</cell>
                <cell>-0.75</cell>
                <cell>40.25</cell>
                <cell></cell>
              </row>
              <row>
                <cell><m>(y_i-E(Y))^2</m></cell>
                <cell>25520.06</cell>
                <cell>0.56</cell>
                <cell>1620.06</cell>
                <cell></cell>
              </row>
              <row bottom="minor">
                <cell><m>(y_i-E(Y))^2 \times P(Y)</m></cell>
                <cell>3828.0</cell>
                <cell>0.1</cell>
                <cell>972.0</cell>
                <cell><m>\approx 4800</m></cell>
              </row>
            </tabular>
          </figure>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-linear-combinations">
      <title>Linear combinations of random variables</title>
      
      <p>
        So far, we have thought of each variable as being a complete story in and of itself. Sometimes it 
        is more appropriate to use a combination of variables. For instance, the amount of time a person spends 
        commuting to work each week can be broken down into several daily commutes. Similarly, the total gain 
        or loss in a stock portfolio is the sum of the gains and losses in its components.
      </p>

      <example xml:id="ex-john-weekly-commute">
        <statement>
          <p>
            John travels to work five days a week. We will use <m>X_1</m> to represent his travel time on 
            Monday, <m>X_2</m> to represent his travel time on Tuesday, and so on. Write an equation using 
            <m>X_1, \ldots, X_5</m> that represents his travel time for the week, denoted by <m>W</m>.
          </p>
        </statement>
        <solution>
          <p>
            His total weekly travel time is the sum of the five daily values:
            <me>W = X_1 + X_2 + X_3 + X_4 + X_5</me>
            Breaking the weekly travel time <m>W</m> into pieces provides a framework for understanding each 
            source of randomness and is useful for modeling <m>W</m>.
          </p>
        </solution>
      </example>

      <example xml:id="ex-john-weekly-expectation">
        <statement>
          <p>
            It takes John an average of 18 minutes each day to commute to work. What would you expect his 
            average commute time to be for the week?
          </p>
        </statement>
        <solution>
          <p>
            We were told that the average (i.e. expected value) of the commute time is 18 minutes per day: 
            <m>E(X_i) = 18</m>. To get the expected time for the sum of the five days, we can add up the 
            expected time for each individual day:
            <md>
              <mrow>E(W) \amp= E(X_1 + X_2 + X_3 + X_4 + X_5)</mrow>
              <mrow>\amp= E(X_1) + E(X_2) + E(X_3) + E(X_4) + E(X_5)</mrow>
              <mrow>\amp= 18 + 18 + 18 + 18 + 18 = 90 \text{ minutes}</mrow>
            </md>
            The expectation of the total time is equal to the sum of the expected individual times. More 
            generally, the expectation of a sum of random variables is always the sum of the expectation for 
            each random variable.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-elena-auction">
        <statement>
          <p>
            Elena is selling a TV at a cash auction and also intends to buy a toaster oven in the auction. 
            If <m>X</m> represents the profit for selling the TV and <m>Y</m> represents the cost of the 
            toaster oven, write an equation that represents the net change in Elena's cash.
          </p>
        </statement>
        <solution>
          <p>
            She will make <m>X</m> dollars on the TV but spend <m>Y</m> dollars on the toaster oven: 
            <m>X - Y</m>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-elena-expectation">
        <statement>
          <p>
            Based on past auctions, Elena figures she should expect to make about $175 on the TV and pay 
            about $23 for the toaster oven. In total, how much should she expect to make or spend?
          </p>
        </statement>
        <solution>
          <p>
            <m>E(X - Y) = E(X) - E(Y) = 175 - 23 = \$152</m>. She should expect to make about $152.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-uncertainty-sum">
        <statement>
          <p>
            Would you be surprised if John's weekly commute wasn't exactly 90 minutes or if Elena didn't 
            make exactly $152? Explain.
          </p>
        </statement>
        <solution>
          <p>
            No, since there is probably some variability. For example, the traffic will vary from one day 
            to next, and auction prices will vary depending on the quality of the merchandise and the 
            interest of the attendees.
          </p>
        </solution>
      </exercise>

      <p>
        Two important concepts concerning combinations of random variables have so far been introduced. First, 
        a final value can sometimes be described as the sum of its parts in an equation. Second, intuition 
        suggests that putting the individual average values into this equation gives the average value we would 
        expect in total. This second point needs clarification -- it is guaranteed to be true in what are called 
        <em>linear combinations of random variables</em>.
      </p>

      <p>
        A <term>linear combination</term> of two random variables <m>X</m> and <m>Y</m> is a combination
        <me>aX + bY</me>
        where <m>a</m> and <m>b</m> are some fixed and known numbers. For John's commute time, there were five 
        random variables -- one for each work day -- and each random variable could be written as having a 
        fixed coefficient of 1:
        <me>1X_1 + 1X_2 + 1X_3 + 1X_4 + 1X_5</me>
        For Elena's net gain or loss, the <m>X</m> random variable had a coefficient of +1 and the <m>Y</m> 
        random variable had a coefficient of -1.
      </p>

      <p>
        When considering the average of a linear combination of random variables, it is safe to plug in the 
        mean of each random variable and then compute the final result. For some examples of nonlinear combinations 
        of random variables -- cases where we cannot simply plug in the means -- consider: <m>X^{1+Y}</m>, <m>X \times Y</m>, 
        <m>X/Y</m>. In such cases, plugging in the average value for each random variable and computing the 
        result will not generally lead to an accurate average value for the end result.
      </p>

      <assemblage xml:id="linear-combination-mean">
        <title>Linear combinations of random variables and the average result</title>
        <p>
          If <m>X</m> and <m>Y</m> are random variables, then a linear combination of the random variables 
          is given by
          <me>aX + bY</me>
          where <m>a</m> and <m>b</m> are some fixed numbers. To compute the average value of a linear 
          combination of random variables, plug in the average of each individual random variable and 
          compute the result:
          <me>a \times E(X) + b \times E(Y)</me>
          Recall that the expected value is the same as the mean, e.g. <m>E(X) = \mu_X</m>.
        </p>
      </assemblage>

      <example xml:id="ex-leonard-portfolio">
        <statement>
          <p>
            Leonard has invested $6000 in Caterpillar Inc (stock ticker: CAT) and $2000 in Exxon Mobil Corp 
            (XOM). If <m>X</m> represents the change in Caterpillar's stock next month and <m>Y</m> represents 
            the change in Exxon Mobil's stock next month, write an equation that describes how much money will 
            be made or lost in Leonard's stocks for the month.
          </p>
        </statement>
        <solution>
          <p>
            For simplicity, we will suppose <m>X</m> and <m>Y</m> are not in percents but are in decimal form 
            (e.g. if Caterpillar's stock increases 1%, then <m>X = 0.01</m>; or if it loses 1%, then 
            <m>X = -0.01</m>). Then we can write an equation for Leonard's gain as
            <me>\$6000 \times X + \$2000 \times Y</me>
            If we plug in the change in the stock value for <m>X</m> and <m>Y</m>, this equation gives the 
            change in value of Leonard's stock portfolio for the month. A positive value represents a gain, 
            and a negative value represents a loss.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-leonard-expected-return">
        <statement>
          <p>
            Caterpillar stock has recently been rising at 2.0% and Exxon Mobil's at 0.2% per month, 
            respectively. Compute the expected change in Leonard's stock portfolio for next month.
          </p>
        </statement>
        <solution>
          <p>
            <m>E(\$6000 \times X + \$2000 \times Y) = \$6000 \times 0.020 + \$2000 \times 0.002 = \$124</m>
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-leonard-risk">
        <statement>
          <p>
            You should have found that Leonard expects a positive gain in <xref ref="ex-leonard-expected-return"/>. 
            However, would you be surprised if he actually had a loss this month?
          </p>
        </statement>
        <solution>
          <p>
            No. While stocks tend to rise over time, they are often volatile in the short term.
          </p>
        </solution>
      </exercise>

      <p>
        Quantifying the average outcome from a linear combination of random variables is helpful, but it is 
        also important to have some sense of the uncertainty associated with the total outcome of that 
        combination of random variables. The expected net gain or loss of Leonard's stock portfolio was 
        considered in <xref ref="ex-leonard-expected-return"/>. However, there was no quantitative discussion 
        of the volatility of this portfolio.
      </p>

      <figure xml:id="fig-leonard-portfolio-monthly">
        <caption>The change in a portfolio like Leonard's for 36 months, where $6000 is in Caterpillar's 
          stock and $2000 is in Exxon Mobil's.</caption>
        <image source="ch_probability/figures/changeInLeonardsStockPortfolioFor36Months" width="60%"/>
      </figure>

      <p>
        For instance, while the average monthly gain might be about $124 according to the data, that gain 
        is not guaranteed. The figure shows the monthly changes in a portfolio like Leonard's during a three 
        year period. The gains and losses vary widely, and quantifying these fluctuations is important when 
        investing in stocks.
      </p>

      <p>
        Just as we have done in many previous cases, we use the variance and standard deviation to describe 
        the uncertainty associated with Leonard's monthly returns. The variance of a linear combination of 
        random variables can be computed by plugging in the variances of the individual random variables 
        and squaring the coefficients of the random variables. This equation is valid as long as the random 
        variables are independent.
      </p>

      <assemblage xml:id="variance-linear-combination">
        <title>Variability of linear combinations of random variables</title>
        <p>
          The variance of a linear combination of random variables may be computed by squaring the constants, 
          substituting in the variances for the random variables, and computing the result:
          <me>\text{Var}(aX + bY) = a^2 \times \text{Var}(X) + b^2 \times \text{Var}(Y)</me>
          This equation is valid as long as the random variables are independent of each other. The standard 
          deviation of the linear combination may be found by taking the square root of the variance.
        </p>
      </assemblage>

      <example xml:id="ex-john-weekly-sd">
        <statement>
          <p>
            Suppose John's daily commute has a standard deviation of 4 minutes. What is the uncertainty in 
            his total commute time for the week?
          </p>
        </statement>
        <solution>
          <p>
            The expression for John's commute time was
            <me>X_1 + X_2 + X_3 + X_4 + X_5</me>
            Each coefficient is 1, and the variance of each day's time is <m>4^2 = 16</m>. Thus, the 
            variance of the total weekly commute time is
            <md>
              <mrow>\text{variance} \amp= 1^2 \times 16 + 1^2 \times 16 + 1^2 \times 16 + 1^2 \times 16 + 1^2 \times 16 = 5 \times 16 = 80</mrow>
              <mrow>\text{standard deviation} \amp= \sqrt{\text{variance}} = \sqrt{80} = 8.94</mrow>
            </md>
            The standard deviation for John's weekly work commute time is about 9 minutes.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-john-independence">
        <statement>
          <p>
            The computation in <xref ref="ex-john-weekly-sd"/> relied on an important assumption: the 
            commute time for each day is independent of the time on other days of that week. Do you think 
            this is valid? Explain.
          </p>
        </statement>
        <solution>
          <p>
            One concern is whether traffic patterns tend to have a weekly cycle (e.g. Fridays may be worse 
            than other days). If that is the case, and John drives, then the assumption is probably not 
            reasonable. However, if John walks to work, then his commute is probably not affected by any 
            weekly traffic cycle.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-elena-variability">
        <statement>
          <p>
            Consider Elena's two auctions from <xref ref="ex-elena-auction"/>. Suppose these auctions are 
            approximately independent and the variability in auction prices associated with the TV and 
            toaster oven can be described using standard deviations of $25 and $8. Compute the standard 
            deviation of Elena's net gain.
          </p>
        </statement>
        <solution>
          <p>
            The equation for Elena can be written as
            <me>(1) \times X + (-1) \times Y</me>
            The variances of <m>X</m> and <m>Y</m> are 625 and 64. We square the coefficients and plug in 
            the variances:
            <me>(1)^2 \times \text{Var}(X) + (-1)^2 \times \text{Var}(Y) = 1 \times 625 + 1 \times 64 = 689</me>
            The variance of the linear combination is 689, and the standard deviation is the square root of 
            689: about $26.25.
          </p>
        </solution>
      </exercise>

      <p>
        Consider again <xref ref="ex-elena-variability"/>. The negative coefficient for <m>Y</m> in the 
        linear combination was eliminated when we squared the coefficients. This generally holds true: negatives 
        in a linear combination will have no impact on the variability computed for a linear combination, but 
        they do impact the expected value computations.
      </p>
    </subsection>
  </section>

  <!-- Section 3.5: Continuous distributions -->
  <section xml:id="sec-continuous-distributions">
    <title>Continuous distributions</title>

    <introduction>
      <p>
        So far in this chapter we've discussed cases where the outcome of a variable is discrete. 
        In this section, we consider a context where the outcome is a continuous numerical variable.
      </p>
    </introduction>

    <example xml:id="ex-us-heights">
      <title>US adult heights histograms</title>
      <statement>
        <p>
          <xref ref="fig-fdic-histograms"/> shows a few different hollow histograms for the heights of US adults. 
          How does changing the number of bins allow you to make different interpretations of the data?
        </p>
      </statement>
      <solution>
        <p>
          Adding more bins provides greater detail. This sample is extremely large, which is why much smaller 
          bins still work well. Usually we do not use so many bins with smaller sample sizes since small counts 
          per bin mean the bin heights are very volatile.
        </p>
      </solution>
    </example>

    <figure xml:id="fig-fdic-histograms">
      <caption>Four hollow histograms of US adults heights with varying bin widths</caption>
      <image source="ch_probability/figures/fdicHistograms" width="95%"/>
    </figure>

    <example xml:id="ex-height-180-185">
      <title>Probability from histogram</title>
      <statement>
        <p>
          What proportion of the sample is between 180 cm and 185 cm tall (about 5'11" to 6'1")?
        </p>
      </statement>
      <solution>
        <p>
          We can add up the heights of the bins in the range 180 cm and 185 cm and divide by the sample size. 
          For instance, this can be done with the two shaded bins shown in <xref ref="fig-us-heights-180-185"/>. 
          The two bins in this region have counts of 195,307 and 156,239 people, resulting in the following 
          estimate of the probability:
          <me>\frac{195307 + 156239}{3,000,000} = 0.1172</me>
          This fraction is the same as the proportion of the histogram's area that falls in the range 
          180 to 185 cm.
        </p>
      </solution>
    </example>

    <figure xml:id="fig-us-heights-180-185">
      <caption>A histogram with bin sizes of 2.5 cm. The shaded region represents individuals with 
        heights between 180 and 185 cm.</caption>
      <image source="ch_probability/figures/usHeightsHist180185" width="90%"/>
    </figure>

    <subsection xml:id="subsec-histograms-to-continuous">
      <title>From histograms to continuous distributions</title>
      
      <p>
        Examine the transition from a boxy hollow histogram in the top-left of <xref ref="fig-fdic-histograms"/> 
        to the much smoother plot in the lower-right. In this last plot, the bins are so slim that the hollow 
        histogram is starting to resemble a smooth curve. This suggests the population height as a continuous 
        numerical variable might best be explained by a curve that represents the outline of extremely slim bins.
      </p>

      <p>
        This smooth curve represents a <term>probability density function</term> (also called a <term>density</term> 
        or <term>distribution</term>), and such a curve is shown in <xref ref="fig-fdic-height-cont-dist"/> 
        overlaid on a histogram of the sample. A density has a special property: the total area under the 
        density's curve is 1.
      </p>

      <figure xml:id="fig-fdic-height-cont-dist">
        <caption>The continuous probability distribution of heights for US adults.</caption>
        <image source="ch_probability/figures/fdicHeightContDist" width="90%"/>
      </figure>
    </subsection>

    <subsection xml:id="subsec-probabilities-continuous">
      <title>Probabilities from continuous distributions</title>
      
      <p>
        We computed the proportion of individuals with heights 180 to 185 cm in <xref ref="ex-height-180-185"/> 
        as a fraction:
        <me>\frac{\text{number of people between 180 and 185}}{\text{total sample size}}</me>
        We found the number of people with heights between 180 and 185 cm by determining the fraction of the 
        histogram's area in this region. Similarly, we can use the area in the shaded region under the curve 
        to find a probability (with the help of a computer):
        <me>P(\text{height between 180 and 185}) = \text{area between 180 and 185} = 0.1157</me>
        The probability that a randomly selected person is between 180 and 185 cm is 0.1157. This is very close 
        to the estimate from <xref ref="ex-height-180-185"/>: 0.1172.
      </p>

      <figure xml:id="fig-fdic-height-filled">
        <caption>Density for heights in the US adult population with the area between 180 and 185 cm shaded. 
          Compare this plot with <xref ref="fig-us-heights-180-185"/>.</caption>
        <image source="ch_probability/figures/fdicHeightContDistFilled" width="70%"/>
      </figure>

      <exercise xml:id="ex-height-three-adults">
        <statement>
          <p>
            Three US adults are randomly selected. The probability a single adult is between 180 and 185 cm 
            is 0.1157.
          </p>
          <p>
            <ol marker="(a)">
              <li><p>What is the probability that all three are between 180 and 185 cm tall?</p></li>
              <li><p>What is the probability that none are between 180 and 185 cm?</p></li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            Brief answers:
            <ol marker="(a)">
              <li><p><m>0.1157 \times 0.1157 \times 0.1157 = 0.0015</m></p></li>
              <li><p><m>(1 - 0.1157)^3 = 0.692</m></p></li>
            </ol>
          </p>
        </solution>
      </exercise>

      <example xml:id="ex-exactly-180-cm">
        <statement>
          <p>
            What is the probability that a randomly selected person is exactly 180 cm? Assume you can 
            measure perfectly.
          </p>
        </statement>
        <solution>
          <p>
            This probability is zero. A person might be close to 180 cm, but not exactly 180 cm tall. This 
            also makes sense with the definition of probability as area; there is no area captured between 
            180 cm and 180 cm.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-rounded-height">
        <statement>
          <p>
            Suppose a person's height is rounded to the nearest centimeter. Is there a chance that a random 
            person's measured height will be 180 cm?
          </p>
        </statement>
        <solution>
          <p>
            This has positive probability. Anyone between 179.5 cm and 180.5 cm will have a measured height of 
            180 cm. This is probably a more realistic scenario to encounter in practice versus 
            <xref ref="ex-exactly-180-cm"/>.
          </p>
        </solution>
      </exercise>
    </subsection>
  </section>

</chapter>
