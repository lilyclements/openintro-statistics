<?xml version="1.0" encoding="UTF-8" ?>
<chapter xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="ch-simple-linear-regression">
  <title>Introduction to Linear Regression</title>
  
  <introduction>
    <p>
      Linear regression is a very powerful statistical technique. Many people have some familiarity with regression just from reading the news, where straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables.
    </p>
  </introduction>

  <!-- Section 8.1: Fitting a line, residuals, and correlation -->
  <section xml:id="sec-fitting-line-residuals-correlation">
    <title>Fitting a line, residuals, and correlation</title>
    
    <introduction>
      <p>
        It's helpful to think deeply about the line fitting process. In this section, we define the form of a linear model, explore criteria for what makes a good fit, and introduce a new statistic called <term>correlation</term>.
      </p>
    </introduction>

    <subsection xml:id="subsec-fitting-line-to-data">
      <title>Fitting a line to data</title>
      
      <p>
        <xref ref="fig-perfLinearModel"/> shows two variables whose relationship can be modeled perfectly with a straight line. The equation for the line is
        <me>y = 5 + 64.96 x</me>
        Consider what a perfect linear relationship means: we know the exact value of <m>y</m> just by knowing the value of <m>x</m>. This is unrealistic in almost any natural process. For example, if we took family income (<m>x</m>), this value would provide some useful information about how much financial support a college may offer a prospective student (<m>y</m>). However, the prediction would be far from perfect, since other factors play a role in financial support beyond a family's finances.
      </p>

      <figure xml:id="fig-perfLinearModel">
        <caption>Requests from twelve separate buyers were simultaneously placed with a trading company to purchase Target Corporation stock (ticker TGT, December 28th, 2018), and the total cost of the shares were reported. Because the cost is computed using a linear formula, the linear fit is perfect.</caption>
        <image source="ch_regr_simple_linear/figures/perfLinearModel" width="60%">
          <description>A scatterplot with a straight line fit to the data are shown for the date December 28th, 2018. The horizontal axis is "Number of Target Corporation Stocks to Purchase" and the vertical axis is "Total Cost of the Shares Purchase". Twelve data points are shown that all fall exactly on a straight line with an equation of y equals 5 plus 64.96 times x. Because the cost is computed using a linear formula, this explains why the linear fit is perfect.</description>
        </image>
      </figure>

      <p>
        Linear regression is the statistical method for fitting a line to data where the relationship between two variables, <m>x</m> and <m>y</m>, can be modeled by a straight line with some error:
        <me>y = \beta_0 + \beta_1 x + \varepsilon</me>
        The values <m>\beta_0</m> and <m>\beta_1</m> represent the model's <term>parameters</term> (<m>\beta</m> is the Greek letter <em>beta</em>), and the error is represented by <m>\varepsilon</m> (the Greek letter <em>epsilon</em>). The parameters are estimated using data, and we write their point estimates as <m>b_0</m> and <m>b_1</m>. When we use <m>x</m> to predict <m>y</m>, we usually call <m>x</m> the <term>explanatory variable</term> or <term>predictor</term> variable, and we call <m>y</m> the <term>response</term>; we also often drop the <m>\varepsilon</m> term when writing down the model since our main focus is often on the prediction of the average outcome.
      </p>

      <p>
        It is rare for all of the data to fall perfectly on a straight line. Instead, it's more common for data to appear as a <term>cloud of points</term>, such as those examples shown in <xref ref="fig-imperfLinearModel"/>. In each case, the data fall around a straight line, even if none of the observations fall exactly on the line. The first plot shows a relatively strong downward linear trend, where the remaining variability in the data around the line is minor relative to the strength of the relationship between <m>x</m> and <m>y</m>. The second plot shows an upward trend that, while evident, is not as strong as the first. The last plot shows a very weak downward trend in the data, so slight we can hardly notice it. In each of these examples, we will have some uncertainty regarding our estimates of the model parameters, <m>\beta_0</m> and <m>\beta_1</m>. For instance, we might wonder, should we move the line up or down a little, or should we tilt it more or less? As we move forward in this chapter, we will learn about criteria for line-fitting, and we will also learn about the uncertainty associated with estimates of model parameters.
      </p>

      <figure xml:id="fig-imperfLinearModel">
        <caption>Three data sets where a linear model may be useful even though the data do not all fall exactly on the line.</caption>
        <image source="ch_regr_simple_linear/figures/imperfLinearModel" width="90%">
          <description>Three scatterplots are shown. The first has data ranging from -50 to positive 50 on both the horizontal and vertical axes. The data start in the upper left corner of the plot and then move steadily down to the right corner. The second plot has the horizontal axis running from 500 to about 2,000 and the vertical axis from about 0 to 25,000. At the left side of the plot, the data are in the lower half of the plot, and the points generally are steadily higher as we move right, where most points near the right end of the plot are in the upper region of the plot. A upwards trending line has been fit to these points. The last plot runs from about -10 to positive 50 on the horizontal axis and about -200 to positive 400 on the vertical axis. The points are scattered broadly across the range, with only the slightest downward trend evident in the data. A trend line has been fit to this data, though it is nearly flat.</description>
        </image>
      </figure>

      <p>
        There are also cases where fitting a straight line to the data, even if there is a clear relationship between the variables, is not helpful. One such case is shown in <xref ref="fig-notGoodAtAllForALinearModel"/> where there is a very clear relationship between the variables even though the trend is not linear. We discuss <term>nonlinear</term> trends in this chapter and the next, but details of fitting nonlinear models are saved for a later course.
      </p>

      <figure xml:id="fig-notGoodAtAllForALinearModel">
        <caption>A linear model is not useful in this nonlinear case. These data are from an introductory physics experiment.</caption>
        <image source="ch_regr_simple_linear/figures/notGoodAtAllForALinearModel" width="80%">
          <description>A linear model is not useful in a nonlinear set of data shown in this plot. The data are from an introductory physics experiment, where a ball is shot at many angles of inclination between 0 degrees and 90 degrees (represented by the horizontal axis), and the measured horizontal distance traveled by the ball before it hits the ground is shown in meters. The first point, at an angle of inclination of 0 hits the ground at 0 meters traveled. As the angle is increased, the ball travels further before it hits the ground until reaching a peak at 45 degrees angle of inclination, at which point it decreases again until we reach an angle of 90 degrees, at which point the ball again does not travel any horizontal distance before it hits the ground. For the data shown, the best fitting straight line is shown and is flat. This is a good example of why a straight line fit to data where there is curvature is often not useful.</description>
        </image>
      </figure>
    </subsection>

    <subsection xml:id="subsec-possum-head-lengths">
      <title>Using linear regression to predict possum head lengths</title>

      <p>
        Brushtail possums are a marsupial that lives in Australia, and a photo of one is shown in <xref ref="fig-brushtail-possum"/>. Researchers captured 104 of these animals and took body measurements before releasing the animals back into the wild. We consider two of these measurements: the total length of each possum, from head to tail, and the length of each possum's head.
      </p>

      <figure xml:id="fig-brushtail-possum">
        <caption>The common brushtail possum of Australia. Photo by Greg Schechter (https://flic.kr/p/9BAFbR). CC BY 2.0 license.</caption>
        <image source="ch_regr_simple_linear/figures/brushtail_possum" width="50%">
          <description>A common brushtail possum of Australia is shown. It has a brown fur coat with some gray sprinkled in along with a face and ears that somewhat resemble a house cat. The possum also has a big bushy tail.</description>
        </image>
      </figure>

      <p>
        <xref ref="fig-scattHeadLTotalL"/> shows a scatterplot for the head length and total length of the possums. Each point represents a single possum from the data. The head and total length variables are associated: possums with an above average total length also tend to have above average head lengths. While the relationship is not perfectly linear, it could be helpful to partially explain the connection between these variables with a straight line.
      </p>

      <figure xml:id="fig-scattHeadLTotalL">
        <caption>A scatterplot showing head length against total length for 104 brushtail possums. A point representing a possum with head length 94.1mm and total length 89cm is highlighted.</caption>
        <image source="ch_regr_simple_linear/figures/scattHeadLTotalL" width="75%">
          <description>A scatterplot showing head length against total length for 104 brushtail possums, where the horizontal axis for total length runs from 75 centimeters to about 97 centimeters (2.5 to 3.3 feet) and the vertical axis for head length runs from about 82 millimeters up to about 104 millimeters (3 to 4 inches). For possums with a total length between 75 to 80 centimeters, there are three points shown, each with head lengths of about 85 millimeters. For possums with total length from 80 to 85 centimeters, most head lengths range from about 85 millimeters to 95 millimeters. For possums with total lengths from 85 to 90 centimeters, head lengths mostly lie between 90 millimeters and 97 millimeters. For possums with total lengths larger than 90 centimeters, the head lengths are mostly between 93 millimeters and 100 millimeters. The trend is evidently upward and approximately linear. A point representing a possum with head length 94.1mm and total length 89cm is highlighted (although not relevant for any other purpose than giving an example or reminder for how a point is read in a scatterplot).</description>
        </image>
      </figure>

      <p>
        We want to describe the relationship between the head length and total length variables in the possum data set using a line. In this example, we will use the total length as the predictor variable, <m>x</m>, to predict a possum's head length, <m>y</m>. We could fit the linear relationship by eye, as in <xref ref="fig-scattHeadLTotalLLine"/>. The equation for this line is
        <me>\hat{y} = 41 + 0.59x</me>
        A "hat" on <m>y</m> is used to signify that this is an estimate. We can use this line to discuss properties of possums. For instance, the equation predicts a possum with a total length of 80 cm will have a head length of
        <md>
          <mrow>\hat{y} \amp = 41 + 0.59 \times 80</mrow>
          <mrow>\amp = 88.2</mrow>
        </md>
        The estimate may be viewed as an average: the equation predicts that possums with a total length of 80 cm will have an average head length of 88.2 mm. Absent further information about an 80 cm possum, the prediction for head length that uses the average is a reasonable estimate.
      </p>

      <figure xml:id="fig-scattHeadLTotalLLine">
        <caption>A reasonable linear model was fit to represent the relationship between head length and total length.</caption>
        <image source="ch_regr_simple_linear/figures/scattHeadLTotalLLine" width="70%">
          <description>The same scatterplot showing head length against total length for 104 brushtail possums is shown. A linear trend line has been added with an equation of y-hat equals 41 plus 0.59 times x, which shows the clear upward trajectory of the data. Additionally, three points are highlighted. The first is labeled with an "X" and is at approximately (77, 85) and lies about 1 unit below the trend line. A second point labeled with a "plus sign" is at about (85, 98) and appears to be about 7 units above the trend line. The last point highlighted is a "triangle" and is located at about (95, 93) and is about 3 units below the trend line.</description>
        </image>
      </figure>

      <example xml:id="ex-possum-predictors">
        <statement>
          <p>
            What other variables might help us predict the head length of a possum besides its length?
          </p>
        </statement>
        <solution>
          <p>
            Perhaps the relationship would be a little different for male possums than female possums, or perhaps it would differ for possums from one region of Australia versus another region. In <xref ref="ch-multiple-logistic-regression"/>, we'll learn about how we can include more than one predictor. Before we get there, we first need to better understand how to best build a simple linear model with one predictor.
          </p>
        </solution>
      </example>
    </subsection>

    <subsection xml:id="subsec-residuals">
      <title>Residuals</title>
      
      <p>
        <term>Residuals</term> are the leftover variation in the data after accounting for the model fit:
        <me>\text{Data} = \text{Fit} + \text{Residual}</me>
        Each observation will have a residual, and three of the residuals for the linear model we fit for the possum data is shown in <xref ref="fig-scattHeadLTotalLLine"/>. If an observation is above the regression line, then its residual, the vertical distance from the observation to the line, is positive. Observations below the line have negative residuals. One goal in picking the right linear model is for these residuals to be as small as possible.
      </p>

      <p>
        Let's look closer at the three residuals featured in <xref ref="fig-scattHeadLTotalLLine"/>. The observation marked by an "<m>\times</m>" has a small, negative residual of about -1; the observation marked by "<m>+</m>" has a large residual of about +7; and the observation marked by "<m>\triangle</m>" has a moderate residual of about -4. The size of a residual is usually discussed in terms of its absolute value. For example, the residual for "<m>\triangle</m>" is larger than that of "<m>\times</m>" because <m>|-4|</m> is larger than <m>|-1|</m>.
      </p>

      <assemblage xml:id="assem-residual-definition">
        <title>Residual: difference between observed and expected</title>
        <p>
          The <term>residual</term> of the <m>i^{\text{th}}</m> observation <m>(x_i, y_i)</m> is the difference of the observed response (<m>y_i</m>) and the response we would predict based on the model fit (<m>\hat{y}_i</m>):
          <me>e_i = y_i - \hat{y}_i</me>
          We typically identify <m>\hat{y}_i</m> by plugging <m>x_i</m> into the model.
        </p>
      </assemblage>

      <example xml:id="ex-compute-residual">
        <statement>
          <p>
            The linear fit shown in <xref ref="fig-scattHeadLTotalLLine"/> is given as <m>\hat{y} = 41 + 0.59x</m>. Based on this line, formally compute the residual of the observation <m>(77.0, 85.3)</m>. This observation is denoted by "<m>\times</m>" in <xref ref="fig-scattHeadLTotalLLine"/>. Check it against the earlier visual estimate, -1.
          </p>
        </statement>
        <solution>
          <p>
            We first compute the predicted value of point "<m>\times</m>" based on the model:
            <me>\hat{y}_{\times} = 41 + 0.59x_{\times} = 41 + 0.59 \times 77.0 = 86.4</me>
            Next we compute the difference of the actual head length and the predicted head length:
            <me>e_{\times} = y_{\times} - \hat{y}_{\times} = 85.3 - 86.4 = -1.1</me>
            The model's error is <m>e_{\times} = -1.1</m> mm, which is very close to the visual estimate of -1 mm. The negative residual indicates that the linear model overpredicted head length for this particular possum.
          </p>
        </solution>
      </example>

      <exercise xml:id="ex-residual-sign">
        <statement>
          <p>
            If a model underestimates an observation, will the residual be positive or negative? What about if it overestimates the observation?
          </p>
        </statement>
        <solution>
          <p>
            If a model underestimates an observation, then the model estimate is below the actual. The residual, which is the actual observation value minus the model estimate, must then be positive. The opposite is true when the model overestimates the observation: the residual is negative.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-compute-more-residuals">
        <statement>
          <p>
            Compute the residuals for the "<m>+</m>" observation <m>(85.0, 98.6)</m> and the "<m>\triangle</m>" observation <m>(95.5, 94.0)</m> in the figure using the linear relationship <m>\hat{y} = 41 + 0.59x</m>.
          </p>
        </statement>
        <solution>
          <p>
            (<m>+</m>) First compute the predicted value based on the model:
            <me>\hat{y}_{+} = 41 + 0.59x_{+} = 41 + 0.59 \times 85.0 = 91.15</me>
            Then the residual is given by
            <me>e_{+} = y_{+} - \hat{y}_{+} = 98.6 - 91.15 = 7.45</me>
            This was close to the earlier estimate of 7.
          </p>
          <p>
            (<m>\triangle</m>) <m>\hat{y}_{\triangle} = 41 + 0.59x_{\triangle} = 97.3</m>. <m>e_{\triangle} = y_{\triangle} - \hat{y}_{\triangle} = -3.3</m>, close to the estimate of -4.
          </p>
        </solution>
      </exercise>

      <p>
        Residuals are helpful in evaluating how well a linear model fits a data set. We often display them in a <term>residual plot</term> such as the one shown in <xref ref="fig-scattHeadLTotalLResidualPlot"/> for the regression line in <xref ref="fig-scattHeadLTotalLLine"/>. The residuals are plotted at their original horizontal locations but with the vertical coordinate as the residual. For instance, the point <m>(85.0, 98.6)_{+}</m> had a residual of 7.45, so in the residual plot it is placed at <m>(85.0, 7.45)</m>. Creating a residual plot is sort of like tipping the scatterplot over so the regression line is horizontal.
      </p>

      <figure xml:id="fig-scattHeadLTotalLResidualPlot">
        <caption>Residual plot for the model in <xref ref="fig-scattHeadLTotalLLine"/>.</caption>
        <image source="ch_regr_simple_linear/figures/scattHeadLTotalLResidualPlot" width="70%">
          <description>A residual plot for the trend line fit to the brushtail possum data is shown. Here, the horizontal axis is the same -- representing "total length", it spans 75 to 97 -- while the vertical axis represents "Residuals" and spans from about -7 to positive 8. There is no evident trend in the residuals. Three points are specifically highlighted to reflect the three points discussed in the last figure. The first is labeled with an "X" with a total length of 77 and a residual of about -1. The second is labeled with a "plus sign" and has a total length of 85 and a residual of about 7. The last point highlighted is a "triangle" with a total length of about 95 and a residual of about -3. Note that the location of the residuals above and below the trend line reflects exactly with whether the residual is positive or negative, respectively.</description>
        </image>
      </figure>

      <example xml:id="ex-residual-patterns">
        <statement>
          <p>
            One purpose of residual plots is to identify characteristics or patterns still apparent in data after fitting a model. <xref ref="fig-sampleLinesAndResPlots"/> shows three scatterplots with linear models in the first row and residual plots in the second row. Can you identify any patterns remaining in the residuals?
          </p>
        </statement>
        <solution>
          <p>
            In the first data set (first column), the residuals show no obvious patterns. The residuals appear to be scattered randomly around the dashed line that represents 0.
          </p>
          <p>
            The second data set shows a pattern in the residuals. There is some curvature in the scatterplot, which is more obvious in the residual plot. We should not use a straight line to model these data. Instead, a more advanced technique should be used.
          </p>
          <p>
            The last plot shows very little upwards trend, and the residuals also show no obvious patterns. It is reasonable to try to fit a linear model to the data. However, it is unclear whether there is statistically significant evidence that the slope parameter is different from zero. The point estimate of the slope parameter, labeled <m>b_1</m>, is not zero, but we might wonder if this could just be due to chance. We will address this sort of scenario in <xref ref="sec-inference-for-regression"/>.
          </p>
        </solution>
      </example>

      <figure xml:id="fig-sampleLinesAndResPlots">
        <caption>Sample data with their best fitting lines (top row) and their corresponding residual plots (bottom row).</caption>
        <image source="ch_regr_simple_linear/figures/sampleLinesAndResPlots" width="90%">
          <description>Sample data with their best fitting lines (top row of three plots) and their corresponding residual plots (bottom row of three plots). The upper left plot shows a scatterplot where the data trend downwards steadily with a straight line fit to the data, which appears to fit well everywhere. The bottom left plot is the residual plot of this first scatterplot, and it likewise shows no pattern in the residuals when looking left to right. The upper middle plot shows data with a downward trend, but the data's trend is more steep on the right side of the plot, so the overall shape of the data is that it trends downward and curves downward. A straight, downward-trending line has also been fit to this data, but it doesn't fit as well. The data are below this downward trending line initially, but it is above the line in the middle, and finally on the right it is once again below the linear trend line. The residual plot for this scatterplot is shown in the lower middle plot, and the curvature in the residuals is more evident than what was visible in the scatterplot: the residuals have negative values on the left and trend upwards until peaking with positive residuals in the middle, and then trending back down and having negative residual values again on the right. The last scatterplot in the upper right shows data with very little trend, but a slightly-upward trending straight line has been fit to the data. The corresponding residual plot, shown as the bottom right plot, also shows data with no evident trend or pattern, where observations appear relatively randomly scattered above and below 0 (in the vertical).</description>
        </image>
      </figure>
    </subsection>

    <subsection xml:id="subsec-correlation">
      <title>Describing linear relationships with correlation</title>
      
      <p>
        We've seen plots with strong linear relationships and others with very weak linear relationships. It would be useful if we could quantify the strength of these linear relationships with a statistic.
      </p>

      <assemblage xml:id="assem-correlation-definition">
        <title>Correlation: strength of a linear relationship</title>
        <p>
          <term>Correlation</term>, which always takes values between -1 and 1, describes the strength of the linear relationship between two variables. We denote the correlation by <m>R</m>.
        </p>
      </assemblage>

      <p>
        We can compute the correlation using a formula, just as we did with the sample mean and standard deviation. This formula is rather complex,<fn>Formally, we can compute the correlation for observations <m>(x_1, y_1)</m>, <m>(x_2, y_2)</m>, <ellipsis/>, <m>(x_n, y_n)</m> using the formula
        <me>R = \frac{1}{n-1} \sum_{i=1}^{n} \frac{x_i - \bar{x}}{s_x} \cdot \frac{y_i - \bar{y}}{s_y}</me>
        where <m>\bar{x}</m>, <m>\bar{y}</m>, <m>s_x</m>, and <m>s_y</m> are the sample means and standard deviations for each variable.</fn> and like with other statistics, we generally perform the calculations on a computer or calculator. <xref ref="fig-posNegCorPlots"/> shows eight plots and their corresponding correlations. Only when the relationship is perfectly linear is the correlation either -1 or 1. If the relationship is strong and positive, the correlation will be near +1. If it is strong and negative, it will be near -1. If there is no apparent linear relationship between the variables, then the correlation will be near zero.
      </p>

      <figure xml:id="fig-posNegCorPlots">
        <caption>Sample scatterplots and their correlations. The first row shows variables with a positive relationship, represented by the trend up and to the right. The second row shows one plot with an approximately neutral trend and three plots with a negative trend.</caption>
        <image source="ch_regr_simple_linear/figures/posNegCorPlots" width="90%">
          <description>Eight scatterplots are shown, each with their correlation noted. Each scatterplot appears to represent about 50 points. The first has a correlation of R equals 0.33, and there is a slight upward trend evident in the data -- if a trend line were drawn for this data, much of the data would fall relatively far from the line. The second plot has a correlation of R equals 0.69, and a clearer upward trend is evident, but it is still pretty volatile with many points deviating far from where the trend line would be. The third plot has a correlation of 0.98, and the data show a very clear upward trend, where if a trend line were drawn, the data would be (relatively) quite close to this line. The fourth plot shows a correlation of R equals 1.00, and here the points appear exactly on a line with an upward trajectory. The fifth plot shows data with a correlation of R equals 0.08, where no trend is visually evident in the data. The sixth plot has a correlation of R equals -0.64, and a downward trend is evident in the data, but the individual observations would in many cases be pretty distant from any trend line fit to the data (on a relative basis). The seventh plot has a correlation of R equals -0.92 and shows data with a clear downward trend, where the data would deviate just a modest amount from a trend line fit to the data. The last plot shows a correlation of R equals -1, where the observations would fit exactly on a line trending downwards.</description>
        </image>
      </figure>

      <p>
        The correlation is intended to quantify the strength of a linear trend. Nonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship; see three such examples in <xref ref="fig-corForNonLinearPlots"/>.
      </p>

      <figure xml:id="fig-corForNonLinearPlots">
        <caption>Sample scatterplots and their correlations. In each case, there is a strong relationship between the variables. However, because the relationship is nonlinear, the correlation is relatively weak.</caption>
        <image source="ch_regr_simple_linear/figures/corForNonLinearPlots" width="85%">
          <description>Three scatterplots are shown. In each case, there is a strong relationship between the variables. However, because the relationship is nonlinear, the correlation is relatively weak. The first plot shows data that trends upwards on the left before peaking and then trending downward on the right -- the correlation of the data in this plot is R equals -0.23. The second plot shows data with a sharp downward trend on the left before reaching a trough and rising then sharply upward before reaching a peak and then trending sharply downwards again -- the correlation of the data in this plot is R equals 0.31. The third plot shows data that without a trend on the far left, followed by a steep drop, a trough, and then a steep rise to a peak, and then another drop and then finally a slight increase at the end -- the correlation of the data in this plot is R equals 0.50.</description>
        </image>
      </figure>

      <exercise xml:id="ex-nonlinear-curves">
        <statement>
          <p>
            No straight line is a good fit for the data sets represented in <xref ref="fig-corForNonLinearPlots"/>. Try drawing nonlinear curves on each plot. Once you create a curve for each, describe what is important in your fit.
          </p>
        </statement>
        <solution>
          <p>
            We'll leave it to you to draw the lines. In general, the lines you draw should be close to most points and reflect overall trends in the data.
          </p>
        </solution>
      </exercise>
    </subsection>

    <!-- Exercises for Section 8.1 -->
    <exercises xml:id="exercises-sec-fitting-line-residuals-correlation">
      <title>Section 8.1 Exercises</title>

      <exercise xml:id="ex-visualize-residuals">
        <title>Visualize the residuals</title>
        <statement>
          <p>
            The scatterplots shown below each have a superimposed regression line. If we were to construct a residual plot (residuals versus <m>x</m>) for each, describe what those plots would look like.
          </p>
          <sidebyside widths="42% 42%">
            <image source="ch_regr_simple_linear/figures/eoce/visualize_residuals/visualize_residuals_linear">
              <description>A scatterplot is shown, where the data have a steady upward trend throughout. The observations above and below the line appear random and have stable variability moving from left to right.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/visualize_residuals/visualize_residuals_fan_back">
              <description>A scatterplot is shown, where the data have a steady upward trend throughout. The observations above and below the line appear random. If looking at the leftmost region of data, the observations are more broadly scattered around the line, while when moving further right the variability of the points around the line gets notably smaller by a factor of at least 5 (if using standard deviation).</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-trends-in-residuals">
        <title>Trends in the residuals</title>
        <statement>
          <p>
            Shown below are two plots of residuals remaining after fitting a linear model to two different sets of data. Describe important features and determine if a linear model would be appropriate for these data. Explain your reasoning.
          </p>
          <sidebyside widths="42% 42%">
            <image source="ch_regr_simple_linear/figures/eoce/trends_in_residuals/trends_in_residuals_fan">
              <description>A scatterplot of the residuals is shown. When looking at any horizontal region of the plot, the observations are consistently scattered around the "y equals 0" line. On the left, the points tend to be very close to this horizontal 0 line. The further moving to the right, the more variability that is evident in the observations around "y equals 0".</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/trends_in_residuals/trends_in_residuals_log">
              <description>A scatterplot of the residuals is shown. The points on the very left tend to be below the "y equals 0" line for the first 5% of the horizontal region, where the trend is sharply upwards to the "y equals 0" line. The points then tend to be stably clustered around "y equals 0", if not slightly above, with a slight downward trend evident in the observations on the right half of the plot. The vertical variability of observations is about stable throughout.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-identify-relationships-1">
        <title>Identify relationships, Part I</title>
        <statement>
          <p>
            For each of the six plots, identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable.
          </p>
          <sidebyside widths="32% 32% 32%">
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_1/identify_relationships_u">
              <description>A scatterplot is shown. The observations start in the upper left corner of the plot, trend sharply downwards before tapering off and stabilizing at about the middle of the plot, before steadily and then faster rising again to the upper right corner of the plot. The trend is approximately symmetric from left-to-right.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_1/identify_relationships_lin_pos_strong">
              <description>A scatterplot is shown. The points start on the lower left corner, only spanning about 20% of the vertical region of the plot, and have a steady upwards trend to the upper right corner of the plot. The vertical variability of the points around the trend is relatively stable across the plot.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_1/identify_relationships_lin_pos_weak">
              <description>A scatterplot is shown. On the left side of the plot, the points appear randomly scattered across the full range of the plot, and this property holds across the entire plot. No trend is evident.</description>
            </image>
          </sidebyside>
          <sidebyside widths="32% 32% 32%">
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_1/identify_relationships_n">
              <description>A scatterplot is shown. On the left side of the plot, the observations are concentrated in the bottom half of the plot but rise steadily. The trend peaks near the center of the plot, where nearly all the points in the (horizontal) center region of the scatterplot are concentrated in the upper half of the scatterplot. On the right side of the plot, the points show a trend downwards, with points concentrated in the lower quarter of the scatterplot for the rightmost handful of points.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_1/identify_relationships_lin_neg_strong">
              <description>A scatterplot is shown. The points start on the upper left corner, only spanning about 20% of the vertical region of the plot, and have a steady downwards trend to the bottom right corner of the plot. The vertical variability of the points around the trend is relatively stable across the plot.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_1/identify_relationships_none">
              <description>A scatterplot is shown. On the left side of the plot, the points appear randomly scattered across the full range of the plot, and this property holds across the entire plot. No trend is evident or at least obvious.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-identify-relationships-2">
        <title>Identify relationships, Part II</title>
        <statement>
          <p>
            For each of the six plots, identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable.
          </p>
          <sidebyside widths="32% 32% 32%">
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_2/identify_relationships_s">
              <description>A scatterplot is shown. On the left side of the plot, the observations are concentrated in the upper corner of the plot, with a sharp trend downwards, before stabilizing, then rising slightly at halfway through the plot, reaching a peak, and then declining again, with a sharp decline on the right-most portion of the plot to the bottom-right corner of the plot.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_2/identify_relationships_hockey_stick">
              <description>A scatterplot is shown. On the left side of the plot, the observations are concentrated around a region about 30% of the way up from the bottom-left corner of the plot, there is a slight downward trend that reaches the bottom area of the plot for about the center half of the plot, then the points rise gradually and then sharply in the last 25-30% of the plot.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_2/identify_relationships_pos_lin_strong">
              <description>A scatterplot is shown. Points in the leftmost region of the plot are concentrated in the lower-left corner, ranging from the bottom up to about 25% of the way up the plot. The points follow a steady upward trend to the top-right corner of the plot and show consistent vertical variability around the trend throughout.</description>
            </image>
          </sidebyside>
          <sidebyside widths="32% 32% 32%">
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_2/identify_relationships_pos_weak">
              <description>A scatterplot is shown. The points appear randomly scattered across the left, middle, and right portion of the plot. There might be a very slight upward trend.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_2/identify_relationships_pos_weaker">
              <description>A scatterplot is shown. The points appear randomly scattered across the left, middle, and right portion of the plot. There is a very slight downward trend.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/identify_relationships_2/identify_relationships_neg_lin_weak">
              <description>A scatterplot is shown. The points on the leftmost side are concentrated in the upper half of the plot, and the data trend steadily downwards and with consistent variability to the bottom right portion of the plot.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-exams-grades-correlation">
        <title>Exams and grades</title>
        <statement>
          <p>
            The two scatterplots below show the relationship between final and mid-semester exam grades recorded during several years for a Statistics course at a university.
          </p>
          <sidebyside widths="48% 48%">
            <image source="ch_regr_simple_linear/figures/eoce/exams_grades_correlation/exam_grades_1">
              <description>A scatter plot with 100 points is shown with an upward trending line fit to the data. Exam 1 scores are on the horizontal axis and range from 40 to 100. Final Exam scores are on the vertical axis and also range from 40 to 100. Only about ten Exam 1 scores are below 60, and these have Final Exam scores between about 55 and 85. Exam 1 scores between 60 and 80 represent about 50% of the points shown and have Final Exam scores mostly between 50 and 85. For the points where Midterm 1 scores are larger than 80, Final Exam scores mostly lie between 65 and 90, where a slightly upward trend is evident.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/exams_grades_correlation/exam_grades_2">
              <description>A scatter plot with 100 points is shown with an upward trending line fit to the data. Exam 2 scores are on the horizontal axis and range from 40 to 100. Final Exam scores are on the vertical axis and also range from 40 to 100. Midterm 2 scores are roughly uniformly distributed across the full range. For Exam 2 scores below 60, these mostly have Final Exam scores between about 45 and 70. Exam 2 scores between 60 and 80 have Final Exam scores mostly between 55 and 80. For the points where Midterm 2 scores are larger than 80, Final Exam scores mostly lie between 70 and 90.</description>
            </image>
          </sidebyside>
          <p>
            <ol>
              <li>Based on these graphs, which of the two exams has the strongest correlation with the final exam grade? Explain.</li>
              <li>Can you think of a reason why the correlation between the exam you chose in part (a) and the final exam is higher?</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-husbands-wives-correlation">
        <title>Husbands and wives, Part I</title>
        <statement>
          <p>
            The Great Britain Office of Population Census and Surveys once collected data on a random sample of 170 married couples in Britain, recording the age (in years) and heights (converted here to inches) of the husbands and wives. The scatterplot on the left shows the wife's age plotted against her husband's age, and the plot on the right shows wife's height plotted against husband's height.
          </p>
          <sidebyside widths="35% 35%">
            <image source="ch_regr_simple_linear/figures/eoce/husbands_wives_correlation/husbands_wives_age">
              <description>A scatterplot is shown. The horizontal axis represents "Husband's Age (in years)" with values ranging from about 20 to 65. The vertical axis represents "Wife's Age (in years)" with values ranging from about 18 to 65. When husband age is between 20 and 30, wife age mostly ranges from 18 to about 30. When husband age is between 30 and 40, wife age mostly ranges from 23 to about 40. When husband age is between 40 and 50, wife age mostly ranges from 35 to about 50. When husband age is between 50 and 60, wife age mostly ranges from 45 to about 60. When husband age is larger than 60, wife age mostly ranges from 55 to about 65.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/husbands_wives_correlation/husbands_wives_height">
              <description>A scatterplot is shown. The horizontal axis represents "Husband's Height (in inches)" with values ranging from about 60 to 75. The vertical axis represents "Wife's Height (in inches)" with values ranging from about 55 to 70. When husband height is between 60 and 65, wife height mostly ranges from about 58 to 65 inches, though there are only about 10 points in this range, which is about 5% of the data. When husband height is between 65 and 70, wife height mostly ranges from 57 to 68 inches. When husband height is larger than 70 inches, wife height mostly ranges from 61 to about 74 inches.</description>
            </image>
          </sidebyside>
          <p>
            <ol>
              <li>Describe the relationship between husbands' and wives' ages.</li>
              <li>Describe the relationship between husbands' and wives' heights.</li>
              <li>Which plot shows a stronger correlation? Explain your reasoning.</li>
              <li>Data on heights were originally collected in centimeters, and then converted to inches. Does this conversion affect the correlation between husbands' and wives' heights?</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-match-correlation-1">
        <title>Match the correlation, Part I</title>
        <statement>
          <p>
            Match each correlation to the corresponding scatterplot.
          </p>
          <p>
            <ol marker="a.">
              <li><m>R = -0.7</m></li>
              <li><m>R = 0.45</m></li>
              <li><m>R = 0.06</m></li>
              <li><m>R = 0.92</m></li>
            </ol>
          </p>
          <sidebyside widths="24% 24% 24% 24%">
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_1/match_corr_1_u">
              <description>A scatterplot is shown. The observations start in the upper left corner of the plot, trend sharply downwards before tapering off and stabilizing at about the middle of the plot, before steadily and then faster rising again to the upper right corner of the plot. The trend is approximately symmetric from left-to-right.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_1/match_corr_2_strong_pos">
              <description>A scatterplot is shown. The points start on the lower left corner, only spanning about 20% of the vertical region of the plot, and have a steady upwards trend to the upper right corner of the plot. The vertical variability of the points around the trend is relatively stable across the plot.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_1/match_corr_3_weak_pos">
              <description>A scatterplot is shown. The points appear randomly scattered across the left, middle, and right portion of the plot. There is a very slight upward trend.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_1/match_corr_4_weak_neg">
              <description>A scatterplot is shown. The points appear randomly scattered across the left, middle, and right portion of the plot. There is a very slight downward trend.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-match-correlation-2">
        <title>Match the correlation, Part II</title>
        <statement>
          <p>
            Match each correlation to the corresponding scatterplot.
          </p>
          <p>
            <ol marker="a.">
              <li><m>R = 0.49</m></li>
              <li><m>R = -0.48</m></li>
              <li><m>R = -0.03</m></li>
              <li><m>R = -0.85</m></li>
            </ol>
          </p>
          <sidebyside widths="24% 24% 24% 24%">
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_2/match_corr_1_strong_neg_curved">
              <description>A scatterplot is shown. For the left half of the plot, the points are scattered around the upper half of the plot. On the right portion of the plot, the data show a clear downward trend, and for the points on the far right, they are concentrated in the lower 25% of the plot.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_2/match_corr_2_weak_pos">
              <description>A scatterplot is shown. The points appear randomly scattered across the left, middle, and right portion of the plot. There is a very slight upward trend.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_2/match_corr_3_n">
              <description>A scatterplot is shown. The observations start in the lower left corner of the plot, trend sharply upwards before tapering off and stabilizing at about the middle of the plot, before steadily and then faster dropping to the lower right corner of the plot. The trend is approximately symmetric from left-to-right.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/match_corr_2/match_corr_4_weak_neg">
              <description>A scatterplot is shown. The points appear randomly scattered across the left, middle, and right portion of the plot. There is a very slight downward trend.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-speed-height">
        <title>Speed and height</title>
        <statement>
          <p>
            1,302 UCLA students were asked to fill out a survey where they were asked about their height, fastest speed they have ever driven, and gender. The scatterplot on the left displays the relationship between height and fastest speed, and the scatterplot on the right displays the breakdown by gender in this relationship.
          </p>
          <sidebyside widths="40% 40%">
            <image source="ch_regr_simple_linear/figures/eoce/speed_height_gender/speed_height">
              <description>A scatterplot is shown. The horizontal axis represents "Height (in inches)" with values ranging from about 50 to 80. The vertical axis represents "Fastest Speed (in mph)" and has values ranging from 0 to 150. First, it is worth noting that there several points along the bottom of the plot with a fastest speed of 0 mph. The remainder of the description will concentrate on the other points. A small portion of the points are shown with heights below 60 inches, and these have fastest speeds mostly ranging from about 70 to 110 mph. For points shown with heights between 60 and 70, fastest speeds mostly ranged from about 30 to 120 mph. For points shown with heights of 70 or more, fastest speeds mostly ranged from about 50 to 140 mph. There were no points corresponding to heights greater than 75 that had fastest speeds slower than about 75 mph, which left a sort of gap in the lower right portion of the scatterplot.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/speed_height_gender/speed_height_gender">
              <description>A scatterplot is shown, where points are colored to differentiate between males and females. The horizontal axis represents "Height (in inches)" with values ranging from about 50 to 80. The vertical axis represents "Fastest Speed (in mph)" and has values ranging from 0 to 150. Female heights are largely 70 inches or smaller, while Male heights are largely 65 inches and taller. When focusing exclusively on Females, no upward trend is evident, with about 95% of observations having Fastest Speed between about 30 mph and 120 mph. When focusing exclusively on Males, no upward trend is evident there either, with about 95% of observations having Fastest Speed between about 50 mph and 140 mph. In contrast, if we ignore the male/female differentiation, there is a slight upward trend in the points.</description>
            </image>
          </sidebyside>
          <p>
            <ol>
              <li>Describe the relationship between height and fastest speed.</li>
              <li>Why do you think these variables are positively associated?</li>
              <li>What role does gender play in the relationship between height and fastest driving speed?</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-guess-correlation">
        <title>Guess the correlation</title>
        <statement>
          <p>
            Eduardo and Rosie are both collecting data on number of rainy days in a year and the total rainfall for the year. Eduardo records rainfall in inches and Rosie in centimeters. How will their correlation coefficients compare?
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-coast-starlight-corr">
        <title>The Coast Starlight, Part I</title>
        <statement>
          <p>
            The Coast Starlight Amtrak train runs from Seattle to Los Angeles. The scatterplot below displays the distance between each stop (in miles) and the amount of time it takes to travel from one stop to another (in minutes).
          </p>
          <sidebyside widths="55% 40%">
            <stack>
              <p>
                <ol>
                  <li>Describe the relationship between distance and travel time.</li>
                  <li>How would the relationship change if travel time was instead measured in hours, and distance was instead measured in kilometers?</li>
                  <li>The correlation between travel time (in miles) and distance (in minutes) is <m>r = 0.636</m>. Suppose we had instead measured travel time in hours and measured distance in kilometers (km). What would be the correlation in these different units?</li>
                </ol>
              </p>
            </stack>
            <image source="ch_regr_simple_linear/figures/eoce/coast_starlight_corr_units/coast_starlight">
              <description>A scatterplot is shown with about 15 points. The horizontal axis represents "Distance (miles)" with values ranging from just over 0 to about 350. The vertical axis represents "Travel Time (in minutes)" and has values ranging from about 20 to 380. The point with the smallest distance -- about 10 miles -- shows a travel time of about 40 minutes. Next, there is a cluster of 6 points with distances between 40 and 60 miles and travel times ranging from about 20 to 60 minutes. The remainder of the points are scattered pretty broadly but may show a slightly upward trend. A few points that highlight the widely varying nature of the data are located at the following approximate locations: (190 miles, 60 minutes), (240 miles, 250 minutes), (250 miles, 380 minutes), and (350 miles, 200 minutes).</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-crawling-babies-corr">
        <title>Crawling babies, Part I</title>
        <statement>
          <p>
            A study conducted at the University of Denver investigated whether babies take longer to learn to crawl in cold months, when they are often bundled in clothes that restrict their movement, than in warmer months. Infants born during the study year were split into twelve groups, one for each birth month. We consider the average crawling age of babies in each group against the average temperature when the babies are six months old (that's when babies often begin trying to crawl). Temperature is measured in degrees Fahrenheit (<m>^\circ</m>F) and age is measured in weeks.
          </p>
          <sidebyside widths="55% 40%">
            <stack>
              <p>
                <ol>
                  <li>Describe the relationship between temperature and crawling age.</li>
                  <li>How would the relationship change if temperature was measured in degrees Celsius (<m>^\circ</m>C) and age was measured in months?</li>
                  <li>The correlation between temperature in <m>^\circ</m>F and age in weeks was <m>r = -0.70</m>. If we converted the temperature to <m>^\circ</m>C and age to months, what would the correlation be?</li>
                </ol>
              </p>
            </stack>
            <image source="ch_regr_simple_linear/figures/eoce/crawling_babies_corr_units/crawling_babies">
              <description>A scatterplot is shown with a dozen points. The horizontal axis is "Temperature (F)" with values ranging from 30 to 75. The vertical axis is "Average Crawling Age (weeks)" with values ranging from 28.5 to 34. For those points with temperatures from 30 to 40, average crawling ages range from 31.5 to 34. For the single point with temperatures between 40 to 50, average crawling age was about 33.5. For the two points with temperature between 50 and 60, average crawling age was 28.5 and 32.5. For the last 4 points with temperature above 60, average crawling ages were 32, 30, 30, and 30.5.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-body-measurements-shoulder">
        <title>Body measurements, Part I</title>
        <statement>
          <p>
            Researchers studying anthropometry collected body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender for 507 physically active individuals. The scatterplot below shows the relationship between height and shoulder girth (over deltoid muscles), both measured in centimeters.
          </p>
          <sidebyside widths="55% 40%">
            <stack>
              <p>
                <ol>
                  <li>Describe the relationship between shoulder girth and height.</li>
                  <li>How would the relationship change if shoulder girth was measured in inches while the units of height remained in centimeters?</li>
                </ol>
              </p>
            </stack>
            <image source="ch_regr_simple_linear/figures/eoce/body_measurements_shoulder_height_corr_units/body_measurements_height_shoulder_girth">
              <description>A scatter plot with several hundred points is shown. The horizontal axis represents "Shoulder Girth (cm)" with values ranging from about 85 to 135. The vertical axis represents "Height (cm)" with values ranging from about 145 to 200. For points where Shoulder Girth is smaller than 100, 95% of points have heights between 152 and 170. For points where Shoulder Girth is between 100 and 110, 95% of points have heights between 155 and 180. For points where Shoulder Girth is between 110 and 120, 95% of points have heights between 162 and 190. For points where Shoulder Girth larger than 120, 95% of points have heights between 170 and 190.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-body-measurements-hip">
        <title>Body measurements, Part II</title>
        <statement>
          <p>
            The scatterplot below shows the relationship between weight measured in kilograms and hip girth measured in centimeters from the data described in <xref ref="ex-body-measurements-shoulder"/>.
          </p>
          <sidebyside widths="55% 40%">
            <stack>
              <p>
                <ol>
                  <li>Describe the relationship between hip girth and weight.</li>
                  <li>How would the relationship change if weight was measured in pounds while the units for hip girth remained in centimeters?</li>
                </ol>
              </p>
            </stack>
            <image source="ch_regr_simple_linear/figures/eoce/body_measurements_hip_weight_corr_units/body_measurements_weight_hip_girth">
              <description>A scatter plot with several hundred points is shown. The horizontal axis represents "Hip Girth (cm)" with values ranging from about 80 to 115, with about 4 observations with larger hip girth up to about 130 cm. The vertical axis represents "Weight (kg)" with values ranging from about 40 to 105, with a few observations with larger weights up to 120. For points where Hip Girth is smaller than 90, 95% of points have weight between roughly 45 and 60. For points where Hip Girth is between 90 and 100, 95% of points have weight between roughly 50 and 80. For points where Hip Girth is between 100 and 110, 95% of points have weight between roughly 65 and 90. For points where Hip Girth is between 110 and 115, points have weight between roughly 70 and 105. There are four additional points located at about (115, 120), (115, 90), (118, 90), and (128, 105).</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-correlation-husband-wife-age">
        <title>Correlation, Part I</title>
        <statement>
          <p>
            What would be the correlation between the ages of husbands and wives if men always married women who were
          </p>
          <p>
            <ol>
              <li>3 years younger than themselves?</li>
              <li>2 years older than themselves?</li>
              <li>half as old as themselves?</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-correlation-salary">
        <title>Correlation, Part II</title>
        <statement>
          <p>
            What would be the correlation between the annual salaries of males and females at a company if for a certain type of position men always made
          </p>
          <p>
            <ol>
              <li>$5,000 more than women?</li>
              <li>25% more than women?</li>
              <li>15% less than women?</li>
            </ol>
          </p>
        </statement>
      </exercise>
    </exercises>
  </section>

  <!-- Section 8.2: Least squares regression -->
  <section xml:id="sec-least-squares-regression">
    <title>Least squares regression</title>
    
    <introduction>
      <p>
        Fitting linear models by eye is open to criticism since it is based on an individual's preference. In this section, we use <term>least squares regression</term> as a more rigorous approach.
      </p>
    </introduction>

    <subsection xml:id="subsec-gift-aid-elmhurst">
      <title>Gift aid for freshman at Elmhurst College</title>
      
      <p>
        This section considers family income and gift aid data from a random sample of fifty students in the freshman class of Elmhurst College in Illinois. Gift aid is financial aid that does not need to be paid back, as opposed to a loan. A scatterplot of the data is shown in <xref ref="fig-elmhurstScatterW2Lines"/> along with two linear fits. The lines follow a negative trend in the data; students who have higher family incomes tended to have lower gift aid from the university.
      </p>

      <figure xml:id="fig-elmhurstScatterW2Lines">
        <caption>Gift aid and family income for a random sample of 50 freshman students from Elmhurst College. Two lines are fit to the data, the solid line being the <em>least squares line</em>.</caption>
        <image source="ch_regr_simple_linear/figures/elmhurstScatterW2Lines" width="67%">
          <description>A scatterplot is shown for a random sample of 50 freshman students from Elmhurst College. The horizontal axis is for "family income" and has values ranging from $0 to about $300,000. The vertical axis is for "gift aid" and has values ranging from $0 to about $35,000. Two lines are fit to the data showing a slight downward trend. One of those lines is a solid line representing what is called the "least squares line". About 10 observations are shown where family income is between $0 and $50,000, and gift aid for these values is roughly between $17,000 and $28,000. About 20 observations are shown where family income is between $50,000 and $100,000, and gift aid for these values is roughly between $10,000 and $33,000. About 10 observations are shown where family income is between $100,000 and $150,000, and gift aid for these values is roughly between $9,000 and $25,000. Three observations are shown where family income is between $150,000 and $200,000, and gift aid for these values are $25,000, $12,000, and $13,000. Six more observations are shown where family income is larger than $200,000, and gift aid for these values ranges from about $7,000 to $22,000. The data in this graph will be frequently discussed throughout this section and referred to as the "Elmhurst data".</description>
        </image>
      </figure>

      <exercise xml:id="ex-elmhurst-correlation-sign">
        <statement>
          <p>
            Is the correlation positive or negative in <xref ref="fig-elmhurstScatterW2Lines"/>?
          </p>
        </statement>
        <solution>
          <p>
            Larger family incomes are associated with lower amounts of aid, so the correlation will be negative. Using a computer, the correlation can be computed: -0.499.
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-least-squares-criterion">
      <title>An objective measure for finding the best line</title>
      
      <p>
        We begin by thinking about what we mean by <q>best</q>. Mathematically, we want a line that has small residuals. The first option that may come to mind is to minimize the sum of the residual magnitudes:
        <me>|e_1| + |e_2| + \dots + |e_n|</me>
        which we could accomplish with a computer program. The resulting dashed line shown in <xref ref="fig-elmhurstScatterW2Lines"/> demonstrates this fit can be quite reasonable. However, a more common practice is to choose the line that minimizes the sum of the squared residuals:
        <me>e_{1}^2 + e_{2}^2 + \dots + e_{n}^2</me>
      </p>

      <p>
        The line that minimizes this <term>least squares criterion</term> is represented as the solid line in <xref ref="fig-elmhurstScatterW2Lines"/>. This is commonly called the <term>least squares line</term>. The following are three possible reasons to choose this option instead of trying to minimize the sum of residual magnitudes without any squaring:
        <ol>
          <li><p>It is the most commonly used method.</p></li>
          <li><p>Computing the least squares line is widely supported in statistical software.</p></li>
          <li><p>In many applications, a residual twice as large as another residual is more than twice as bad. For example, being off by 4 is usually more than twice as bad as being off by 2. Squaring the residuals accounts for this discrepancy.</p></li>
        </ol>
        The first two reasons are largely for tradition and convenience; the last reason explains why the least squares criterion is typically most helpful.<fn>There are applications where the sum of residual magnitudes may be more useful, and there are plenty of other criteria we might consider. However, this book only applies the least squares criterion.</fn>
      </p>
    </subsection>

    <subsection xml:id="subsec-regression-conditions">
      <title>Conditions for the least squares line</title>
      
      <p>
        When fitting a least squares line, we generally require
      </p>

      <p>
        <dl>
          <li>
            <title>Linearity</title>
            <p>The data should show a linear trend. If there is a nonlinear trend (e.g. left panel of <xref ref="fig-whatCanGoWrongWithLinearModel"/>), an advanced regression method from another book or later course should be applied.</p>
          </li>
          <li>
            <title>Nearly normal residuals</title>
            <p>Generally, the residuals must be nearly normal. When this condition is found to be unreasonable, it is usually because of outliers or concerns about influential points, which we'll talk about more in <xref ref="sec-outliers-in-regression"/>. An example of a residual that would be a potential concern is shown in <xref ref="fig-whatCanGoWrongWithLinearModel"/>, where one observation is clearly much further from the regression line than the others.</p>
          </li>
          <li>
            <title>Constant variability</title>
            <p>The variability of points around the least squares line remains roughly constant. An example of non-constant variability is shown in the third panel of <xref ref="fig-whatCanGoWrongWithLinearModel"/>, which represents the most common pattern observed when this condition fails: the variability of <m>y</m> is larger when <m>x</m> is larger.</p>
          </li>
          <li>
            <title>Independent observations</title>
            <p>Be cautious about applying regression to <term>time series</term> data, which are sequential observations in time such as a stock price each day. Such data may have an underlying structure that should be considered in a model and analysis. An example of a data set where successive observations are not independent is shown in the fourth panel of <xref ref="fig-whatCanGoWrongWithLinearModel"/>. There are also other instances where correlations within the data are important, which is further discussed in Chapter 9.</p>
          </li>
        </dl>
      </p>

      <figure xml:id="fig-whatCanGoWrongWithLinearModel">
        <caption>Four examples showing when the methods in this chapter are insufficient to apply to the data. First panel: linearity fails. Second panel: there are outliers, most especially one point that is very far away from the line. Third panel: the variability of the errors is related to the value of <m>x</m>. Fourth panel: a time series data set is shown, where successive observations are highly correlated.</caption>
        <image source="ch_regr_simple_linear/figures/whatCanGoWrongWithLinearModel" width="100%">
          <description>Four scatterplots are shown, each with their own residual plot. These four examples show when methods in this chapter are insufficient to apply to the data. In the first set, a scatterplot with arch-shaped data is shown with a straight line fit to the data, which poorly fits the curved nature of the data; this is meant to highlight an example where "linearity" fails. In the second set, a set of data with a line fit is shown, where the data tightly pack around the line, except one point in particular that is far from the line and represents the case where there are "extreme outliers" in the data. The third set shows a case where a straight line fits the data, but the variability around the line changes, where observations tend to be quite close to the line on the left, but when looking further right, the observations tend to be increasingly far from the line, indicating "changing variability" in the residuals over different regions of the plot. The fourth set provides another case of what is called "time series" data, which is a context where "successive observations are correlated".</description>
        </image>
      </figure>

      <exercise xml:id="ex-elmhurst-conditions">
        <statement>
          <p>
            Should we have concerns about applying least squares regression to the Elmhurst data in <xref ref="fig-elmhurstScatterW2Lines"/>?
          </p>
        </statement>
        <solution>
          <p>
            The trend appears to be linear, the data fall around the line with no obvious outliers, the variance is roughly constant. These are also not time series observations. Least squares regression can be applied to these data.
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-finding-least-squares-line">
      <title>Finding the least squares line</title>
      
      <p>
        For the Elmhurst data, we could write the equation of the least squares regression line as
        <me>\widehat{\text{aid}} = \beta_0 + \beta_{1}\times \text{family\_income}</me>
        Here the equation is set up to predict gift aid based on a student's family income, which would be useful to students considering Elmhurst. These two values, <m>\beta_0</m> and <m>\beta_1</m>, are the parameters of the regression line.
      </p>

      <p>
        As in Chapters 4, 5, and 6, the parameters are estimated using observed data. In practice, this estimation is done using a computer in the same way that other estimates, like a sample mean, can be estimated using a computer or calculator. However, we can also find the parameter estimates by applying two properties of the least squares line:
        <ul>
          <li>
            <p>The slope of the least squares line can be estimated by
            <me>b_1 = \frac{s_y}{s_x} R</me>
            where <m>R</m> is the correlation between the two variables, and <m>s_x</m> and <m>s_y</m> are the sample standard deviations of the explanatory variable and response, respectively.</p>
          </li>
          <li>
            <p>If <m>\bar{x}</m> is the sample mean of the explanatory variable and <m>\bar{y}</m> is the sample mean of the vertical variable, then the point <m>(\bar{x}, \bar{y})</m> is on the least squares line.</p>
            
            <p><xref ref="fig-summaryStatsElmhurstRegr"/> shows the sample means for the family income and gift aid as $101,780 and $19,940, respectively. We could plot the point (101.8, 19.94) on <xref ref="fig-elmhurstScatterW2Lines"/> to verify it falls on the least squares line (the solid line).</p>
          </li>
        </ul>
        Next, we formally find the point estimates <m>b_0</m> and <m>b_1</m> of the parameters <m>\beta_0</m> and <m>\beta_1</m>.
      </p>

      <figure xml:id="fig-summaryStatsElmhurstRegr">
        <caption>Summary statistics for family income and gift aid.</caption>
        <tabular halign="center">
          <row bottom="medium">
            <cell></cell>
            <cell>Family Income (<m>x</m>)</cell>
            <cell>Gift Aid (<m>y</m>)</cell>
          </row>
          <row>
            <cell>mean</cell>
            <cell><m>\bar{x} = \$101,780</m></cell>
            <cell><m>\bar{y} = \$19,940</m></cell>
          </row>
          <row bottom="medium">
            <cell>sd</cell>
            <cell><m>s_x = \$63,200</m></cell>
            <cell><m>s_y = \$5,460</m></cell>
          </row>
          <row>
            <cell></cell>
            <cell colspan="2"><m>R = -0.499</m></cell>
          </row>
        </tabular>
      </figure>

      <exercise xml:id="ex-finding-slope-elmhurst">
        <statement>
          <p>
            Using the summary statistics in <xref ref="fig-summaryStatsElmhurstRegr"/>, compute the slope for the regression line of gift aid against family income.
          </p>
        </statement>
        <solution>
          <p>
            Compute the slope using the summary statistics from <xref ref="fig-summaryStatsElmhurstRegr"/>:
            <md>
              <mrow>b_1 \amp = \frac{s_y}{s_x} R</mrow>
              <mrow>\amp = \frac{5,460}{63,200}(-0.499)</mrow>
              <mrow>\amp = -0.0431</mrow>
            </md>
          </p>
        </solution>
      </exercise>

      <p>
        You might recall the <term>point-slope</term> form of a line from math class, which we can use to find the model fit, including the estimate of <m>b_0</m>. Given the slope of a line and a point on the line, <m>(x_0, y_0)</m>, the equation for the line can be written as
        <me>y - y_0 = \text{slope}\times (x - x_0)</me>
      </p>

      <assemblage xml:id="assem-identifying-least-squares-line">
        <title>Identifying the least squares line from summary statistics</title>
        <p>
          To identify the least squares line from summary statistics:
          <ul>
            <li><p>Estimate the slope parameter, <m>b_1 = (s_y / s_x) R</m>.</p></li>
            <li><p>Noting that the point <m>(\bar{x}, \bar{y})</m> is on the least squares line, use <m>x_0 = \bar{x}</m> and <m>y_0 = \bar{y}</m> with the point-slope equation: <m>y - \bar{y} = b_1 (x - \bar{x})</m>.</p></li>
            <li><p>Simplify the equation, which would reveal that <m>b_0 = \bar{y} - b_1 \bar{x}</m>.</p></li>
          </ul>
        </p>
      </assemblage>

      <example xml:id="ex-find-lsr-line-elmhurst">
        <statement>
          <p>
            Using the point (101780, 19940) from the sample means and the slope estimate <m>b_1 = -0.0431</m> from <xref ref="ex-finding-slope-elmhurst"/>, find the least-squares line for predicting aid based on family income.
          </p>
        </statement>
        <solution>
          <p>
            Apply the point-slope equation using (101.78, 19.94) and the slope <m>b_1 = -0.0431</m>:
            <md>
              <mrow>y - y_0 \amp = b_1 (x - x_0)</mrow>
              <mrow>y - 19,940 \amp = -0.0431(x - 101,780)</mrow>
            </md>
            Expanding the right side and then adding 19,940 to each side, the equation simplifies:
            <me>\widehat{\text{aid}} = 24,327 - 0.0431 \times \text{family\_income}</me>
            Here we have replaced <m>y</m> with <m>\widehat{\text{aid}}</m> and <m>x</m> with family_income to put the equation in context. The final equation should always include a <q>hat</q> on the variable being predicted, whether it is a generic <q><m>y</m></q> or a named variable like <q><m>\text{aid}</m></q>.
          </p>
        </solution>
      </example>

      <p>
        A computer is usually used to compute the least squares line, and a summary table generated using software for the Elmhurst regression line is shown in <xref ref="fig-rOutputForIncomeAidLSRLine"/>. The first column of numbers provides estimates for <m>{b}_0</m> and <m>{b}_1</m>, respectively. These results match those from <xref ref="ex-find-lsr-line-elmhurst"/> (with some minor rounding error).
      </p>

      <figure xml:id="fig-rOutputForIncomeAidLSRLine">
        <caption>Summary of least squares fit for the Elmhurst data. Compare the parameter estimates in the first column to the results of <xref ref="ex-find-lsr-line-elmhurst"/>.</caption>
        <tabular halign="center">
          <row bottom="medium">
            <cell></cell>
            <cell>Estimate</cell>
            <cell>Std. Error</cell>
            <cell>t value</cell>
            <cell>Pr(<m>></m><m>|</m>t<m>|</m>)</cell>
          </row>
          <row>
            <cell>(Intercept)</cell>
            <cell>24319.3</cell>
            <cell>1291.5</cell>
            <cell>18.83</cell>
            <cell><m>\lt</m>0.0001</cell>
          </row>
          <row>
            <cell>family_income</cell>
            <cell>-0.0431</cell>
            <cell>0.0108</cell>
            <cell>-3.98</cell>
            <cell>0.0002</cell>
          </row>
        </tabular>
      </figure>

      <example xml:id="ex-interpret-regression-output">
        <statement>
          <p>
            Examine the second, third, and fourth columns in <xref ref="fig-rOutputForIncomeAidLSRLine"/>. Can you guess what they represent? (If you have not reviewed any inference chapter yet, skip this example.)
          </p>
        </statement>
        <solution>
          <p>
            We'll describe the meaning of the columns using the second row, which corresponds to <m>\beta_1</m>. The first column provides the point estimate for <m>\beta_1</m>, as we calculated in an earlier example: <m>b_1 = -0.0431</m>. The second column is a standard error for this point estimate: <m>SE_{b_1} = 0.0108</m>. The third column is a <m>t</m>-test statistic for the null hypothesis that <m>\beta_1 = 0</m>: <m>T = -3.98</m>. The last column is the p-value for the <m>t</m>-test statistic for the null hypothesis <m>\beta_1 = 0</m> and a two-sided alternative hypothesis: 0.0002. We will get into more of these details in <xref ref="sec-inference-for-regression"/>.
          </p>
        </solution>
      </example>

      <example xml:id="ex-student-using-model">
        <statement>
          <p>
            Suppose a high school senior is considering Elmhurst College. Can she simply use the linear equation that we have estimated to calculate her financial aid from the university?
          </p>
        </statement>
        <solution>
          <p>
            She may use it as an estimate, though some qualifiers on this approach are important. First, the data all come from one freshman class, and the way aid is determined by the university may change from year to year. Second, the equation will provide an imperfect estimate. While the linear equation is good at capturing the trend in the data, no individual student's aid will be perfectly predicted.
          </p>
        </solution>
      </example>
    </subsection>

    <subsection xml:id="subsec-interpreting-regression-parameters">
      <title>Interpreting regression model parameter estimates</title>
      
      <p>
        Interpreting parameters in a regression model is often one of the most important steps in the analysis.
      </p>

      <example xml:id="ex-interpret-elmhurst-parameters">
        <statement>
          <p>
            The intercept and slope estimates for the Elmhurst data are <m>b_0 = 24,319</m> and <m>b_1 = -0.0431</m>. What do these numbers really mean?
          </p>
        </statement>
        <solution>
          <p>
            Interpreting the slope parameter is helpful in almost any application. For each additional $1,000 of family income, we would expect a student to receive a net difference of <m>\$1,000\times (-0.0431) = -\$43.10</m> in aid on average, i.e. $43.10 <em>less</em>. Note that a higher family income corresponds to less aid because the coefficient of family income is negative in the model. We must be cautious in this interpretation: while there is a real association, we cannot interpret a causal connection between the variables because these data are observational. That is, increasing a student's family income may not cause the student's aid to drop. (It would be reasonable to contact the college and ask if the relationship is causal, i.e. if Elmhurst College's aid decisions are partially based on students' family income.)
          </p>

          <p>
            The estimated intercept <m>b_0 = 24,319</m> describes the average aid if a student's family had no income. The meaning of the intercept is relevant to this application since the family income for some students at Elmhurst is $0. In other applications, the intercept may have little or no practical value if there are no observations where <m>x</m> is near zero.
          </p>
        </solution>
      </example>

      <assemblage xml:id="assem-interpreting-parameters">
        <title>Interpreting parameters estimated by least squares</title>
        <p>
          The slope describes the estimated difference in the <m>y</m> variable if the explanatory variable <m>x</m> for a case happened to be one unit larger. The intercept describes the average outcome of <m>y</m> if <m>x=0</m> <em>and</em> the linear model is valid all the way to <m>x=0</m>, which in many applications is not the case.
        </p>
      </assemblage>
    </subsection>

    <subsection xml:id="subsec-extrapolation">
      <title>Extrapolation is treacherous</title>
      
      <blockquote>
        <p>
          <em>When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.</em>
        </p>
        <attribution>Stephen Colbert, April 6th, 2010<fn>www.cc.com/video-clips/l4nkoq</fn></attribution>
      </blockquote>

      <p>
        Linear models can be used to approximate the relationship between two variables. However, these models have real limitations. Linear regression is simply a modeling framework. The truth is almost always much more complex than our simple line. For example, we do not know how the data outside of our limited window will behave.
      </p>

      <example xml:id="ex-extrapolation-danger">
        <statement>
          <p>
            Use the model <m>\widehat{\text{aid}} = 24,319 - 0.0431 \times \text{family\_income}</m> to estimate the aid of another freshman student whose family had income of $1 million.
          </p>
        </statement>
        <solution>
          <p>
            We want to calculate the aid for <m>\text{family\_income} = 1,000,000</m>:
            <md>
              <mrow>24,319 - 0.0431\times \text{family\_income} \amp = 24,319 - 0.0431\times 1,000,000</mrow>
              <mrow>\amp = -18,781</mrow>
            </md>
            The model predicts this student will have -$18,781 in aid (!). However, Elmhurst College does not offer <em>negative aid</em> where they select some students to pay extra on top of tuition to attend.
          </p>
        </solution>
      </example>

      <p>
        Applying a model estimate to values outside of the realm of the original data is called <term>extrapolation</term>. Generally, a linear model is only an approximation of the real relationship between two variables. If we extrapolate, we are making an unreliable bet that the approximate linear relationship will be valid in places where it has not been analyzed.
      </p>
    </subsection>

    <subsection xml:id="subsec-r-squared">
      <title>Using <m>R^2</m> to describe the strength of a fit</title>
      
      <p>
        We evaluated the strength of the linear relationship between two variables earlier using the correlation, <m>R</m>. However, it is more common to explain the strength of a linear fit using <m>R^2</m>, called <term>R-squared</term>. If provided with a linear model, we might like to describe how closely the data cluster around the linear fit.
      </p>

      <figure xml:id="fig-elmhurstScatterWLSROnly">
        <caption>Gift aid and family income for a random sample of 50 freshman students from Elmhurst College, shown with the least squares regression line.</caption>
        <image source="ch_regr_simple_linear/figures/elmhurstScatterWLSROnly" width="70%">
          <description>A scatterplot of the Elmhurst data is shown for gift aid and family income with the least squares regression line overlaid against the data, which has a slight downward trend.</description>
        </image>
      </figure>

      <p>
        The <m>R^2</m> of a linear model describes the amount of variation in the response that is explained by the least squares line. For example, consider the Elmhurst data, shown in <xref ref="fig-elmhurstScatterWLSROnly"/>. The variance of the response variable, aid received, is about <m>s_{\text{aid}}^2 \approx 29.8</m> million. However, if we apply our least squares line, then this model reduces our uncertainty in predicting aid using a student's family income. The variability in the residuals describes how much variation remains after using the model: <m>s_{_{RES}}^2 \approx 22.4</m> million. In short, there was a reduction of
        <md>
          <mrow>\frac{s_{\text{aid}}^2 - s_{_{RES}}^2}{s_{\text{aid}}^2} \amp = \frac{29,800,000 - 22,400,000}{29,800,000}</mrow>
          <mrow>\amp = \frac{7,500,000}{29,800,000}</mrow>
          <mrow>\amp = 0.25</mrow>
        </md>
        or about 25% in the data's variation by using information about family income for predicting aid using a linear model. This corresponds exactly to the R-squared value:
        <md>
          <mrow>R \amp = -0.499 \amp R^2 \amp = 0.25</mrow>
        </md>
      </p>

      <exercise xml:id="ex-r-squared-interpretation">
        <statement>
          <p>
            If a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the response is explained by the explanatory variable?
          </p>
        </statement>
        <solution>
          <p>
            About <m>R^2 = (-0.97)^2 = 0.94</m> or 94% of the variation is explained by the linear model.
          </p>
        </solution>
      </exercise>
    </subsection>

    <subsection xml:id="subsec-categorical-predictors-two-levels">
      <title>Categorical predictors with two levels</title>
      
      <p>
        Categorical variables are also useful in predicting outcomes. Here we consider a categorical predictor with two levels (recall that a <em>level</em> is the same as a <em>category</em>). We'll consider Ebay auctions for a video game, <em>Mario Kart</em> for the Nintendo Wii, where both the total price of the auction and the condition of the game were recorded. Here we want to predict total price based on game condition, which takes values <c>used</c> and <c>new</c>. A plot of the auction data is shown in <xref ref="fig-marioKartNewUsed"/>.
      </p>

      <figure xml:id="fig-marioKartNewUsed">
        <caption>Total auction prices for the video game <em>Mario Kart</em>, divided into used (<m>x=0</m>) and new (<m>x=1</m>) condition games. The least squares regression line is also shown.</caption>
        <image source="ch_regr_simple_linear/figures/marioKartNewUsed" width="60%">
          <description>A scatterplot is shown for total auction prices for the video game "Mario Kart", broken down by condition on the horizontal axis. The prices are divided into "used" and "new" condition groups. All used games are shown with an x-value of 0 on the left, and all new games are shown with an x-value of 1 on the right of the plot. The used games on the left show a lower average price of about $43, and new games on the right show a higher average price of about $54. The least squares regression line is also shown for this scatterplot, which shows an upward trend and has a formula of "price equals 42.87 plus 10.90 times cond-subscript-new.</description>
        </image>
      </figure>

      <p>
        To incorporate the game condition variable into a regression equation, we must convert the categories into a numerical form. We will do so using an <term>indicator variable</term> called <c>cond_new</c>, which takes value 1 when the game is new and 0 when the game is used. Using this indicator variable, the linear model may be written as
        <me>\widehat{\text{price}} = \beta_0 + \beta_1 \times \text{cond\_new}</me>
        The parameter estimates are given in <xref ref="fig-marioKartNewUsedRegrSummary"/>, and the model equation can be summarized as
        <me>\widehat{\text{price}} = 42.87 + 10.90 \times \text{cond\_new}</me>
        For categorical predictors with just two levels, the linearity assumption will always be satisfied. However, we must evaluate whether the residuals in each group are approximately normal and have approximately equal variance. As can be seen in <xref ref="fig-marioKartNewUsed"/>, both of these conditions are reasonably satisfied by the auction data.
      </p>

      <figure xml:id="fig-marioKartNewUsedRegrSummary">
        <caption>Least squares regression summary for the final auction price against the condition of the game.</caption>
        <tabular halign="center">
          <row bottom="medium">
            <cell></cell>
            <cell>Estimate</cell>
            <cell>Std. Error</cell>
            <cell>t value</cell>
            <cell>Pr(<m>></m><m>|</m>t<m>|</m>)</cell>
          </row>
          <row>
            <cell>(Intercept)</cell>
            <cell>42.87</cell>
            <cell>0.81</cell>
            <cell>52.67</cell>
            <cell><m>\lt</m>0.0001</cell>
          </row>
          <row>
            <cell>cond_new</cell>
            <cell>10.90</cell>
            <cell>1.26</cell>
            <cell>8.66</cell>
            <cell><m>\lt</m>0.0001</cell>
          </row>
        </tabular>
      </figure>

      <example xml:id="ex-interpret-mario-kart">
        <statement>
          <p>
            Interpret the two parameters estimated in the model for the price of <em>Mario Kart</em> in eBay auctions.
          </p>
        </statement>
        <solution>
          <p>
            The intercept is the estimated price when <c>cond_new</c> takes value 0, i.e. when the game is in used condition. That is, the average selling price of a used version of the game is $42.87.
          </p>

          <p>
            The slope indicates that, on average, new games sell for about $10.90 more than used games.
          </p>
        </solution>
      </example>

      <assemblage xml:id="assem-interpret-categorical-predictors">
        <title>Interpreting model estimates for categorical predictors</title>
        <p>
          The estimated intercept is the value of the response variable for the first category (i.e. the category corresponding to an indicator value of 0). The estimated slope is the average change in the response variable between the two categories.
        </p>
      </assemblage>

      <p>
        We'll elaborate further on this topic in Chapter 9, where we examine the influence of many predictor variables simultaneously using multiple regression.
      </p>
    </subsection>

    <exercises xml:id="exercises-least-squares-regression">
      <title>Exercises</title>

      <exercise xml:id="ex-regression-units">
        <statement>
          <p>
            <em>Units of regression.</em> Consider a regression predicting weight (kg) from height (cm) for a sample of adult males. What are the units of the correlation coefficient, the intercept, and the slope?
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-which-higher-scatter">
        <statement>
          <p>
            <em>Which is higher.</em> Determine if I or II is higher or if they are equal. Explain your reasoning. For a regression line, the uncertainty associated with the slope estimate, <m>b_1</m>, is higher when
          </p>
          <p>
            <ol marker="I.">
              <li><p>there is a lot of scatter around the regression line or</p></li>
              <li><p>there is very little scatter around the regression line</p></li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-residual-apple-weight">
        <statement>
          <p>
            <em>Over-under, Part I.</em> Suppose we fit a regression line to predict the shelf life of an apple based on its weight. For a particular apple, we predict the shelf life to be 4.6 days. The apple's residual is -0.6 days. Did we over or under estimate the shelf-life of the apple? Explain your reasoning.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-residual-sun-cancer">
        <statement>
          <p>
            <em>Over-under, Part II.</em> Suppose we fit a regression line to predict the number of incidents of skin cancer per 1,000 people from the number of sunny days in a year. For a particular year, we predict the incidence of skin cancer to be 1.5 per 1,000 people, and the residual for this year is 0.5. Did we over or under estimate the incidence of skin cancer? Explain your reasoning.
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-tourism-spending-reg-conds">
        <statement>
          <p>
            <em>Tourism spending.</em> The Association of Turkish Travel Agencies reports the number of foreign tourists visiting Turkey and tourist spending by year. Three plots are provided: scatterplot showing the relationship between these two variables along with the least squares fit, residuals plot, and histogram of residuals.
          </p>
          <sidebyside widths="32% 32% 32%" margins="auto">
            <image source="ch_regr_simple_linear/figures/eoce/tourism_spending_reg_conds/tourism_spending_count">
              <description>A scatterplot with a least squares regression line is fit based on about 50 points. The horizontal axis represents "Number of tourists" and has values ranging from about 0 to about 27 million. The vertical axis represents "Spending, in US dollars", with values ranging from about $0 to about $17 billion. There are many points shown with the number of tourists between 0 and 5 million, which has spending between about $0 and $3 billion, where even on this small scale a roughly linear trend is evident. The linear trend continues on across the plot and is quite strong -- where residuals generally do not deviate from the least square line by more than very roughly $1 billion. The data are also more sparse for larger values in the plot. There is one region in the center of the plot where about 10 points in a row lie above the regression line. Also consider the next two plots before answering any questions for this exercise.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/tourism_spending_reg_conds/tourism_spending_count_residuals">
              <description>A residual plot is shown. The horizontal axis represents "Number of tourists" and has values ranging from about 0 to about 27 million. Residuals are shown on the vertical axis and have values ranging from about -$1.5 billion to about $1.2 billion. The points on the far left between 0 and 3 million tourists shows a "v" pattern. There are about 15 points with number of tourists between 3 million and 10 million, which shows an slight upward trend from about -$700 million to $1.2 billion. There about 10 points with number of tourists greater than 10 million up to about 27 million, and these show a slight downward trend from about $1 billion to -$1.5 billion.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/tourism_spending_reg_conds/tourism_spending_count_residuals_hist">
              <description>A histogram is shown for the residuals, which shows a roughly bell-shaped distribution centered at 0 and a standard deviation of about $500 million.</description>
            </image>
          </sidebyside>
          <p>
            <ol>
              <li><p>Describe the relationship between number of tourists and spending.</p></li>
              <li><p>What are the explanatory and response variables?</p></li>
              <li><p>Why might we want to fit a regression line to these data?</p></li>
              <li><p>Do the data meet the conditions required for fitting a least squares line? In addition to the scatterplot, use the residual plot and histogram to answer this question.</p></li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-starbucks-cals-carbos">
        <statement>
          <p>
            <em>Nutrition at Starbucks, Part I.</em> The scatterplot below shows the relationship between the number of calories and amount of carbohydrates (in grams) Starbucks food menu items contain. Since Starbucks only lists the number of calories on the display items, we are interested in predicting the amount of carbs a menu item has based on its calorie content.
          </p>
          <sidebyside widths="32% 32% 32%" margins="auto">
            <image source="ch_regr_simple_linear/figures/eoce/starbucks_cals_carbos/starbucks_cals_carbos">
              <description>A scatterplot is shown with about 75 points and an overlaid regression line that trends upward. The horizontal axis represents "Calories" and has values ranging from about 100 to 500. The vertical axis represents "Carbs, in grams" and has values ranging from about 20 to 80. About 15 points are shown with fewer than 200 calories, and these have between about 18 and 25 grams of carbs. About 30 points are shown with 200 to 400 calories, and these mostly have between 30 and 60 grams of carbs. About 20 points are shown with more than 400 calories, and these mostly have between 35 and 80 grams of carbs.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/starbucks_cals_carbos/starbucks_cals_carbos_residuals">
              <description>A residual plot is shown with about 75 points. The horizontal axis represents "Calories" and has values ranging from about 100 to 500. The vertical axis represents "Residuals" and has values ranging from about -30 to 30. About 15 points are shown with fewer than 200 calories, and these have residuals roughly between -7 and positive 2. About 30 points are shown with 200 to 400 calories, and these residuals largely range from about -15 to positive 15. About 20 points are shown with more than 400 calories, and the residuals for these points mostly range between -20 and positive 20.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/starbucks_cals_carbos/starbucks_cals_carbos_residuals_hist">
              <description>A histogram is shown for the residuals, which shows a roughly bell-shaped distribution centered at 0 and a standard deviation of about 10.</description>
            </image>
          </sidebyside>
          <p>
            <ol>
              <li><p>Describe the relationship between number of calories and amount of carbohydrates (in grams) that Starbucks food menu items contain.</p></li>
              <li><p>In this scenario, what are the explanatory and response variables?</p></li>
              <li><p>Why might we want to fit a regression line to these data?</p></li>
              <li><p>Do these data meet the conditions required for fitting a least squares line?</p></li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-coast-starlight-reg">
        <statement>
          <p>
            <em>The Coast Starlight, Part II.</em> An earlier exercise introduces data on the Coast Starlight Amtrak train that runs from Seattle to Los Angeles. The mean travel time from one stop to the next on the Coast Starlight is 129 minutes, with a standard deviation of 113 minutes. The mean distance traveled from one stop to the next is 108 miles with a standard deviation of 99 miles. The correlation between travel time and distance is 0.636.
          </p>
          <p>
            <ol>
              <li><p>Write the equation of the regression line for predicting travel time.</p></li>
              <li><p>Interpret the slope and the intercept in this context.</p></li>
              <li><p>Calculate <m>R^2</m> of the regression line for predicting travel time from distance traveled for the Coast Starlight, and interpret <m>R^2</m> in the context of the application.</p></li>
              <li><p>The distance between Santa Barbara and Los Angeles is 103 miles. Use the model to estimate the time it takes for the Starlight to travel between these two cities.</p></li>
              <li><p>It actually takes the Coast Starlight about 168 minutes to travel from Santa Barbara to Los Angeles. Calculate the residual and explain the meaning of this residual value.</p></li>
              <li><p>Suppose Amtrak is considering adding a stop to the Coast Starlight 500 miles away from Los Angeles. Would it be appropriate to use this linear model to predict the travel time from Los Angeles to this point?</p></li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-body-measurements-shoulder-height-reg">
        <statement>
          <p>
            <em>Body measurements, Part III.</em> An earlier exercise introduces data on shoulder girth and height of a group of individuals. The mean shoulder girth is 107.20 cm with a standard deviation of 10.37 cm. The mean height is 171.14 cm with a standard deviation of 9.41 cm. The correlation between height and shoulder girth is 0.67.
          </p>
          <p>
            <ol>
              <li><p>Write the equation of the regression line for predicting height.</p></li>
              <li><p>Interpret the slope and the intercept in this context.</p></li>
              <li><p>Calculate <m>R^2</m> of the regression line for predicting height from shoulder girth, and interpret it in the context of the application.</p></li>
              <li><p>A randomly selected student from your class has a shoulder girth of 100 cm. Predict the height of this student using the model.</p></li>
              <li><p>The student from part (d) is 160 cm tall. Calculate the residual, and explain what this residual means.</p></li>
              <li><p>A one-year-old has a shoulder girth of 56 cm. Would it be appropriate to use this linear model to predict the height of this child?</p></li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-murders-poverty-reg">
        <statement>
          <p>
            <em>Murders and poverty, Part I.</em> The following regression output is for predicting annual murders per million from percentage living in poverty in a random sample of 20 metropolitan areas.
          </p>
          <sidebyside widths="54% 41%" margins="auto" valign="middle">
            <stack>
              <tabular halign="center">
                <row bottom="medium">
                  <cell></cell>
                  <cell>Estimate</cell>
                  <cell>Std. Error</cell>
                  <cell>t value</cell>
                  <cell>Pr(<m>></m><m>|</m>t<m>|</m>)</cell>
                </row>
                <row>
                  <cell>(Intercept)</cell>
                  <cell>-29.901</cell>
                  <cell>7.789</cell>
                  <cell>-3.839</cell>
                  <cell>0.001</cell>
                </row>
                <row>
                  <cell>poverty%</cell>
                  <cell>2.559</cell>
                  <cell>0.390</cell>
                  <cell>6.562</cell>
                  <cell>0.000</cell>
                </row>
              </tabular>
              <p><m>s = 5.512 \quad R^2 = 70.52\% \quad R^2_{\text{adj}} = 68.89\%</m></p>
              <p>
                <ol>
                  <li><p>Write out the linear model.</p></li>
                  <li><p>Interpret the intercept.</p></li>
                  <li><p>Interpret the slope.</p></li>
                  <li><p>Interpret <m>R^2</m>.</p></li>
                  <li><p>Calculate the correlation coefficient.</p></li>
                </ol>
              </p>
            </stack>
            <image source="ch_regr_simple_linear/figures/eoce/murders_poverty_reg/murders_poverty">
              <description>A scatterplot is shown with 20 points. The horizontal axis is "Percent in Poverty" and has values ranging from 14% to 26%. The vertical axis is "Annual Murders per Million" with values ranging from about 5 to 40. There are 6 points with poverty below 18%, and the Murder Rate for these values ranges from 5 to 13, with one exception of a point at about 17% with a murder rate of about 25. There are 9 points with a poverty rate of 18% to 22%, and the murder rate for these points largely range from 14 to 25, with one exception of a point at about 21% poverty and a murder rate of 35. There are 5 points where poverty is larger than 22%, and these have murder rates ranging from 25 to 40.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-cat-body-heart-reg">
        <statement>
          <p>
            <em>Cats, Part I.</em> The following regression output is for predicting the heart weight (in g) of cats from their body weight (in kg). The coefficients are estimated using a dataset of 144 domestic cats.
          </p>
          <sidebyside widths="54% 41%" margins="auto" valign="middle">
            <stack>
              <tabular halign="center">
                <row bottom="medium">
                  <cell></cell>
                  <cell>Estimate</cell>
                  <cell>Std. Error</cell>
                  <cell>t value</cell>
                  <cell>Pr(<m>></m><m>|</m>t<m>|</m>)</cell>
                </row>
                <row>
                  <cell>(Intercept)</cell>
                  <cell>-0.357</cell>
                  <cell>0.692</cell>
                  <cell>-0.515</cell>
                  <cell>0.607</cell>
                </row>
                <row>
                  <cell>body wt</cell>
                  <cell>4.034</cell>
                  <cell>0.250</cell>
                  <cell>16.119</cell>
                  <cell>0.000</cell>
                </row>
              </tabular>
              <p><m>s = 1.452 \quad R^2 = 64.66\% \quad R^2_{\text{adj}} = 64.41\%</m></p>
              <p>
                <ol>
                  <li><p>Write out the linear model.</p></li>
                  <li><p>Interpret the intercept.</p></li>
                  <li><p>Interpret the slope.</p></li>
                  <li><p>Interpret <m>R^2</m>.</p></li>
                  <li><p>Calculate the correlation coefficient.</p></li>
                </ol>
              </p>
            </stack>
            <image source="ch_regr_simple_linear/figures/eoce/cat_body_heart_reg/cat_body_heart">
              <description>A scatterplot is shown with about 150 points. The horizontal axis is "Body weight, in kilograms" and has values ranging from 2 to 4. The vertical axis is "Heart weight, in grams" with values ranging from about 5 to 20. About 25% of the data has a body weight below 2.5 kilograms, and these have heart weights mostly ranging from 7 to 11 grams. About 35% of the data has body weights between 2.5 and 3 kilograms, and the heart weight for these values mostly ranges from 8 to 12 grams. About 30% of the data has body weights between 3 and 3.5 kilograms, and the heart weight for these values mostly ranges from 11 to 15 grams. About 10% of the data has body weights above 3.5 kilograms, and the heart weight for these values mostly ranges from 12 to 17 grams.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>
    </exercises>
  </section>

  <!-- Section 8.3: Outliers in linear regression -->
  <section xml:id="sec-outliers-in-regression">
    <title>Types of outliers in linear regression</title>
    
    <p>
      In this section, we identify criteria for determining which outliers are important and influential. Outliers in regression are observations that fall far from the cloud of points. These points are especially important because they can have a strong influence on the least squares line.
    </p>

    <example xml:id="outlierPlotsExample">
      <statement>
        <p>
          There are six plots shown in <xref ref="outlierPlots"/> along with the least squares line and residual plots. For each scatterplot and residual plot pair, identify the outliers and note how they influence the least squares line. Recall that an outlier is any point that doesn't appear to belong with the vast majority of the other points.
        </p>
      </statement>
      <solution>
        <p>
          <ol>
            <li>
              <p>
                There is one outlier far from the other points, though it only appears to slightly influence the line.
              </p>
            </li>
            <li>
              <p>
                There is one outlier on the right, though it is quite close to the least squares line, which suggests it wasn't very influential.
              </p>
            </li>
            <li>
              <p>
                There is one point far away from the cloud, and this outlier appears to pull the least squares line up on the right; examine how the line around the primary cloud doesn't appear to fit very well.
              </p>
            </li>
            <li>
              <p>
                There is a primary cloud and then a small secondary cloud of four outliers. The secondary cloud appears to be influencing the line somewhat strongly, making the least square line fit poorly almost everywhere. There might be an interesting explanation for the dual clouds, which is something that could be investigated.
              </p>
            </li>
            <li>
              <p>
                There is no obvious trend in the main cloud of points and the outlier on the right appears to largely control the slope of the least squares line.
              </p>
            </li>
            <li>
              <p>
                There is one outlier far from the cloud. However, it falls quite close to the least squares line and does not appear to be very influential.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </example>

    <figure xml:id="outlierPlots">
      <caption>Six plots, each with a least squares line and residual plot. All data sets have at least one outlier.</caption>
      <image source="ch_regr_simple_linear/figures/outlierPlots/outlierPlots" width="90%">
        <description>Six scatterplots, each with a least squares line and residual plot. All data sets have at least one outlier. (1) A clear positive upward trend is evident in the points with a regression line overlaying these points, but one point is shown deviating substantially from the line about one-third of the way from the left side of the plot and far below the other points. (2) A slight downward trend is evident in the points on the left half of the plot with a regression line overlaying these points and extending to a single point on the far right of the plot that is also very close to the regression line. (3) A positive upward trend is evident for points shown on the left two-thirds of the plot with a regression line overlaying these points, but a single point is shown on the far right and lying substantially above the line. This one point appears to be "pulling" the regression line up on the right, making the line fit the rest of the data less well. (4) Most of the data is shown in the left two-thirds of the plot with a clear downward, linear trend. A cluster of 4 points is shown on the far right but deviating notably above the trend of the other points. The regression line fit to the data shows it largely "trying" to fit the bulk of the data on the left but being "pulled" upward on the right towards the cluster of points deviating from the linear trend. (5) A large cluster of points is shown on the far bottom-left, and there is no apparent trend in this large cluster. A single point is shown on the far upper-right. A regression line is fit to the data with a line extending from the cluster on the bottom-left and trending upwards near the single point on the upper right. (6) A clear downward trend is evident in the points on the right two-thirds of the plot with a regression line overlaying these points and extending to a single point on the far left of the plot that is also very close to the regression line.</description>
      </image>
    </figure>

    <p>
      Examine the residual plots in <xref ref="outlierPlots"/>. You will probably find that there is some trend in the main clouds of (3) and (4). In these cases, the outliers influenced the slope of the least squares lines. In (5), data with no clear trend were assigned a line with a large trend simply due to one outlier (!).
    </p>

    <assemblage xml:id="leverage-box">
      <title>Leverage</title>
      <p>
        Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with <term>high leverage</term>.
      </p>
    </assemblage>

    <p>
      Points that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line. If one of these high leverage points does appear to actually invoke its influence on the slope of the line<mdash/>as in cases (3), (4), and (5) of <xref ref="outlierPlotsExample"/><mdash/>then we call it an <term>influential point</term>. Usually we can say a point is influential if, had we fitted the line without it, the influential point would have been unusually far from the least squares line.
    </p>

    <p>
      It is tempting to remove outliers. Don't do this without a very good reason. Models that ignore exceptional (and interesting) cases often perform poorly. For instance, if a financial firm ignored the largest market swings<mdash/>the <q>outliers</q><mdash/>they would soon go bankrupt by making poorly thought-out investments.
    </p>

    <exercises>
      <title>Section Exercises</title>

      <exercise xml:id="ex-outliers-1">
        <title>Outliers, Part I</title>
        <statement>
          <p>
            Identify the outliers in the scatterplots shown below, and determine what type of outliers they are. Explain your reasoning.
          </p>
          <sidebyside widths="30% 30% 30%">
            <image source="ch_regr_simple_linear/figures/eoce/outliers_1/outliers_1_influential">
              <description>Most of the data is shown in the left third of the plot with a clear downward, linear trend extending from from the upper-left corner of the plot and to the bottom of the plot only a third of the way from the left side of the plot. A single point is shown on the bottom-right of the plot. A regression line is fit to the data, but it does not fit the bulk of the data well: On the furthest left portion, the line is below the points, crosses over the trend of the bulk of the data, then lies above the remainder of the bulk of the data. If it were shown fully, it would extend well below the single point on the bottom-right.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/outliers_1/outliers_2_leverage">
              <description>A clear downward trend is evident in the points on the left third of the plot with a regression line overlaying these points and extending to a single point on the far bottom right of the plot that is also almost exactly on the regression line.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/outliers_1/outliers_3_outlier">
              <description>A downward trend is evident in the bulk of the points with an overlaid regression line. A single point is shown far above the regression line at the center-top of the plot.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-outliers-2">
        <title>Outliers, Part II</title>
        <statement>
          <p>
            Identify the outliers in the scatterplots shown below and determine what type of outliers they are. Explain your reasoning.
          </p>
          <sidebyside widths="30% 30% 30%">
            <image source="ch_regr_simple_linear/figures/eoce/outliers_2/outliers_1_influential">
              <description>Most of the data is shown in the right half of the plot with a clear upward, linear trend extending from from the bottom-center and extending to the upper-right corner of the plot. A single point is shown on the upper-left of the plot. A regression line is fit to the data, but it does not fit the bulk of the data well: Focusing first on the bulk of points at the bottom center of the plot, the regression line is well above these points, crosses over the trend of the bulk of the data about 25% from the right of the plot, then lies below the remainder of the bulk of the data in the upper-right. If it were shown fully, the regression line would extend well below the single point on the upper-left.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/outliers_2/outliers_2_influential">
              <description>A clear upward trend is evident in the points on the right half of the plot with a regression line approximately overlaying these points and extending towards a single point on the far bottom left of the plot, but the regression line is notably higher than this single point, which would have by far the largest residual (in absolute value) of all other points shown in the plot. Close inspection of the regression line fit over the bulk of points, it appears to be partially misfitting that data, "pulled" down on the left side.</description>
            </image>
            <image source="ch_regr_simple_linear/figures/eoce/outliers_2/outliers_3_outlier">
              <description>An upper trend is evident in the bulk of the points with an overlaid regression line. A single point is shown far above the regression line at the center-top of the plot.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-urban-homeowners-outlier">
        <title>Urban homeowners, Part I</title>
        <statement>
          <p>
            The scatterplot below shows the percent of families who own their home vs. the percent of the population living in urban areas. There are 52 observations, each corresponding to a state in the US. Puerto Rico and District of Columbia are also included.
          </p>
          <sidebyside widths="50% 45%">
            <stack>
              <p>
                <ol marker="a.">
                  <li>
                    <p>
                      Describe the relationship between the percent of families who own their home and the percent of the population living in urban areas.
                    </p>
                  </li>
                  <li>
                    <p>
                      The outlier at the bottom right corner is District of Columbia, where 100% of the population is considered urban. What type of an outlier is this observation?
                    </p>
                  </li>
                </ol>
              </p>
            </stack>
            <image source="ch_regr_simple_linear/figures/eoce/urban_homeowners_outlier/urban_homeowners_outlier">
              <description>A scatterplot is shown with about 50 points. The horizontal axis is for "Percent Urban Population" and has values ranging from 40% to 100%. The vertical axis is for "Percent Own Their Home" with values ranging from about 40% to about 75%. About 10 points have Urban Population with values smaller than 60%, and these have Homeownership rates between 65% and 75%, with most of those points above 70%. About 20 points have Urban Population with values between 60% and 70%, and these have Homeownership rates between 62% and 75%. About 20 points have Urban Population with values greater than 70%, and these have Homeownership rates between 55% and 73%, with one exception of a point with 100% urban population that has a homeownership rate of about 43%.</description>
            </image>
          </sidebyside>
        </statement>
      </exercise>

      <exercise xml:id="ex-crawling-babies-outlier">
        <title>Crawling babies, Part II</title>
        <statement>
          <p>
            Exercise <xref ref="ex-crawling-babies-corr"/> introduces data on the average monthly temperature during the month babies first try to crawl (about 6 months after birth) and the average first crawling age for babies born in a given month. A scatterplot of these two variables reveals a potential outlying month when the average temperature is about 53F and average crawling age is about 28.5 weeks. Does this point have high leverage? Is it an influential point?
          </p>
        </statement>
      </exercise>

    </exercises>
  </section>

  <!-- Section 8.4: Inference for linear regression -->
  <section xml:id="sec-inference-for-regression">
    <title>Inference for linear regression</title>
    
    <introduction>
      <p>
        In this section, we discuss uncertainty in the estimates of the slope and y-intercept for a regression line. Just as we identified standard errors for point estimates in previous chapters, we first discuss standard errors for these new estimates.
      </p>
    </introduction>

    <subsection xml:id="subsec-midterm-elections-unemployment">
      <title>Midterm elections and unemployment</title>
      
      <p>
        Elections for members of the United States House of Representatives occur every two years, coinciding every four years with the U.S. Presidential election. The set of House elections occurring during the middle of a Presidential term are called <term>midterm elections</term>. In America's two-party system, one political theory suggests the higher the unemployment rate, the worse the President's party will do in the midterm elections.
      </p>

      <p>
        To assess the validity of this claim, we can compile historical data and look for a connection. We consider every midterm election from 1898 to 2018, with the exception of those elections during the Great Depression. <xref ref="fig-unemploymentAndChangeInHouse"/> shows these data and the least-squares regression line:
        <md>
          <mrow>\amp\text{% change in House seats for President's party}</mrow>
          <mrow>\amp\qquad\qquad= -7.36 - 0.89 \times \text{(unemployment rate)}</mrow>
        </md>
        We consider the percent change in the number of seats of the President's party (e.g. percent change in the number of seats for Republicans in 2018) against the unemployment rate.
      </p>

      <p>
        Examining the data, there are no clear deviations from linearity, the constant variance condition, or substantial outliers. While the data are collected sequentially, a separate analysis was used to check for any apparent correlation between successive observations; no such correlation was found.
      </p>

      <figure xml:id="fig-unemploymentAndChangeInHouse">
        <caption>The percent change in House seats for the President's party in each midterm election from 1898 to 2018 plotted against the unemployment rate. The two points for the Great Depression have been removed, and a least squares regression line has been fit to the data.</caption>
        <image source="ch_regr_simple_linear/figures/unemploymentAndChangeInHouse/unemploymentAndChangeInHouse" width="75%">
          <description>A scatterplot is shown for the percent change in House seats for the President's party in each midterm election from 1898 to 2018 plotted against the unemployment rate. The two points for the Great Depression have been removed, and a least squares regression line has been fit to the data with a slightly downward trend. The horizontal axis is for "Unemployment Rate" with values ranging from about 3% to 12%. The vertical axis is for "Percent Change in Seats of the President's Party in the House of Representatives" with values ranging from about -30% to positive 10%. The bulk of the observations have Unemployment Rate between 3% and 8%, and these have the percent change in seats ranging from about -27% to positive 4% without any discernible trend. There are four observations with unemployment rate above 8%, and these have the percent change in seats ranging from -25% to -9%. Each point in the scatterplot is also labeled as "Democrat" in blue or "Republican" in red, though this doesn't reveal any additional pattern.</description>
        </image>
      </figure>

      <exercise>
        <statement>
          <p>
            The data for the Great Depression (1934 and 1938) were removed because the unemployment rate was 21% and 18%, respectively. Do you agree that they should be removed for this investigation? Why or why not?
          </p>
        </statement>
        <solution>
          <p>
            We will provide two considerations. Each of these points would have very high leverage on any least-squares regression line, and years with such high unemployment may not help us understand what would happen in other years where the unemployment is only modestly high. On the other hand, these are exceptional cases, and we would be discarding important information if we exclude them from a final analysis.
          </p>
        </solution>
      </exercise>

      <p>
        There is a negative slope in the line shown in <xref ref="fig-unemploymentAndChangeInHouse"/>. However, this slope (and the y-intercept) are only estimates of the parameter values. We might wonder, is this convincing evidence that the <q>true</q> linear model has a negative slope? That is, do the data provide strong evidence that the political theory is accurate, where the unemployment rate is a useful predictor of the midterm election? We can frame this investigation into a statistical hypothesis test:
        <ul>
          <li><m>H_0</m>: <m>\beta_1 = 0</m>. The true linear model has slope zero.</li>
          <li><m>H_A</m>: <m>\beta_1 \neq 0</m>. The true linear model has a slope different than zero. The unemployment is predictive of whether the President's party wins or loses seats in the House of Representatives.</li>
        </ul>
        We would reject <m>H_0</m> in favor of <m>H_A</m> if the data provide strong evidence that the true slope parameter is different than zero. To assess the hypotheses, we identify a standard error for the estimate, compute an appropriate test statistic, and identify the p-value.
      </p>
    </subsection>

    <subsection xml:id="subsec-understanding-regression-output">
      <title>Understanding regression output from software</title>
      
      <p>
        Just like other point estimates we have seen before, we can compute a standard error and test statistic for <m>b_1</m>. We will generally label the test statistic using a <m>T</m>, since it follows the <m>t</m>-distribution.
      </p>

      <p>
        We will rely on statistical software to compute the standard error and leave the explanation of how this standard error is determined to a second or third statistics course. <xref ref="fig-midtermUnempRegTable"/> shows software output for the least squares regression line in <xref ref="fig-unemploymentAndChangeInHouse"/>. The row labeled <em>unemp</em> includes the point estimate and other hypothesis test information for the slope, which is the coefficient of the unemployment variable.
      </p>

      <figure xml:id="fig-midtermUnempRegTable">
        <caption>Output from statistical software for the regression line modeling the midterm election losses for the President's party as a response to unemployment.</caption>
        <tabular>
          <row bottom="major">
            <cell></cell>
            <cell>Estimate</cell>
            <cell>Std. Error</cell>
            <cell>t value</cell>
            <cell>Pr(<m>></m>|t|)</cell>
          </row>
          <row>
            <cell>(Intercept)</cell>
            <cell>-7.3644</cell>
            <cell>5.1553</cell>
            <cell>-1.43</cell>
            <cell>0.1646</cell>
          </row>
          <row>
            <cell>unemp</cell>
            <cell>-0.8897</cell>
            <cell>0.8350</cell>
            <cell>-1.07</cell>
            <cell>0.2961</cell>
          </row>
          <row>
            <cell colspan="5" halign="right"><m>df=27</m></cell>
          </row>
        </tabular>
      </figure>

      <example xml:id="ex-midterm-reg-table-cols">
        <statement>
          <p>
            What do the first and second columns of <xref ref="fig-midtermUnempRegTable"/> represent?
          </p>
        </statement>
        <solution>
          <p>
            The entries in the first column represent the least squares estimates, <m>b_0</m> and <m>b_1</m>, and the values in the second column correspond to the standard errors of each estimate. Using the estimates, we could write the equation for the least square regression line as
            <me>\hat{y} = -7.3644 - 0.8897 x</me>
            where <m>\hat{y}</m> in this case represents the predicted change in the number of seats for the president's party, and <m>x</m> represents the unemployment rate.
          </p>
        </solution>
      </example>

      <p>
        We previously used a <m>t</m>-test statistic for hypothesis testing in the context of numerical data. Regression is very similar. In the hypotheses we consider, the null value for the slope is 0, so we can compute the test statistic using the T (or Z) score formula:
        <me>T = \frac{\text{estimate} - \text{null value}}{\text{SE}} = \frac{-0.8897 - 0}{0.8350} = -1.07</me>
        This corresponds to the third column of <xref ref="fig-midtermUnempRegTable"/>.
      </p>

      <example xml:id="ex-midterm-pvalue">
        <statement>
          <p>
            Use the table in <xref ref="fig-midtermUnempRegTable"/> to determine the p-value for the hypothesis test.
          </p>
        </statement>
        <solution>
          <p>
            The last column of the table gives the p-value for the two-sided hypothesis test for the coefficient of the unemployment rate: 0.2961. That is, the data do not provide convincing evidence that a higher unemployment rate has any correspondence with smaller or larger losses for the President's party in the House of Representatives in midterm elections.
          </p>
        </solution>
      </example>

      <assemblage xml:id="assem-inference-for-regression">
        <title>Inference for regression</title>
        <p>
          We usually rely on statistical software to identify point estimates, standard errors, test statistics, and p-values in practice. However, be aware that software will not generally check whether the method is appropriate, meaning we must still verify conditions are met.
        </p>
      </assemblage>

      <example xml:id="ex-elmhurst-slope-assessment">
        <statement>
          <p>
            Examine <xref ref="fig-elmhurstScatterWLSROnly"/>, which relates the Elmhurst College aid and student family income. How sure are you that the slope is statistically significantly different from zero? That is, do you think a formal hypothesis test would reject the claim that the true slope of the line should be zero?
          </p>
        </statement>
        <solution>
          <p>
            While the relationship between the variables is not perfect, there is an evident decreasing trend in the data. This suggests the hypothesis test will reject the null claim that the slope is zero.
          </p>
        </solution>
      </example>

      <exercise>
        <statement>
          <p>
            <xref ref="fig-rOutputForIncomeAidLSRLineInInferenceSection"/> shows statistical software output from fitting the least squares regression line shown in <xref ref="fig-elmhurstScatterWLSROnly"/>. Use this output to formally evaluate the following hypotheses.
            <ul>
              <li><m>H_0</m>: The true coefficient for family income is zero.</li>
              <li><m>H_A</m>: The true coefficient for family income is not zero.</li>
            </ul>
          </p>
        </statement>
        <solution>
          <p>
            We look in the second row corresponding to the family income variable. We see the point estimate of the slope of the line is -0.0431, the standard error of this estimate is 0.0108, and the <m>t</m>-test statistic is <m>T = -3.98</m>. The p-value corresponds exactly to the two-sided test we are interested in: 0.0002. The p-value is so small that we reject the null hypothesis and conclude that family income and financial aid at Elmhurst College for freshman entering in the year 2011 are negatively correlated and the true slope parameter is indeed less than 0, just as we believed in <xref ref="ex-elmhurst-slope-assessment"/>.
          </p>
        </solution>
      </exercise>

      <figure xml:id="fig-rOutputForIncomeAidLSRLineInInferenceSection">
        <caption>Summary of least squares fit for the Elmhurst College data, where we are predicting the gift aid by the university based on the family income of students.</caption>
        <tabular>
          <row bottom="major">
            <cell></cell>
            <cell>Estimate</cell>
            <cell>Std. Error</cell>
            <cell>t value</cell>
            <cell>Pr(<m>></m>|t|)</cell>
          </row>
          <row>
            <cell>(Intercept)</cell>
            <cell>24319.3</cell>
            <cell>1291.5</cell>
            <cell>18.83</cell>
            <cell><m>\lt</m>0.0001</cell>
          </row>
          <row>
            <cell>family_income</cell>
            <cell>-0.0431</cell>
            <cell>0.0108</cell>
            <cell>-3.98</cell>
            <cell>0.0002</cell>
          </row>
          <row>
            <cell colspan="5" halign="right"><m>df=48</m></cell>
          </row>
        </tabular>
      </figure>
    </subsection>

    <subsection xml:id="subsec-ci-for-coefficient">
      <title>Confidence interval for a coefficient</title>
      
      <p>
        Similar to how we can conduct a hypothesis test for a model coefficient using regression output, we can also construct a confidence interval for that coefficient.
      </p>

      <example xml:id="ex-ci-for-family-income">
        <statement>
          <p>
            Compute the 95% confidence interval for the <c>family_income</c> coefficient using the regression output from <xref ref="fig-rOutputForIncomeAidLSRLineInInferenceSection"/>.
          </p>
        </statement>
        <solution>
          <p>
            The point estimate is -0.0431 and the standard error is <m>SE = 0.0108</m>. When constructing a confidence interval for a model coefficient, we generally use a <m>t</m>-distribution. The degrees of freedom for the distribution are noted in the regression output, <m>df = 48</m>, allowing us to identify <m>t_{48}^{\star} = 2.01</m> for use in the confidence interval.
          </p>
          <p>
            We can now construct the confidence interval in the usual way:
            <md>
              <mrow>\text{point estimate} \pm t_{48}^{\star} \times SE \amp\to -0.0431 \pm 2.01 \times 0.0108</mrow>
              <mrow>\amp\to (-0.0648, -0.0214)</mrow>
            </md>
            We are 95% confident that with each dollar increase in <c>family_income</c>, the university's gift aid is predicted to decrease on average by $0.0214 to $0.0648.
          </p>
        </solution>
      </example>

      <assemblage xml:id="assem-ci-for-coefficients">
        <title>Confidence intervals for coefficients</title>
        <p>
          Confidence intervals for model coefficients can be computed using the <m>t</m>-distribution:
          <me>b_i \ \pm\ t_{df}^{\star} \times SE_{b_{i}}</me>
          where <m>t_{df}^{\star}</m> is the appropriate <m>t</m>-value corresponding to the confidence level with the model's degrees of freedom.
        </p>
      </assemblage>

      <p>
        On the topic of intervals in this book, we've focused exclusively on confidence intervals for model parameters. However, there are other types of intervals that may be of interest, including <term>prediction intervals</term> for a response value and also confidence intervals for a <term>mean response value</term> in the context of regression. These two interval types are introduced in an online extra that you may download at <url href="https://www.openintro.org/d?file=stat_extra_linear_regression_supp" visual="www.openintro.org/d?file=stat_extra_linear_regression_supp">www.openintro.org/d?file=stat_extra_linear_regression_supp</url>
      </p>
    </subsection>

    <exercises>
      <title>Section Exercises</title>

      <introduction>
        <p>
          In the following exercises, visually check the conditions for fitting a least squares regression line. However, you do not need to report these conditions in your solutions.
        </p>
      </introduction>

      <exercise xml:id="ex-body-measurements-weight-height-inf">
        <title>Body measurements, Part IV</title>
        <statement>
          <p>
            The scatterplot and least squares summary below show the relationship between weight measured in kilograms and height measured in centimeters of 507 physically active individuals.
          </p>
          <sidebyside widths="40% 55%">
            <image source="ch_regr_simple_linear/figures/eoce/body_measurements_weight_height_inf/body_measurements_weight_height">
              <description>A scatterplot is shown with around 500 points. The horizontal axis is for "Height, in centimeters" and takes values between about 150 to 200 centimeters. The vertical axis is for "Weight, in kilograms" and takes values between about 40 to 120 kilograms. For heights smaller than about 160 centimeters, weights mostly range between 45 and 70 kilograms. For heights between 160 and 175 centimeters, weights mostly range between 55 and 80 kilograms. For heights between 175 and 185 centimeters, weights mostly range between 65 and 90 kilograms. For heights between 185 and 195 centimeters, where there are fewer points, weights mostly range between 80 and 95 kilograms. There are two points with heights at about 196cm, and these have weights of about 85 and 95 kilograms.</description>
            </image>
            <tabular>
              <row bottom="major">
                <cell></cell>
                <cell>Estimate</cell>
                <cell>Std. Error</cell>
                <cell>t value</cell>
                <cell>Pr(<m>></m>|t|)</cell>
              </row>
              <row>
                <cell>(Intercept)</cell>
                <cell>-105.0113</cell>
                <cell>7.5394</cell>
                <cell>-13.93</cell>
                <cell>0.0000</cell>
              </row>
              <row>
                <cell>height</cell>
                <cell>1.0176</cell>
                <cell>0.0440</cell>
                <cell>23.13</cell>
                <cell>0.0000</cell>
              </row>
            </tabular>
          </sidebyside>
          <p>
            <ol>
              <li>Describe the relationship between height and weight.</li>
              <li>Write the equation of the regression line. Interpret the slope and intercept in context.</li>
              <li>Do the data provide strong evidence that an increase in height is associated with an increase in weight? State the null and alternative hypotheses, report the p-value, and state your conclusion.</li>
              <li>The correlation coefficient for height and weight is 0.72. Calculate <m>R^2</m> and interpret it in context.</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-beer-blood-alcohol-inf">
        <title>Beer and blood alcohol content</title>
        <statement>
          <p>
            Many people believe that gender, weight, drinking habits, and many other factors are much more important in predicting blood alcohol content (BAC) than simply considering the number of drinks a person consumed. Here we examine data from sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer. These students were evenly divided between men and women, and they differed in weight and drinking habits. Thirty minutes later, a police officer measured their blood alcohol content (BAC) in grams of alcohol per deciliter of blood. The scatterplot and regression table summarize the findings.
          </p>
          <sidebyside widths="40% 55%">
            <image source="ch_regr_simple_linear/figures/eoce/beer_blood_alcohol_inf/beer_blood_alcohol">
              <description>A scatterplot is shown with around 15 points. The horizontal axis is for "Cans of beer" and takes values between about 1 and 9. The vertical axis is for "Blood Alcohol Concentration (BAC), in grams per deciliter" and takes values between about 0.01 to 0.2. The point at 1 can of beer is at 0.01 BAC, lower than any other values. For the four points at 2 and 3 cans of beer, BAC ranges from 0.02 to 0.07. For the six points at 4 and 5 cans of beer, BAC ranges from 0.05 to 0.10. Two points are at 7 cans of beer and have BAC of 0.09 and 0.10. There is a single point for 8 cans of beer, which has a BAC of 0.12, and one last point at 9 cans of beer, which has a BAC of about 0.19.</description>
            </image>
            <tabular>
              <row bottom="major">
                <cell></cell>
                <cell>Estimate</cell>
                <cell>Std. Error</cell>
                <cell>t value</cell>
                <cell>Pr(<m>></m>|t|)</cell>
              </row>
              <row>
                <cell>(Intercept)</cell>
                <cell>-0.0127</cell>
                <cell>0.0126</cell>
                <cell>-1.00</cell>
                <cell>0.3320</cell>
              </row>
              <row>
                <cell>beers</cell>
                <cell>0.0180</cell>
                <cell>0.0024</cell>
                <cell>7.48</cell>
                <cell>0.0000</cell>
              </row>
            </tabular>
          </sidebyside>
          <p>
            <ol>
              <li>Describe the relationship between the number of cans of beer and BAC.</li>
              <li>Write the equation of the regression line. Interpret the slope and intercept in context.</li>
              <li>Do the data provide strong evidence that drinking more cans of beer is associated with an increase in blood alcohol? State the null and alternative hypotheses, report the p-value, and state your conclusion.</li>
              <li>The correlation coefficient for number of cans of beer and BAC is 0.89. Calculate <m>R^2</m> and interpret it in context.</li>
              <li>Suppose we visit a bar, ask people how many drinks they have had, and also take their BAC. Do you think the relationship between number of drinks and BAC would be as strong as the relationship found in the Ohio State study?</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-husbands-wives-height-inf">
        <title>Husbands and wives, Part II</title>
        <statement>
          <p>
            The scatterplot below summarizes husbands' and wives' heights in a random sample of 170 married couples in Britain, where both partners' ages are below 65 years. Summary output of the least squares fit for predicting wife's height from husband's height is also provided in the table.
          </p>
          <sidebyside widths="40% 55%">
            <image source="ch_regr_simple_linear/figures/eoce/husbands_wives_height_inf_2s/husbands_wives_height_inf_2s">
              <description>A scatterplot is shown with around 200 points. The horizontal axis is for "Husband's height, in inches" and takes values between 60 and 75 inches. The vertical axis is for "Wife's height, in inches" and takes values between 55 and 70 inches. For the approximately fifteen husband heights smaller than 65 inches, wife heights are mostly between 59 and 65 inches. For the approximately 100 husband heights between 65 and 70 inches, wife heights are mostly between 59 and 66 inches. For the approximately 30 husband heights taller than 70 inches, wife heights are mostly between 62 and 67 inches.</description>
            </image>
            <tabular>
              <row bottom="major">
                <cell></cell>
                <cell>Estimate</cell>
                <cell>Std. Error</cell>
                <cell>t value</cell>
                <cell>Pr(<m>></m>|t|)</cell>
              </row>
              <row>
                <cell>(Intercept)</cell>
                <cell>43.5755</cell>
                <cell>4.6842</cell>
                <cell>9.30</cell>
                <cell>0.0000</cell>
              </row>
              <row>
                <cell>height_husband</cell>
                <cell>0.2863</cell>
                <cell>0.0686</cell>
                <cell>4.17</cell>
                <cell>0.0000</cell>
              </row>
            </tabular>
          </sidebyside>
          <p>
            <ol>
              <li>Is there strong evidence that taller men marry taller women? State the hypotheses and include any information used to conduct the test.</li>
              <li>Write the equation of the regression line for predicting wife's height from husband's height.</li>
              <li>Interpret the slope and intercept in the context of the application.</li>
              <li>Given that <m>R^2 = 0.09</m>, what is the correlation of heights in this data set?</li>
              <li>You meet a married man from Britain who is 5'9" (69 inches). What would you predict his wife's height to be? How reliable is this prediction?</li>
              <li>You meet another married man from Britain who is 6'7" (79 inches). Would it be wise to use the same linear model to predict his wife's height? Why or why not?</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-urban-homeowners-cond">
        <title>Urban homeowners, Part II</title>
        <statement>
          <p>
            Exercise <xref ref="ex-urban-homeowners-outlier"/> gives a scatterplot displaying the relationship between the percent of families that own their home and the percent of the population living in urban areas. Below is a similar scatterplot, excluding District of Columbia, as well as the residuals plot. There were 51 cases.
          </p>
          <figure>
            <image source="ch_regr_simple_linear/figures/eoce/urban_homeowners_cond/urban_homeowners_cond" width="50%">
              <description>A scatterplot showing the relationship between percent of the population living in urban areas (horizontal axis) and the percent of families that own their home (vertical axis), excluding District of Columbia. The scatterplot shows data points scattered with an apparent trend, along with a corresponding residual plot to assess the fit of a linear model.</description>
            </image>
          </figure>
          <p>
            <ol>
              <li>For these data, <m>R^2=0.28</m>. What is the correlation? How can you tell if it is positive or negative?</li>
              <li>Examine the residual plot. What do you observe? Is a simple least squares fit appropriate for these data?</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-murders-poverty-inf">
        <title>Murders and poverty, Part II</title>
        <statement>
          <p>
            Exercise <xref ref="ex-murders-poverty-reg"/> presents regression output from a model for predicting annual murders per million from percentage living in poverty based on a random sample of 20 metropolitan areas. The model output is also provided below.
          </p>
          <tabular>
            <row bottom="major">
              <cell></cell>
              <cell>Estimate</cell>
              <cell>Std. Error</cell>
              <cell>t value</cell>
              <cell>Pr(<m>></m>|t|)</cell>
            </row>
            <row>
              <cell>(Intercept)</cell>
              <cell>-29.901</cell>
              <cell>7.789</cell>
              <cell>-3.839</cell>
              <cell>0.001</cell>
            </row>
            <row>
              <cell>poverty%</cell>
              <cell>2.559</cell>
              <cell>0.390</cell>
              <cell>6.562</cell>
              <cell>0.000</cell>
            </row>
          </tabular>
          <p>
            <m>s = 5.512</m> <m>\qquad</m> <m>R^2 = 70.52\%</m> <m>\qquad</m> <m>R^2_{adj} = 68.89\%</m>
          </p>
          <p>
            <ol>
              <li>What are the hypotheses for evaluating whether poverty percentage is a significant predictor of murder rate?</li>
              <li>State the conclusion of the hypothesis test from part (a) in context of the data.</li>
              <li>Calculate a 95% confidence interval for the slope of poverty percentage, and interpret it in context of the data.</li>
              <li>Do your results from the hypothesis test and the confidence interval agree? Explain.</li>
            </ol>
          </p>
        </statement>
      </exercise>

      <exercise xml:id="ex-babies-head-gestation-inf">
        <title>Babies</title>
        <statement>
          <p>
            Is the gestational age (time between conception and birth) of a low birth-weight baby useful in predicting head circumference at birth? Twenty-five low birth-weight babies were studied at a Harvard teaching hospital; the investigators calculated the regression of head circumference (measured in centimeters) against gestational age (measured in weeks). The estimated regression line is
            <me>\widehat{\text{head circumference}} = 3.91 + 0.78 \times \text{gestational age}</me>
          </p>
          <p>
            <ol>
              <li>What is the predicted head circumference for a baby whose gestational age is 28 weeks?</li>
              <li>The standard error for the coefficient of gestational age is 0.35, which is associated with <m>df=23</m>. Does the model provide strong evidence that gestational age is significantly associated with head circumference?</li>
            </ol>
          </p>
        </statement>
      </exercise>

    </exercises>
  </section>

  <!-- Section 8.5: Review exercises -->
  <section xml:id="sec-ch08-review">
    <title>Chapter 8 Review</title>
    
    <exercises>
      <title>Chapter Review</title>
      
      <!-- Exercise 37 -->
      <exercise xml:id="tf_correlation">
        <title>True / False</title>
        <statement>
          <p>
            Determine if the following statements are true or false. If false, explain why.
          </p>
          <p>
            <ol>
              <li>
                <p>
                  A correlation coefficient of -0.90 indicates a stronger linear relationship than a 
                  correlation of 0.5.
                </p>
              </li>
              <li>
                <p>
                  Correlation is a measure of the association between any two variables.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </exercise>
      
      <!-- Exercise 38 -->
      <exercise xml:id="trees_volume_height_diameter">
        <title>Trees</title>
        <statement>
          <p>
            The scatterplots below show the relationship between height, diameter, and volume of timber 
            in 31 felled black cherry trees. The diameter of the tree is measured 4.5 feet above the ground.
          </p>
          <sidebyside widths="46% 46%">
            <figure xml:id="fig-trees-volume-height">
              <caption>Volume vs Height</caption>
              <image source="ch_regr_simple_linear/figures/eoce/trees_volume_height_diameter/trees_volume_height" width="100%">
                <description>A scatterplot is shown with around 30 points. The horizontal axis is for "Height, in feet" and takes values between 60 and 90 feet. The vertical axis is for "Volume, in cubic feet" and takes values between 8 and 80 cubic feet. For the five points with heights smaller than 70 feet, volumes range from about 8 to 25 cubic feet. For the fifteen points with heights between 70 and 80 feet, volumes mostly range from about 15 to 50 cubic feet. For the ten points with heights larger than 80 feet, volumes mostly range from about 20 to 65 cubic feet, with one outlier with a height of about 88 feet and a volume of about 80 cubic feet.</description>
              </image>
            </figure>
            <figure xml:id="fig-trees-volume-diameter">
              <caption>Volume vs Diameter</caption>
              <image source="ch_regr_simple_linear/figures/eoce/trees_volume_height_diameter/trees_volume_diameter" width="100%">
                <description>A scatterplot is shown with around 30 points. The horizontal axis is for "Diameter, in inches" and takes values between 8 and 22 inches. The vertical axis is for "Volume, in cubic feet" and takes values between 8 and 80 cubic feet. About 15 points with diameters smaller than 12 inches have volumes ranging from about 8 to 25 cubic feet. For the approximately ten points with diameters between 12 and 16 inches, volumes range from 22 to 35 cubic feet. For the 6 points with diameters larger than 16 inches, volumes range from 40 to 60 cubic feet, with one outlier with a diameter of 22 inches and a volume of about 80 cubic feet.</description>
              </image>
            </figure>
          </sidebyside>
          <p>
            <ol>
              <li><p>Describe the relationship between volume and height of these trees.</p></li>
              <li><p>Describe the relationship between volume and diameter of these trees.</p></li>
              <li>
                <p>
                  Suppose you have height and diameter measurements for another black cherry tree. Which 
                  of these variables would be preferable to use to predict the volume of timber in this 
                  tree using a simple linear regression model? Explain your reasoning.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </exercise>
      
      <!-- Exercise 39 -->
      <exercise xml:id="husbands_wives_age_inf">
        <title>Husbands and wives, Part III</title>
        <statement>
          <p>
            Exercise 8.3.13 presents a scatterplot displaying the relationship between husbands' and wives' 
            ages in a random sample of 170 married couples in Britain, where both partners' ages are below 
            65 years. Given below is summary output of the least squares fit for predicting wife's age from 
            husband's age.
          </p>
          <sidebyside widths="45% 50%">
            <figure xml:id="fig-husbands-wives-age">
              <caption>Wife's age vs Husband's age</caption>
              <image source="ch_regr_simple_linear/figures/eoce/husbands_wives_age_inf/husbands_wives_age" width="100%">
                <description>A scatterplot is shown with about 150 points. The horizontal axis is for "Husband's age" and takes values between about 20 and 65. The vertical axis is for "Wife's age" and takes values between about 20 and 65. The data shows a strong positive linear trend with most points lying close to a diagonal line from lower left to upper right.</description>
              </image>
            </figure>
            <tabular halign="center">
              <row bottom="major">
                <cell></cell>
                <cell>Estimate</cell>
                <cell>Std. Error</cell>
                <cell>t value</cell>
                <cell>Pr(<m>></m>|t|)</cell>
              </row>
              <row>
                <cell>(Intercept)</cell>
                <cell>1.5740</cell>
                <cell>1.1501</cell>
                <cell>1.37</cell>
                <cell>0.1730</cell>
              </row>
              <row>
                <cell>age_husband</cell>
                <cell>0.9112</cell>
                <cell>0.0259</cell>
                <cell>35.25</cell>
                <cell>0.0000</cell>
              </row>
              <row>
                <cell colspan="5"><m>df = 168</m></cell>
              </row>
            </tabular>
          </sidebyside>
          <p>
            <ol>
              <li>
                <p>
                  We might wonder, is the age difference between husbands and wives consistent across ages? 
                  If this were the case, then the slope parameter would be <m>\beta_1 = 1</m>. Use the 
                  information above to evaluate if there is strong evidence that the difference in husband 
                  and wife ages differs for different ages.
                </p>
              </li>
              <li>
                <p>
                  Write the equation of the regression line for predicting wife's age from husband's age.
                </p>
              </li>
              <li><p>Interpret the slope and intercept in context.</p></li>
              <li>
                <p>
                  Given that <m>R^2 = 0.88</m>, what is the correlation of ages in this data set?
                </p>
              </li>
              <li>
                <p>
                  You meet a married man from Britain who is 55 years old. What would you predict his 
                  wife's age to be? How reliable is this prediction?
                </p>
              </li>
              <li>
                <p>
                  You meet another married man from Britain who is 85 years old. Would it be wise to use 
                  the same linear model to predict his wife's age? Explain.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </exercise>
      
      <!-- Exercise 40 -->
      <exercise xml:id="cat_body_heart_inf">
        <title>Cats, Part II</title>
        <statement>
          <p>
            Exercise 8.2.4 presents regression output from a model for predicting the heart weight (in g) 
            of cats from their body weight (in kg). The coefficients are estimated using a dataset of 144 
            domestic cats. The model output is also provided below.
          </p>
          <tabular halign="center">
            <row bottom="major">
              <cell></cell>
              <cell>Estimate</cell>
              <cell>Std. Error</cell>
              <cell>t value</cell>
              <cell>Pr(<m>></m>|t|)</cell>
            </row>
            <row>
              <cell>(Intercept)</cell>
              <cell>-0.357</cell>
              <cell>0.692</cell>
              <cell>-0.515</cell>
              <cell>0.607</cell>
            </row>
            <row>
              <cell>body wt</cell>
              <cell>4.034</cell>
              <cell>0.250</cell>
              <cell>16.119</cell>
              <cell>0.000</cell>
            </row>
          </tabular>
          <p>
            <m>s = 1.452 \qquad R^2 = 64.66\% \qquad R^2_{adj} = 64.41\%</m>
          </p>
          <p>
            <ol>
              <li>
                <p>
                  We see that the point estimate for the slope is positive. What are the hypotheses for 
                  evaluating whether body weight is positively associated with heart weight in cats?
                </p>
              </li>
              <li>
                <p>
                  State the conclusion of the hypothesis test from part (a) in context of the data.
                </p>
              </li>
              <li>
                <p>
                  Calculate a 95% confidence interval for the slope of body weight, and interpret it in 
                  context of the data.
                </p>
              </li>
              <li>
                <p>
                  Do your results from the hypothesis test and the confidence interval agree? Explain.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </exercise>
      
      <!-- Exercise 41 -->
      <exercise xml:id="starbucks_cals_protein">
        <title>Nutrition at Starbucks, Part II</title>
        <statement>
          <p>
            Exercise 8.2.8 introduced a data set on nutrition information on Starbucks food menu items. 
            Based on the scatterplot and the residual plot provided, describe the relationship between 
            the protein content and calories of these menu items, and determine if a simple linear model 
            is appropriate to predict amount of protein from the number of calories.
          </p>
          <figure xml:id="fig-starbucks-cals-protein">
            <caption>Protein vs Calories with residual plot</caption>
            <image source="ch_regr_simple_linear/figures/eoce/starbucks_cals_protein/starbucks_cals_protein" width="35%">
              <description>A scatterplot is shown with about 75 points and an overlaid regression line that trends upward along with a residual plot. The horizontal axis represents "Calories" and has values ranging from about 100 to 500. The vertical axis represents "Protein, in grams" and has values ranging from 0 to about 30. Scatterplot: About 15 points are shown with fewer than 200 calories, and these have between about 0 and 5 grams of protein. About 30 points are shown with 200 to 400 calories, and these mostly have between 5 and 30 grams of protein. About 20 points are shown with more than 400 calories, and these mostly have between 5 and 30 grams of carbs. Residual plot: About 15 points are shown with fewer than 200 calories, and these have residuals roughly between -5 and positive 2. About 30 points are shown with 200 to 400 calories, and these residuals largely range from about -10 to positive 20. About 20 points are shown with more than 400 calories, and the residuals for these points mostly range between -10 and positive 8.</description>
            </image>
          </figure>
        </statement>
      </exercise>
      
      <!-- Exercise 42 -->
      <exercise xml:id="helmet_lunch">
        <title>Helmets and lunches</title>
        <statement>
          <p>
            The scatterplot shows the relationship between socioeconomic status measured as the percentage 
            of children in a neighborhood receiving reduced-fee lunches at school (lunch) and the percentage 
            of bike riders in the neighborhood wearing helmets (helmet). The average percentage of children 
            receiving reduced-fee lunches is 30.8% with a standard deviation of 26.7% and the average 
            percentage of bike riders wearing helmets is 38.8% with a standard deviation of 16.9%.
          </p>
          <sidebyside widths="58% 40%">
            <stack>
              <p>
                <ol>
                  <li>
                    <p>
                      If the <m>R^2</m> for the least-squares regression line for these data is 72%, what 
                      is the correlation between lunch and helmet?
                    </p>
                  </li>
                  <li>
                    <p>
                      Calculate the slope and intercept for the least-squares regression line for these data.
                    </p>
                  </li>
                  <li>
                    <p>
                      Interpret the intercept of the least-squares regression line in the context of the 
                      application.
                    </p>
                  </li>
                  <li>
                    <p>
                      Interpret the slope of the least-squares regression line in the context of the application.
                    </p>
                  </li>
                  <li>
                    <p>
                      What would the value of the residual be for a neighborhood where 40% of the children 
                      receive reduced-fee lunches and 40% of the bike riders wear helmets? Interpret the 
                      meaning of this residual in the context of the application.
                    </p>
                  </li>
                </ol>
              </p>
            </stack>
            <figure xml:id="fig-helmet-lunch">
              <caption>Helmet rate vs Lunch rate</caption>
              <image source="ch_regr_simple_linear/figures/eoce/helmet_lunch/helmet_lunch" width="100%">
                <description>A scatterplot is shown with 12 points. The horizontal axis is for "Rate of Receiving a Reduced-Fee Lunch" and takes values between 0% and 82%. The vertical axis is for "Rate of Wearing a Helmet" and takes values between about 3% and 58%. Eight points have a reduced-fee lunch rate smaller than 25%, and these points have helmet wearing rates between about 20% and 58%. Two points have a reduced-fee lunch rate of about 50%, and these points have helmet wearing rates about 21% and 22%. Two points have a reduced-fee lunch rate of 75% and 82%, and these points have helmet wearing rates of 5% and 3%, respectively.</description>
              </image>
            </figure>
          </sidebyside>
        </statement>
      </exercise>
      
      <!-- Exercise 43 -->
      <exercise xml:id="match_corr_3">
        <title>Match the correlation, Part III</title>
        <statement>
          <p>
            Match each correlation to the corresponding scatterplot.
          </p>
          <p>
            <ol marker="(a)">
              <li><p><m>r = -0.72</m></p></li>
              <li><p><m>r = 0.07</m></p></li>
              <li><p><m>r = 0.86</m></p></li>
              <li><p><m>r = 0.99</m></p></li>
            </ol>
          </p>
          <sidebyside widths="24% 24% 24% 24%">
            <figure xml:id="fig-scatter-1">
              <caption>Plot 1</caption>
              <image source="ch_regr_simple_linear/figures/eoce/match_corr_3/scatter_1" width="100%">
                <description>A scatterplot is shown. The left third of the data has values that range in the bottom half of the range in the vertical direction. The middle third of the data has values that mostly range in the middle 50% of the vertical direction. The right third of the data has values that range in the upper half of the range in the vertical direction.</description>
              </image>
            </figure>
            <figure xml:id="fig-scatter-2">
              <caption>Plot 2</caption>
              <image source="ch_regr_simple_linear/figures/eoce/match_corr_3/scatter_2" width="100%">
                <description>A scatterplot is shown. The pattern resembles an arch, where the left third of the arch has been cut off. The peak of this "arch" of data is about a third of the way into the horizontal range.</description>
              </image>
            </figure>
            <figure xml:id="fig-scatter-3">
              <caption>Plot 3</caption>
              <image source="ch_regr_simple_linear/figures/eoce/match_corr_3/scatter_3" width="100%">
                <description>A scatterplot is shown, with what appears to be a stable upward trend in the data. If we were to imagine a line drawn against the data, the residuals would generally have a standard deviation equal to only about 5% of the vertical range of the data. That is, the data would be very "tightly packed" around the regression line.</description>
              </image>
            </figure>
            <figure xml:id="fig-scatter-4">
              <caption>Plot 4</caption>
              <image source="ch_regr_simple_linear/figures/eoce/match_corr_3/scatter_4" width="100%">
                <description>A scatterplot is shown. There is no clear pattern in the data when looking from left to right.</description>
              </image>
            </figure>
          </sidebyside>
        </statement>
      </exercise>
      
      <!-- Exercise 44 -->
      <exercise xml:id="rate_my_prof">
        <title>Rate my professor</title>
        <statement>
          <p>
            Many college courses conclude by giving students the opportunity to evaluate the course and 
            the instructor anonymously. However, the use of these student evaluations as an indicator of 
            course quality and teaching effectiveness is often criticized because these measures may reflect 
            the influence of non-teaching related characteristics, such as the physical appearance of the 
            instructor. Researchers at University of Texas, Austin collected data on teaching evaluation 
            score (higher score means better) and standardized beauty score (a score of 0 means average, 
            negative score means below average, and a positive score means above average) for a sample of 
            463 professors. The scatterplot below shows the relationship between these variables, and 
            regression output is provided for predicting teaching evaluation score from beauty score.
          </p>
          <tabular halign="center">
            <row bottom="major">
              <cell></cell>
              <cell>Estimate</cell>
              <cell>Std. Error</cell>
              <cell>t value</cell>
              <cell>Pr(<m>></m>|t|)</cell>
            </row>
            <row>
              <cell>(Intercept)</cell>
              <cell>4.010</cell>
              <cell>0.0255</cell>
              <cell>157.21</cell>
              <cell>0.0000</cell>
            </row>
            <row>
              <cell>beauty</cell>
              <cell>0.1325</cell>
              <cell>0.0322</cell>
              <cell>4.13</cell>
              <cell>0.0000</cell>
            </row>
          </tabular>
          <sidebyside widths="48% 48%">
            <stack>
              <p>
                <ol>
                  <li>
                    <p>
                      Given that the average standardized beauty score is -0.0883 and average teaching 
                      evaluation score is 3.9983, calculate the slope. Alternatively, the slope may be 
                      computed using just the information provided in the model summary table.
                    </p>
                  </li>
                  <li>
                    <p>
                      Do these data provide convincing evidence that the slope of the relationship between 
                      teaching evaluation and beauty is positive? Explain your reasoning.
                    </p>
                  </li>
                  <li>
                    <p>
                      List the conditions required for linear regression and check if each one is satisfied 
                      for this model based on the following diagnostic plots.
                    </p>
                  </li>
                </ol>
              </p>
            </stack>
            <figure xml:id="fig-rate-my-prof">
              <caption>Teaching evaluation vs Beauty</caption>
              <image source="ch_regr_simple_linear/figures/eoce/rate_my_prof/rate_my_prof_eval_beauty" width="100%">
                <description>A scatterplot is shown for several hundred points. The horizontal axis is for a "Beauty" score and takes values between -1.8 and positive 2. The vertical axis is for "Teaching evaluation" and takes values between 2 and 5. For beauty scores smaller than 0, the Teaching Evaluation scores range mostly between 2.5 and 4.8, with no obvious trend in this region of the data. For beauty scores between 0 and 1, the Teaching Evaluation scores range mostly between 3 and 4.7. For beauty scores between 1 and 2, the Teaching Evaluation scores range mostly between 3.2 and 4.8.</description>
              </image>
            </figure>
          </sidebyside>
          <sidebyside widths="32% 32% 32%">
            <figure xml:id="fig-rate-my-prof-residuals">
              <caption>Residual plot</caption>
              <image source="ch_regr_simple_linear/figures/eoce/rate_my_prof/rate_my_prof_residuals" width="100%">
                <description>A residual plot is shown for several hundred points. The horizontal axis is for a "Beauty" score and takes values between -1.8 and positive 2. The vertical axis is for "Residuals" and takes values between -1.5 and positive 1. For beauty scores smaller than 0, the residuals range mostly between -1.2 and positive 1. For beauty scores between 0 and 1, the residuals range mostly between -1.2 and positive 0.8. For beauty scores between 1 and 2, which has somewhat fewer points, the residuals range mostly between -1.0 and positive 0.5.</description>
              </image>
            </figure>
            <figure xml:id="fig-rate-my-prof-hist">
              <caption>Histogram of residuals</caption>
              <image source="ch_regr_simple_linear/figures/eoce/rate_my_prof/rate_my_prof_residuals_hist" width="100%">
                <description>A histogram is shown for residuals, where bins range between -2 and 1.5. The distribution is centered at zero and very slightly skewed to the left.</description>
              </image>
            </figure>
            <figure xml:id="fig-rate-my-prof-order">
              <caption>Residuals vs Order</caption>
              <image source="ch_regr_simple_linear/figures/eoce/rate_my_prof/rate_my_prof_residuals_order" width="100%">
                <description>A scatterplot is shown. The horizontal axis is for "Order of data collection" and takes values between 1 and about 450. The vertical axis is for "Residuals" and takes values between about -1.5 and positive 1. The residuals mostly lie between -1.2 and 0.9 across the range with no discernible pattern.</description>
              </image>
            </figure>
          </sidebyside>
        </statement>
      </exercise>
      
    </exercises>
  </section>

</chapter>
